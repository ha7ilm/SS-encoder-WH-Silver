{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy, torch\n",
    "from torch import nn,optim\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "figure_folder = 'WH-figures/'\n",
    "\n",
    "#follow the intruction in deepSI on github to install our preliminary toolbox\n",
    "import deepSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Wiener-Hammerstein (WH) data\n",
    "\n",
    "see: https://sites.google.com/view/nonlinear-benchmark/benchmarks/wiener-hammerstein?authuser=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train = System_data of length: 100000 nu=None ny=None normed=False (100000,) (100000,)\n",
      "test = System_data of length: 78800 nu=None ny=None normed=False (78800,) (78800,)\n"
     ]
    }
   ],
   "source": [
    "train, test = deepSI.datasets.WienerHammerBenchMark(split_data=True) #will download the dataset into C:/Users/user/AppData/Local/deepSI/data_sets\n",
    "print('train =',train, train.y.shape, train.u.shape) #training and validation is combined into one training set\n",
    "print('test =',test, test.y.shape, test.u.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit/load state-space encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## new best ########### 1.1419269700718808\n",
      "N_training_samples=99871, batch_size=1024, N_batch_updates_per_epoch=97\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b83c3e66124d4fba60d00741a57c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## new best ########### 0.7293688560787519\n",
      "Epoch:    1 Training loss:  0.9422 Validation loss = 0.7294, time Loss: 30.8%, back: 30.3%, val: 36.2%\n",
      "########## new best ########### 0.313923136220541\n",
      "Epoch:    2 Training loss:  0.2355 Validation loss = 0.3139, time Loss: 31.2%, back: 30.3%, val: 35.9%\n",
      "########## new best ########### 0.2179184204256538\n",
      "Epoch:    3 Training loss: 0.06813 Validation loss = 0.2179, time Loss: 31.5%, back: 30.4%, val: 35.4%\n",
      "########## new best ########### 0.12091346722033165\n",
      "Epoch:    4 Training loss: 0.02676 Validation loss = 0.1209, time Loss: 31.6%, back: 30.4%, val: 35.4%\n",
      "########## new best ########### 0.10587108444389494\n",
      "Epoch:    5 Training loss: 0.01294 Validation loss = 0.1059, time Loss: 31.6%, back: 30.4%, val: 35.4%\n",
      "########## new best ########### 0.09968857410493408\n",
      "Epoch:    6 Training loss: 0.01052 Validation loss = 0.09969, time Loss: 31.6%, back: 30.4%, val: 35.4%\n",
      "########## new best ########### 0.09461095444716446\n",
      "Epoch:    7 Training loss: 0.009405 Validation loss = 0.09461, time Loss: 31.6%, back: 30.4%, val: 35.4%\n",
      "########## new best ########### 0.0914134288956095\n",
      "Epoch:    8 Training loss: 0.008721 Validation loss = 0.09141, time Loss: 31.6%, back: 30.4%, val: 35.4%\n",
      "########## new best ########### 0.08882447873159166\n",
      "Epoch:    9 Training loss: 0.008285 Validation loss = 0.08882, time Loss: 31.6%, back: 30.4%, val: 35.4%\n",
      "########## new best ########### 0.08663575362084872\n",
      "Epoch:   10 Training loss: 0.007846 Validation loss = 0.08664, time Loss: 31.7%, back: 30.4%, val: 35.3%\n",
      "########## new best ########### 0.08598371090910041\n",
      "Epoch:   11 Training loss: 0.007459 Validation loss = 0.08598, time Loss: 31.7%, back: 30.4%, val: 35.3%\n",
      "########## new best ########### 0.08442445171531196\n",
      "Epoch:   12 Training loss: 0.007368 Validation loss = 0.08442, time Loss: 31.7%, back: 30.4%, val: 35.3%\n",
      "Epoch:   13 Training loss: 0.007099 Validation loss = 0.08629, time Loss: 31.7%, back: 30.4%, val: 35.2%\n",
      "########## new best ########### 0.08200928513147268\n",
      "Epoch:   14 Training loss: 0.006915 Validation loss = 0.08201, time Loss: 31.7%, back: 30.5%, val: 35.2%\n",
      "########## new best ########### 0.08192343764507386\n",
      "Epoch:   15 Training loss: 0.006726 Validation loss = 0.08192, time Loss: 31.8%, back: 30.5%, val: 35.0%\n",
      "########## new best ########### 0.08071959339828827\n",
      "Epoch:   16 Training loss: 0.006689 Validation loss = 0.08072, time Loss: 31.8%, back: 30.5%, val: 35.0%\n",
      "Epoch:   17 Training loss: 0.006527 Validation loss = 0.08236, time Loss: 31.8%, back: 30.5%, val: 35.0%\n",
      "########## new best ########### 0.08027666667889234\n",
      "Epoch:   18 Training loss: 0.006502 Validation loss = 0.08028, time Loss: 31.8%, back: 30.5%, val: 35.0%\n",
      "########## new best ########### 0.07899427272260084\n",
      "Epoch:   19 Training loss: 0.006341 Validation loss = 0.07899, time Loss: 31.8%, back: 30.5%, val: 35.1%\n",
      "Epoch:   20 Training loss: 0.006317 Validation loss = 0.08161, time Loss: 31.8%, back: 30.5%, val: 35.0%\n",
      "########## new best ########### 0.07797717342060172\n",
      "Epoch:   21 Training loss: 0.006286 Validation loss = 0.07798, time Loss: 31.8%, back: 30.5%, val: 35.0%\n",
      "Epoch:   22 Training loss: 0.00608 Validation loss = 0.0785, time Loss: 31.8%, back: 30.5%, val: 35.0%\n",
      "########## new best ########### 0.077297387689171\n",
      "Epoch:   23 Training loss: 0.006286 Validation loss = 0.0773, time Loss: 31.8%, back: 30.5%, val: 35.0%\n",
      "Epoch:   24 Training loss: 0.00603 Validation loss = 0.07776, time Loss: 31.9%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.07726290894652192\n",
      "Epoch:   25 Training loss: 0.005873 Validation loss = 0.07726, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "########## new best ########### 0.0757304046805884\n",
      "Epoch:   26 Training loss: 0.00575 Validation loss = 0.07573, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "Epoch:   27 Training loss: 0.005868 Validation loss = 0.07575, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.07557649553823988\n",
      "Epoch:   28 Training loss: 0.005647 Validation loss = 0.07558, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.0752134105732261\n",
      "Epoch:   29 Training loss: 0.005683 Validation loss = 0.07521, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "Epoch:   30 Training loss: 0.005628 Validation loss = 0.07716, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "########## new best ########### 0.07372566459654223\n",
      "Epoch:   31 Training loss: 0.00543 Validation loss = 0.07373, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "########## new best ########### 0.0733328519802789\n",
      "Epoch:   32 Training loss: 0.005416 Validation loss = 0.07333, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "########## new best ########### 0.07221654271798532\n",
      "Epoch:   33 Training loss: 0.005337 Validation loss = 0.07222, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "Epoch:   34 Training loss: 0.005243 Validation loss = 0.07444, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "########## new best ########### 0.07194515299248863\n",
      "Epoch:   35 Training loss: 0.005309 Validation loss = 0.07195, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "########## new best ########### 0.07108970017097477\n",
      "Epoch:   36 Training loss: 0.00505 Validation loss = 0.07109, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "Epoch:   37 Training loss: 0.005146 Validation loss = 0.07247, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "########## new best ########### 0.06946626527605676\n",
      "Epoch:   38 Training loss: 0.005015 Validation loss = 0.06947, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "Epoch:   39 Training loss: 0.004969 Validation loss = 0.0704, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "########## new best ########### 0.06911097615119371\n",
      "Epoch:   40 Training loss: 0.004738 Validation loss = 0.06911, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "Epoch:   41 Training loss: 0.004641 Validation loss = 0.06986, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "Epoch:   42 Training loss: 0.004631 Validation loss = 0.07014, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "Epoch:   43 Training loss: 0.004602 Validation loss = 0.07446, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "########## new best ########### 0.06571235561887305\n",
      "Epoch:   44 Training loss: 0.004426 Validation loss = 0.06571, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "########## new best ########### 0.06461680786017457\n",
      "Epoch:   45 Training loss: 0.004316 Validation loss = 0.06462, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "Epoch:   46 Training loss: 0.004225 Validation loss = 0.06474, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "########## new best ########### 0.06384829714052387\n",
      "Epoch:   47 Training loss:  0.0041 Validation loss = 0.06385, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.06223279836279697\n",
      "Epoch:   48 Training loss: 0.004014 Validation loss = 0.06223, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.06111054856624361\n",
      "Epoch:   49 Training loss: 0.004052 Validation loss = 0.06111, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   50 Training loss: 0.003692 Validation loss = 0.06646, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.06040529015960009\n",
      "Epoch:   51 Training loss: 0.003738 Validation loss = 0.06041, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   52 Training loss: 0.003622 Validation loss = 0.06142, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.057735875107145174\n",
      "Epoch:   53 Training loss: 0.003476 Validation loss = 0.05774, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.05658625034353527\n",
      "Epoch:   54 Training loss: 0.003351 Validation loss = 0.05659, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   55 Training loss: 0.003568 Validation loss = 0.05767, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.05518835769220735\n",
      "Epoch:   56 Training loss: 0.003177 Validation loss = 0.05519, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   57 Training loss: 0.003073 Validation loss = 0.0564, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   58 Training loss: 0.003073 Validation loss = 0.05572, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   59 Training loss: 0.00295 Validation loss = 0.05611, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.05366272620198875\n",
      "Epoch:   60 Training loss: 0.003032 Validation loss = 0.05366, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## new best ########### 0.05207053429238509\n",
      "Epoch:   61 Training loss: 0.002936 Validation loss = 0.05207, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "Epoch:   62 Training loss: 0.002883 Validation loss = 0.05304, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "Epoch:   63 Training loss: 0.002744 Validation loss = 0.05241, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.050763330426269175\n",
      "Epoch:   64 Training loss: 0.002765 Validation loss = 0.05076, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "Epoch:   65 Training loss: 0.00269 Validation loss = 0.05117, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   66 Training loss: 0.002752 Validation loss = 0.05228, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   67 Training loss: 0.002573 Validation loss = 0.05327, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.04995869409330206\n",
      "Epoch:   68 Training loss: 0.002476 Validation loss = 0.04996, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   69 Training loss: 0.002659 Validation loss = 0.0512, time Loss: 31.9%, back: 30.6%, val: 34.9%\n",
      "########## new best ########### 0.048430993153873884\n",
      "Epoch:   70 Training loss: 0.002409 Validation loss = 0.04843, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   71 Training loss: 0.002507 Validation loss = 0.05126, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   72 Training loss: 0.002585 Validation loss = 0.05693, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.047657280967945276\n",
      "Epoch:   73 Training loss: 0.002292 Validation loss = 0.04766, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.047191484495724294\n",
      "Epoch:   74 Training loss: 0.002334 Validation loss = 0.04719, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   75 Training loss: 0.002299 Validation loss = 0.0503, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.04693595662420536\n",
      "Epoch:   76 Training loss: 0.002301 Validation loss = 0.04694, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   77 Training loss: 0.002255 Validation loss = 0.04933, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   78 Training loss: 0.002198 Validation loss = 0.0588, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.04588221528060677\n",
      "Epoch:   79 Training loss: 0.002228 Validation loss = 0.04588, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   80 Training loss: 0.002114 Validation loss = 0.04917, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   81 Training loss: 0.002281 Validation loss = 0.04665, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.04543139316945255\n",
      "Epoch:   82 Training loss: 0.002055 Validation loss = 0.04543, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.04534788839364383\n",
      "Epoch:   83 Training loss: 0.002154 Validation loss = 0.04535, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   84 Training loss: 0.002046 Validation loss = 0.04666, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   85 Training loss: 0.002047 Validation loss = 0.0465, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.04315148217350643\n",
      "Epoch:   86 Training loss: 0.002127 Validation loss = 0.04315, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   87 Training loss: 0.002141 Validation loss = 0.04414, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.04276068827898028\n",
      "Epoch:   88 Training loss: 0.001885 Validation loss = 0.04276, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   89 Training loss: 0.001978 Validation loss = 0.04489, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   90 Training loss: 0.002053 Validation loss = 0.05178, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   91 Training loss: 0.001919 Validation loss = 0.04675, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0425684184625738\n",
      "Epoch:   92 Training loss: 0.00195 Validation loss = 0.04257, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   93 Training loss: 0.001977 Validation loss = 0.04313, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   94 Training loss: 0.001932 Validation loss = 0.04284, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.041931354835533556\n",
      "Epoch:   95 Training loss: 0.001851 Validation loss = 0.04193, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   96 Training loss: 0.001963 Validation loss = 0.04272, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   97 Training loss: 0.00197 Validation loss = 0.04963, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   98 Training loss: 0.001822 Validation loss = 0.04536, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:   99 Training loss: 0.001845 Validation loss = 0.04412, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.04129790300389852\n",
      "Epoch:  100 Training loss: 0.001922 Validation loss = 0.0413, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:  101 Training loss: 0.001852 Validation loss = 0.04204, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.04126768589594729\n",
      "Epoch:  102 Training loss: 0.00179 Validation loss = 0.04127, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.04079510058718615\n",
      "Epoch:  103 Training loss: 0.001775 Validation loss = 0.0408, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.04048552464244709\n",
      "Epoch:  104 Training loss: 0.001914 Validation loss = 0.04049, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:  105 Training loss: 0.001742 Validation loss = 0.0413, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  106 Training loss: 0.001844 Validation loss = 0.04263, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  107 Training loss: 0.001723 Validation loss = 0.04438, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  108 Training loss: 0.001804 Validation loss = 0.04106, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.04043532588002851\n",
      "Epoch:  109 Training loss: 0.001751 Validation loss = 0.04044, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:  110 Training loss: 0.00194 Validation loss = 0.04288, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:  111 Training loss: 0.001676 Validation loss = 0.04363, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.03958583251874977\n",
      "Epoch:  112 Training loss: 0.001684 Validation loss = 0.03959, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:  113 Training loss: 0.001838 Validation loss = 0.04036, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:  114 Training loss: 0.001716 Validation loss =   0.04, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:  115 Training loss: 0.00163 Validation loss = 0.04049, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.03925198430130774\n",
      "Epoch:  116 Training loss: 0.001718 Validation loss = 0.03925, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:  117 Training loss: 0.001707 Validation loss = 0.04014, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:  118 Training loss: 0.001626 Validation loss = 0.04119, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:  119 Training loss: 0.001743 Validation loss = 0.03926, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  120 Training loss: 0.001684 Validation loss = 0.04002, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  121 Training loss: 0.001639 Validation loss = 0.04941, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  122 Training loss: 0.001666 Validation loss = 0.04939, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  123 Training loss: 0.00166 Validation loss = 0.04603, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  124 Training loss: 0.001624 Validation loss = 0.03952, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  125 Training loss: 0.001632 Validation loss = 0.0415, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.03866105246756651\n",
      "Epoch:  126 Training loss: 0.001622 Validation loss = 0.03866, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  127 Training loss: 0.001635 Validation loss = 0.04089, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  128 Training loss: 0.001548 Validation loss = 0.0389, time Loss: 31.8%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  129 Training loss: 0.001584 Validation loss = 0.03967, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  130 Training loss: 0.001583 Validation loss = 0.04041, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  131 Training loss: 0.001593 Validation loss = 0.04062, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.03730117861219639\n",
      "Epoch:  132 Training loss: 0.001552 Validation loss = 0.0373, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  133 Training loss: 0.001603 Validation loss = 0.0457, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  134 Training loss: 0.001521 Validation loss = 0.04201, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  135 Training loss: 0.001528 Validation loss = 0.04013, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  136 Training loss: 0.001583 Validation loss = 0.04041, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  137 Training loss: 0.001562 Validation loss = 0.03946, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  138 Training loss: 0.001587 Validation loss = 0.03982, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  139 Training loss: 0.001444 Validation loss = 0.04358, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  140 Training loss: 0.001531 Validation loss = 0.03846, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  141 Training loss: 0.00148 Validation loss = 0.04775, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  142 Training loss: 0.00148 Validation loss = 0.03877, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.036552913001029315\n",
      "Epoch:  143 Training loss: 0.001497 Validation loss = 0.03655, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  144 Training loss: 0.001602 Validation loss = 0.03735, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  145 Training loss: 0.001426 Validation loss = 0.03826, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  146 Training loss: 0.001394 Validation loss = 0.0382, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  147 Training loss: 0.001464 Validation loss = 0.03838, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  148 Training loss: 0.001469 Validation loss = 0.0382, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  149 Training loss: 0.001362 Validation loss = 0.03888, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  150 Training loss: 0.001519 Validation loss = 0.03716, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.03570558700411262\n",
      "Epoch:  151 Training loss: 0.001389 Validation loss = 0.03571, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  152 Training loss: 0.001435 Validation loss = 0.03888, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  153 Training loss: 0.001414 Validation loss = 0.03709, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.035424270105418064\n",
      "Epoch:  154 Training loss: 0.001437 Validation loss = 0.03542, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  155 Training loss: 0.001395 Validation loss = 0.03604, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  156 Training loss: 0.001389 Validation loss = 0.0402, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  157 Training loss: 0.001371 Validation loss = 0.03792, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  158 Training loss: 0.001412 Validation loss = 0.03615, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  159 Training loss: 0.001382 Validation loss = 0.03638, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  160 Training loss: 0.00132 Validation loss = 0.03801, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.03425020222455668\n",
      "Epoch:  161 Training loss: 0.001361 Validation loss = 0.03425, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  162 Training loss: 0.001372 Validation loss = 0.03675, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  163 Training loss: 0.001431 Validation loss = 0.03508, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  164 Training loss: 0.001284 Validation loss = 0.0371, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  165 Training loss: 0.001344 Validation loss = 0.04239, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  166 Training loss: 0.001328 Validation loss = 0.03785, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  167 Training loss: 0.001316 Validation loss = 0.03588, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  168 Training loss: 0.001262 Validation loss = 0.03557, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  169 Training loss: 0.00131 Validation loss = 0.03756, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.033563107842308953\n",
      "Epoch:  170 Training loss: 0.001432 Validation loss = 0.03356, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  171 Training loss: 0.001273 Validation loss = 0.03487, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  172 Training loss: 0.001283 Validation loss = 0.0338, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  173 Training loss: 0.001273 Validation loss = 0.0355, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  174 Training loss: 0.001237 Validation loss = 0.0358, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  175 Training loss: 0.001288 Validation loss = 0.03621, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.03278854243538056\n",
      "Epoch:  176 Training loss: 0.001223 Validation loss = 0.03279, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  177 Training loss: 0.001344 Validation loss = 0.03605, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  178 Training loss: 0.001239 Validation loss = 0.03355, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  179 Training loss: 0.001152 Validation loss = 0.04069, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  180 Training loss: 0.001333 Validation loss = 0.03404, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  181 Training loss: 0.001203 Validation loss = 0.03771, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  182 Training loss: 0.001206 Validation loss = 0.03516, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  183 Training loss: 0.001264 Validation loss = 0.04007, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  184 Training loss: 0.001183 Validation loss = 0.03921, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  185 Training loss: 0.001183 Validation loss = 0.03293, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  186 Training loss: 0.001189 Validation loss = 0.03283, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  187 Training loss: 0.001235 Validation loss = 0.03496, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  188 Training loss: 0.001146 Validation loss = 0.03361, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  189 Training loss: 0.001148 Validation loss = 0.03385, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0320263446613646\n",
      "Epoch:  190 Training loss: 0.001202 Validation loss = 0.03203, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  191 Training loss: 0.001191 Validation loss = 0.03503, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  192 Training loss: 0.001094 Validation loss = 0.03206, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  193 Training loss: 0.001166 Validation loss = 0.03365, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.03197717382205927\n",
      "Epoch:  194 Training loss: 0.001141 Validation loss = 0.03198, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  195 Training loss: 0.001147 Validation loss = 0.03431, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.03175483248889231\n",
      "Epoch:  196 Training loss: 0.001283 Validation loss = 0.03175, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  197 Training loss: 0.001014 Validation loss = 0.0363, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.03065646428432174\n",
      "Epoch:  198 Training loss: 0.001043 Validation loss = 0.03066, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  199 Training loss: 0.001094 Validation loss = 0.03151, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  200 Training loss: 0.001036 Validation loss = 0.03522, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  201 Training loss: 0.001132 Validation loss = 0.03282, time Loss: 31.8%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## new best ########### 0.03032988355659657\n",
      "Epoch:  202 Training loss: 0.001025 Validation loss = 0.03033, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  203 Training loss: 0.001061 Validation loss = 0.03441, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.03016122881262773\n",
      "Epoch:  204 Training loss: 0.001023 Validation loss = 0.03016, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  205 Training loss: 0.001006 Validation loss = 0.03045, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  206 Training loss: 0.001067 Validation loss = 0.03085, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.029697253123744285\n",
      "Epoch:  207 Training loss: 0.0009925 Validation loss = 0.0297, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  208 Training loss: 0.001016 Validation loss = 0.03014, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.02929084880000473\n",
      "Epoch:  209 Training loss: 0.001067 Validation loss = 0.02929, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  210 Training loss: 0.0009435 Validation loss = 0.02965, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  211 Training loss: 0.0009481 Validation loss = 0.03469, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  212 Training loss: 0.001014 Validation loss = 0.03189, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.02787933849662194\n",
      "Epoch:  213 Training loss: 0.0009101 Validation loss = 0.02788, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  214 Training loss: 0.0009768 Validation loss = 0.03106, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0277606578936809\n",
      "Epoch:  215 Training loss: 0.0009294 Validation loss = 0.02776, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  216 Training loss: 0.0009298 Validation loss = 0.02968, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  217 Training loss: 0.000912 Validation loss = 0.03024, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  218 Training loss: 0.0009188 Validation loss = 0.02869, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  219 Training loss: 0.0008968 Validation loss = 0.02807, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  220 Training loss: 0.0008853 Validation loss = 0.02896, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  221 Training loss: 0.000852 Validation loss = 0.02779, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.027082868554898876\n",
      "Epoch:  222 Training loss: 0.0009127 Validation loss = 0.02708, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.026712845065844842\n",
      "Epoch:  223 Training loss: 0.0008899 Validation loss = 0.02671, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  224 Training loss: 0.0007697 Validation loss = 0.03475, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.02599069581225684\n",
      "Epoch:  225 Training loss: 0.0008822 Validation loss = 0.02599, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.02581205117229298\n",
      "Epoch:  226 Training loss: 0.0008568 Validation loss = 0.02581, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  227 Training loss: 0.0007685 Validation loss = 0.02844, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  228 Training loss: 0.0007719 Validation loss = 0.02584, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  229 Training loss: 0.0008014 Validation loss = 0.02984, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.02558448428120725\n",
      "Epoch:  230 Training loss: 0.0008918 Validation loss = 0.02558, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  231 Training loss: 0.000693 Validation loss = 0.02713, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  232 Training loss: 0.0007643 Validation loss = 0.0288, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  233 Training loss: 0.0007655 Validation loss = 0.02703, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  234 Training loss: 0.0007531 Validation loss = 0.03409, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.023824041027218322\n",
      "Epoch:  235 Training loss: 0.0007154 Validation loss = 0.02382, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  236 Training loss: 0.000725 Validation loss = 0.02856, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  237 Training loss: 0.0006979 Validation loss = 0.04076, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  238 Training loss: 0.0007713 Validation loss = 0.02433, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  239 Training loss: 0.0006647 Validation loss = 0.03198, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0235107287336568\n",
      "Epoch:  240 Training loss: 0.0006977 Validation loss = 0.02351, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.023417812416113713\n",
      "Epoch:  241 Training loss: 0.0007071 Validation loss = 0.02342, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  242 Training loss: 0.0006421 Validation loss = 0.02664, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.022410173963293756\n",
      "Epoch:  243 Training loss: 0.0006452 Validation loss = 0.02241, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  244 Training loss: 0.0007486 Validation loss = 0.02297, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  245 Training loss: 0.000589 Validation loss = 0.02691, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  246 Training loss: 0.0006241 Validation loss = 0.02444, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  247 Training loss: 0.0006677 Validation loss = 0.02285, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  248 Training loss: 0.0006522 Validation loss = 0.02551, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  249 Training loss: 0.0006106 Validation loss = 0.02503, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  250 Training loss: 0.0005977 Validation loss = 0.03367, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  251 Training loss: 0.0005913 Validation loss = 0.02388, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.022176323739380995\n",
      "Epoch:  252 Training loss: 0.000606 Validation loss = 0.02218, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  253 Training loss: 0.000584 Validation loss = 0.0234, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.021599120724911096\n",
      "Epoch:  254 Training loss: 0.0006195 Validation loss = 0.0216, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  255 Training loss: 0.0005988 Validation loss = 0.02267, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  256 Training loss: 0.0005395 Validation loss = 0.03043, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  257 Training loss: 0.000544 Validation loss = 0.02365, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.021012580445169442\n",
      "Epoch:  258 Training loss: 0.0005518 Validation loss = 0.02101, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.02089486025643965\n",
      "Epoch:  259 Training loss: 0.0006189 Validation loss = 0.02089, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  260 Training loss: 0.0005278 Validation loss = 0.02362, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  261 Training loss: 0.0005345 Validation loss = 0.02367, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0205608287174742\n",
      "Epoch:  262 Training loss: 0.0004953 Validation loss = 0.02056, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  263 Training loss: 0.0005511 Validation loss = 0.02369, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  264 Training loss: 0.0006359 Validation loss = 0.02507, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  265 Training loss: 0.0004775 Validation loss = 0.02661, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.02023761590135957\n",
      "Epoch:  266 Training loss: 0.0004881 Validation loss = 0.02024, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  267 Training loss: 0.0004933 Validation loss = 0.02494, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  268 Training loss: 0.0005309 Validation loss = 0.02127, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  269 Training loss: 0.0004893 Validation loss = 0.03603, time Loss: 31.8%, back: 30.6%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## new best ########### 0.019589027459725773\n",
      "Epoch:  270 Training loss: 0.0005039 Validation loss = 0.01959, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  271 Training loss: 0.0005379 Validation loss = 0.02042, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  272 Training loss: 0.0004559 Validation loss = 0.02259, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  273 Training loss: 0.0005064 Validation loss = 0.01967, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  274 Training loss: 0.0004633 Validation loss = 0.02127, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.018889385708277404\n",
      "Epoch:  275 Training loss: 0.0005143 Validation loss = 0.01889, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  276 Training loss: 0.0004536 Validation loss = 0.02097, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  277 Training loss: 0.0004643 Validation loss = 0.02161, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  278 Training loss: 0.0005569 Validation loss = 0.02004, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  279 Training loss: 0.0003933 Validation loss = 0.0193, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  280 Training loss: 0.0004464 Validation loss = 0.02208, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  281 Training loss: 0.0004405 Validation loss = 0.02118, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  282 Training loss: 0.000486 Validation loss = 0.01989, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  283 Training loss: 0.0005572 Validation loss = 0.02039, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  284 Training loss: 0.0003719 Validation loss = 0.01959, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  285 Training loss: 0.000493 Validation loss = 0.0192, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.018344101359608043\n",
      "Epoch:  286 Training loss: 0.00037 Validation loss = 0.01834, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  287 Training loss: 0.0004294 Validation loss = 0.01985, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.018014113739074803\n",
      "Epoch:  288 Training loss: 0.0004481 Validation loss = 0.01801, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  289 Training loss: 0.0004162 Validation loss = 0.02212, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  290 Training loss: 0.0004327 Validation loss = 0.02165, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  291 Training loss: 0.0004288 Validation loss = 0.02184, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  292 Training loss: 0.0004138 Validation loss = 0.02201, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  293 Training loss: 0.0004186 Validation loss = 0.01853, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  294 Training loss: 0.0004514 Validation loss = 0.02206, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  295 Training loss: 0.0003599 Validation loss = 0.01904, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  296 Training loss: 0.0004316 Validation loss = 0.02243, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.017614418557852358\n",
      "Epoch:  297 Training loss: 0.0004081 Validation loss = 0.01761, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  298 Training loss: 0.0003789 Validation loss = 0.02018, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  299 Training loss: 0.0004317 Validation loss = 0.02198, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  300 Training loss: 0.0004083 Validation loss = 0.02604, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  301 Training loss: 0.0004084 Validation loss = 0.01885, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  302 Training loss: 0.0004247 Validation loss = 0.01944, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:  303 Training loss: 0.0003391 Validation loss = 0.01905, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch:  304 Training loss: 0.0003886 Validation loss = 0.01819, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.017311550541474523\n",
      "Epoch:  305 Training loss: 0.0003883 Validation loss = 0.01731, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.017229964758417714\n",
      "Epoch:  306 Training loss: 0.0003956 Validation loss = 0.01723, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  307 Training loss: 0.0003811 Validation loss = 0.01764, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  308 Training loss: 0.0003887 Validation loss = 0.03142, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  309 Training loss: 0.0003658 Validation loss = 0.02335, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  310 Training loss: 0.0003965 Validation loss = 0.01897, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  311 Training loss: 0.0003752 Validation loss = 0.02298, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  312 Training loss: 0.0004334 Validation loss = 0.01966, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  313 Training loss: 0.0003014 Validation loss = 0.01738, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  314 Training loss: 0.0003904 Validation loss = 0.01723, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.017008887958525298\n",
      "Epoch:  315 Training loss: 0.0003606 Validation loss = 0.01701, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.01651349294133408\n",
      "Epoch:  316 Training loss: 0.0003557 Validation loss = 0.01651, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  317 Training loss: 0.0003681 Validation loss = 0.01762, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  318 Training loss: 0.0003428 Validation loss = 0.01778, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  319 Training loss: 0.0003629 Validation loss = 0.01881, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  320 Training loss: 0.0004157 Validation loss = 0.03023, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  321 Training loss: 0.0003275 Validation loss = 0.01728, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.015977754850505427\n",
      "Epoch:  322 Training loss: 0.000327 Validation loss = 0.01598, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  323 Training loss: 0.0003597 Validation loss = 0.01741, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  324 Training loss: 0.0003613 Validation loss = 0.02245, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  325 Training loss: 0.0003386 Validation loss = 0.02463, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  326 Training loss: 0.0003384 Validation loss = 0.01814, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.01541707130334136\n",
      "Epoch:  327 Training loss: 0.000346 Validation loss = 0.01542, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  328 Training loss: 0.0003251 Validation loss = 0.01741, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  329 Training loss: 0.000352 Validation loss = 0.01609, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  330 Training loss: 0.0003395 Validation loss = 0.01878, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  331 Training loss: 0.0003805 Validation loss = 0.0354, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  332 Training loss: 0.0003192 Validation loss = 0.01628, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  333 Training loss: 0.0003123 Validation loss = 0.01655, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  334 Training loss: 0.0003712 Validation loss = 0.01671, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  335 Training loss: 0.0002981 Validation loss = 0.02517, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  336 Training loss: 0.0003271 Validation loss = 0.01627, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  337 Training loss: 0.0003248 Validation loss = 0.01599, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  338 Training loss: 0.0003287 Validation loss = 0.01746, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.015072084091837565\n",
      "Epoch:  339 Training loss: 0.0003342 Validation loss = 0.01507, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  340 Training loss: 0.0003165 Validation loss = 0.02816, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  341 Training loss: 0.0003227 Validation loss = 0.02898, time Loss: 31.8%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  342 Training loss: 0.0003404 Validation loss = 0.01889, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  343 Training loss: 0.0003079 Validation loss = 0.02947, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  344 Training loss: 0.0003203 Validation loss = 0.01634, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  345 Training loss: 0.0003669 Validation loss = 0.01804, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  346 Training loss: 0.000264 Validation loss = 0.0171, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  347 Training loss: 0.0003159 Validation loss = 0.03626, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  348 Training loss: 0.0003014 Validation loss = 0.01706, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  349 Training loss: 0.0003066 Validation loss = 0.02898, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  350 Training loss: 0.0002976 Validation loss = 0.01631, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.014806250166311496\n",
      "Epoch:  351 Training loss: 0.0003313 Validation loss = 0.01481, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  352 Training loss: 0.000306 Validation loss = 0.01537, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  353 Training loss: 0.000332 Validation loss = 0.0157, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  354 Training loss: 0.0002709 Validation loss = 0.01502, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  355 Training loss: 0.0003458 Validation loss = 0.01502, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  356 Training loss: 0.0002666 Validation loss = 0.01556, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.014539122536684329\n",
      "Epoch:  357 Training loss: 0.0003116 Validation loss = 0.01454, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  358 Training loss: 0.0003197 Validation loss = 0.01651, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  359 Training loss: 0.0002824 Validation loss = 0.01516, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  360 Training loss: 0.0003491 Validation loss = 0.01481, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  361 Training loss: 0.0002741 Validation loss = 0.0148, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  362 Training loss: 0.0003654 Validation loss = 0.01842, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  363 Training loss: 0.0002255 Validation loss = 0.0158, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  364 Training loss: 0.0002764 Validation loss = 0.01592, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  365 Training loss: 0.0003024 Validation loss = 0.01476, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  366 Training loss: 0.0003441 Validation loss = 0.01666, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  367 Training loss: 0.0002627 Validation loss = 0.02009, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  368 Training loss: 0.0002699 Validation loss = 0.01524, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  369 Training loss: 0.0002889 Validation loss = 0.01646, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  370 Training loss: 0.0003857 Validation loss = 0.01456, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.014396978581288954\n",
      "Epoch:  371 Training loss: 0.0002051 Validation loss = 0.0144, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  372 Training loss: 0.000284 Validation loss = 0.02076, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  373 Training loss: 0.0002752 Validation loss = 0.01485, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  374 Training loss: 0.0003198 Validation loss = 0.01456, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  375 Training loss: 0.0002291 Validation loss = 0.02057, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  376 Training loss: 0.0003167 Validation loss = 0.0225, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  377 Training loss: 0.00025 Validation loss = 0.01758, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.01429670326163281\n",
      "Epoch:  378 Training loss: 0.0003005 Validation loss = 0.0143, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  379 Training loss: 0.0003121 Validation loss =  0.016, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  380 Training loss: 0.0002583 Validation loss = 0.01689, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.014027859518348862\n",
      "Epoch:  381 Training loss: 0.0002569 Validation loss = 0.01403, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  382 Training loss: 0.0003059 Validation loss = 0.01629, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  383 Training loss: 0.0002409 Validation loss = 0.01536, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.013852866226954284\n",
      "Epoch:  384 Training loss: 0.0002919 Validation loss = 0.01385, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  385 Training loss: 0.0003089 Validation loss = 0.01466, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  386 Training loss: 0.0002405 Validation loss = 0.02673, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  387 Training loss: 0.0002556 Validation loss = 0.02193, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  388 Training loss: 0.0002866 Validation loss = 0.02749, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  389 Training loss: 0.0003744 Validation loss = 0.02328, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  390 Training loss: 0.0002302 Validation loss = 0.01566, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  391 Training loss: 0.0002146 Validation loss = 0.01714, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  392 Training loss: 0.0002738 Validation loss = 0.01672, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  393 Training loss: 0.0002658 Validation loss = 0.01473, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "########## new best ########### 0.01377383308175506\n",
      "Epoch:  394 Training loss: 0.0002539 Validation loss = 0.01377, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  395 Training loss: 0.0002898 Validation loss = 0.01453, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.013757448019623015\n",
      "Epoch:  396 Training loss: 0.0002525 Validation loss = 0.01376, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  397 Training loss: 0.0003015 Validation loss = 0.01449, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  398 Training loss: 0.0002861 Validation loss = 0.01652, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  399 Training loss: 0.0002245 Validation loss = 0.01613, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  400 Training loss: 0.000271 Validation loss = 0.01545, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  401 Training loss: 0.0002345 Validation loss = 0.01397, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  402 Training loss: 0.0002725 Validation loss = 0.01499, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  403 Training loss: 0.0003273 Validation loss = 0.01579, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  404 Training loss: 0.0001994 Validation loss = 0.01456, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  405 Training loss: 0.0002344 Validation loss = 0.03534, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "########## new best ########### 0.013197932290605464\n",
      "Epoch:  406 Training loss: 0.0002768 Validation loss = 0.0132, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  407 Training loss: 0.0002328 Validation loss = 0.01674, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  408 Training loss: 0.0002799 Validation loss = 0.01417, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  409 Training loss: 0.000245 Validation loss = 0.01494, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  410 Training loss: 0.0002439 Validation loss = 0.01546, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  411 Training loss: 0.0002946 Validation loss = 0.01456, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "########## new best ########### 0.013064762795311458\n",
      "Epoch:  412 Training loss: 0.0002216 Validation loss = 0.01306, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  413 Training loss: 0.0002952 Validation loss = 0.01358, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  414 Training loss: 0.0002508 Validation loss = 0.01825, time Loss: 31.8%, back: 30.6%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  415 Training loss: 0.0002251 Validation loss = 0.01476, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  416 Training loss: 0.0002725 Validation loss = 0.01404, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  417 Training loss: 0.0002159 Validation loss = 0.01744, time Loss: 31.8%, back: 30.6%, val: 34.9%\n",
      "Epoch:  418 Training loss: 0.000275 Validation loss = 0.01497, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  419 Training loss: 0.0002297 Validation loss = 0.0252, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  420 Training loss: 0.0003013 Validation loss = 0.01715, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  421 Training loss: 0.0002015 Validation loss = 0.01694, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  422 Training loss: 0.0002904 Validation loss = 0.02051, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  423 Training loss: 0.000209 Validation loss = 0.0134, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  424 Training loss: 0.0002941 Validation loss = 0.01328, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  425 Training loss: 0.0001999 Validation loss = 0.01919, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  426 Training loss: 0.0002422 Validation loss = 0.01438, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  427 Training loss: 0.00025 Validation loss = 0.01437, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  428 Training loss: 0.0002532 Validation loss = 0.01937, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  429 Training loss: 0.0002267 Validation loss = 0.01712, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  430 Training loss: 0.0002653 Validation loss = 0.02072, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  431 Training loss: 0.0002316 Validation loss = 0.0177, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  432 Training loss: 0.0002257 Validation loss = 0.01575, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  433 Training loss: 0.0002355 Validation loss = 0.01431, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  434 Training loss: 0.000243 Validation loss = 0.02203, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.012731571076599012\n",
      "Epoch:  435 Training loss: 0.0002252 Validation loss = 0.01273, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  436 Training loss: 0.0002612 Validation loss = 0.01619, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.012471144057607542\n",
      "Epoch:  437 Training loss: 0.0002314 Validation loss = 0.01247, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  438 Training loss: 0.0002317 Validation loss = 0.01265, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  439 Training loss: 0.000247 Validation loss = 0.01419, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  440 Training loss: 0.0002314 Validation loss = 0.01364, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  441 Training loss: 0.0002555 Validation loss = 0.01319, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  442 Training loss: 0.0002521 Validation loss = 0.01427, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  443 Training loss: 0.0001988 Validation loss = 0.0133, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  444 Training loss: 0.0002306 Validation loss = 0.01376, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  445 Training loss: 0.0002398 Validation loss = 0.01469, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  446 Training loss: 0.0002171 Validation loss = 0.01714, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  447 Training loss: 0.0002405 Validation loss = 0.01346, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  448 Training loss: 0.0002198 Validation loss = 0.01732, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  449 Training loss: 0.0002426 Validation loss = 0.03317, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  450 Training loss: 0.0002397 Validation loss = 0.01334, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  451 Training loss: 0.0002339 Validation loss = 0.01255, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  452 Training loss: 0.0002413 Validation loss = 0.01353, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  453 Training loss: 0.0002071 Validation loss = 0.02117, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  454 Training loss: 0.0002339 Validation loss = 0.01356, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  455 Training loss: 0.0002519 Validation loss = 0.01279, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.012428043397249615\n",
      "Epoch:  456 Training loss: 0.0002015 Validation loss = 0.01243, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  457 Training loss: 0.0002478 Validation loss = 0.01464, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  458 Training loss: 0.0002022 Validation loss = 0.01547, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  459 Training loss: 0.0002141 Validation loss = 0.01495, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  460 Training loss: 0.0002486 Validation loss = 0.01389, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  461 Training loss: 0.0002225 Validation loss = 0.01793, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  462 Training loss: 0.0002131 Validation loss = 0.01453, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  463 Training loss: 0.0003652 Validation loss = 0.01559, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  464 Training loss: 0.0001813 Validation loss = 0.0129, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  465 Training loss: 0.0001644 Validation loss = 0.01425, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  466 Training loss: 0.0002066 Validation loss = 0.01351, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  467 Training loss: 0.0002207 Validation loss = 0.01422, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  468 Training loss: 0.0002171 Validation loss = 0.01336, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  469 Training loss: 0.0002592 Validation loss = 0.03473, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  470 Training loss: 0.0002317 Validation loss = 0.01246, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.012127486131345991\n",
      "Epoch:  471 Training loss: 0.0002205 Validation loss = 0.01213, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  472 Training loss: 0.0001953 Validation loss = 0.01361, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  473 Training loss: 0.000211 Validation loss = 0.01394, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  474 Training loss: 0.0002045 Validation loss = 0.01397, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  475 Training loss: 0.0002404 Validation loss = 0.01322, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  476 Training loss: 0.0002014 Validation loss = 0.0133, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  477 Training loss: 0.0002317 Validation loss = 0.01537, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  478 Training loss: 0.0002166 Validation loss = 0.01651, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  479 Training loss: 0.0002429 Validation loss = 0.01548, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  480 Training loss: 0.0001943 Validation loss = 0.01536, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  481 Training loss: 0.0001947 Validation loss = 0.01367, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  482 Training loss: 0.0002163 Validation loss = 0.01353, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.011973134152091027\n",
      "Epoch:  483 Training loss: 0.0002321 Validation loss = 0.01197, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  484 Training loss: 0.0002091 Validation loss = 0.01286, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  485 Training loss: 0.0002215 Validation loss = 0.01326, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  486 Training loss: 0.0002174 Validation loss = 0.01417, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  487 Training loss: 0.0001873 Validation loss = 0.01308, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  488 Training loss: 0.0002304 Validation loss = 0.01302, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.011949979554136817\n",
      "Epoch:  489 Training loss: 0.0002095 Validation loss = 0.01195, time Loss: 31.8%, back: 30.6%, val: 35.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  490 Training loss: 0.0002204 Validation loss = 0.01707, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  491 Training loss: 0.000203 Validation loss = 0.01449, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  492 Training loss: 0.0002149 Validation loss = 0.01463, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  493 Training loss: 0.0002278 Validation loss = 0.01238, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  494 Training loss: 0.0002226 Validation loss = 0.01219, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  495 Training loss: 0.0001822 Validation loss = 0.01203, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  496 Training loss: 0.0002719 Validation loss = 0.01262, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  497 Training loss: 0.0001799 Validation loss = 0.01328, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  498 Training loss: 0.0002047 Validation loss = 0.01255, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.011839185348392824\n",
      "Epoch:  499 Training loss: 0.0001905 Validation loss = 0.01184, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  500 Training loss: 0.0002029 Validation loss = 0.01269, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  501 Training loss: 0.0002006 Validation loss = 0.01333, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  502 Training loss: 0.0002088 Validation loss = 0.01426, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  503 Training loss: 0.0002018 Validation loss = 0.01495, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  504 Training loss: 0.000211 Validation loss = 0.01323, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  505 Training loss: 0.0002132 Validation loss = 0.01432, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  506 Training loss: 0.0002062 Validation loss = 0.01798, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  507 Training loss: 0.0002236 Validation loss = 0.01605, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  508 Training loss: 0.0001657 Validation loss = 0.01211, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  509 Training loss: 0.0002104 Validation loss = 0.01429, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.011530355756936159\n",
      "Epoch:  510 Training loss: 0.0002183 Validation loss = 0.01153, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  511 Training loss: 0.000233 Validation loss = 0.01976, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  512 Training loss: 0.0001853 Validation loss = 0.01728, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  513 Training loss: 0.0001873 Validation loss = 0.01895, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  514 Training loss: 0.0002071 Validation loss = 0.01504, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  515 Training loss: 0.0002476 Validation loss = 0.02167, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  516 Training loss: 0.000178 Validation loss = 0.01196, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  517 Training loss: 0.0001917 Validation loss = 0.01257, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  518 Training loss: 0.0001993 Validation loss = 0.01353, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  519 Training loss: 0.0003102 Validation loss = 0.01281, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  520 Training loss: 0.0001525 Validation loss = 0.01216, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  521 Training loss: 0.0001516 Validation loss = 0.01626, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  522 Training loss: 0.0001834 Validation loss = 0.01173, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  523 Training loss: 0.0003245 Validation loss = 0.01167, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  524 Training loss: 0.0001442 Validation loss = 0.0123, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  525 Training loss: 0.0001556 Validation loss = 0.01403, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  526 Training loss: 0.000255 Validation loss =  0.012, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  527 Training loss: 0.0001545 Validation loss = 0.01402, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  528 Training loss: 0.0002003 Validation loss = 0.02915, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.011529449262001735\n",
      "Epoch:  529 Training loss: 0.0001882 Validation loss = 0.01153, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  530 Training loss: 0.0001961 Validation loss = 0.01307, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  531 Training loss: 0.0002554 Validation loss = 0.01425, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  532 Training loss: 0.0001878 Validation loss = 0.01154, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  533 Training loss: 0.0001599 Validation loss = 0.01358, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  534 Training loss: 0.0002251 Validation loss = 0.01639, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.011341000321785116\n",
      "Epoch:  535 Training loss: 0.0001879 Validation loss = 0.01134, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  536 Training loss: 0.0002042 Validation loss = 0.01161, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  537 Training loss: 0.0001663 Validation loss = 0.02341, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  538 Training loss: 0.000235 Validation loss = 0.0146, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  539 Training loss: 0.0001677 Validation loss = 0.01912, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  540 Training loss: 0.0001799 Validation loss = 0.01216, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  541 Training loss: 0.0002989 Validation loss = 0.01155, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  542 Training loss: 0.000135 Validation loss = 0.01141, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  543 Training loss: 0.0001863 Validation loss = 0.01324, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  544 Training loss: 0.0001695 Validation loss = 0.01548, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  545 Training loss: 0.0002027 Validation loss = 0.01235, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  546 Training loss: 0.0001937 Validation loss = 0.01607, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  547 Training loss: 0.0001942 Validation loss = 0.01639, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  548 Training loss: 0.0001921 Validation loss = 0.01604, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  549 Training loss: 0.0002173 Validation loss = 0.01318, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  550 Training loss: 0.0001633 Validation loss = 0.01166, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  551 Training loss: 0.000165 Validation loss = 0.01548, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  552 Training loss: 0.0002036 Validation loss = 0.01711, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  553 Training loss: 0.0001821 Validation loss = 0.01183, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  554 Training loss: 0.0002116 Validation loss = 0.01287, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  555 Training loss: 0.0001771 Validation loss = 0.01314, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  556 Training loss: 0.0001911 Validation loss = 0.01649, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  557 Training loss: 0.0002142 Validation loss = 0.01209, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  558 Training loss: 0.000178 Validation loss = 0.01556, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  559 Training loss: 0.000179 Validation loss = 0.01605, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.011106605592320674\n",
      "Epoch:  560 Training loss: 0.0001588 Validation loss = 0.01111, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  561 Training loss: 0.0002048 Validation loss = 0.0166, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  562 Training loss: 0.0001898 Validation loss = 0.01177, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  563 Training loss: 0.0001812 Validation loss = 0.0136, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  564 Training loss: 0.0002061 Validation loss = 0.01432, time Loss: 31.8%, back: 30.6%, val: 35.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## new best ########### 0.011069404987750484\n",
      "Epoch:  565 Training loss: 0.0001648 Validation loss = 0.01107, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  566 Training loss: 0.0001688 Validation loss = 0.01125, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  567 Training loss: 0.000206 Validation loss = 0.01192, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  568 Training loss: 0.0001768 Validation loss = 0.01828, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  569 Training loss: 0.0001928 Validation loss = 0.01339, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  570 Training loss: 0.000171 Validation loss = 0.02696, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  571 Training loss: 0.0001982 Validation loss = 0.01355, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  572 Training loss: 0.0001848 Validation loss = 0.01244, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.011053590386579795\n",
      "Epoch:  573 Training loss: 0.0001775 Validation loss = 0.01105, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  574 Training loss: 0.0001911 Validation loss = 0.0117, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  575 Training loss: 0.0001877 Validation loss = 0.01169, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  576 Training loss: 0.0001979 Validation loss = 0.01536, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  577 Training loss: 0.0001629 Validation loss = 0.01123, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  578 Training loss: 0.0002265 Validation loss = 0.01287, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  579 Training loss: 0.0001329 Validation loss = 0.01201, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  580 Training loss: 0.0001938 Validation loss = 0.0119, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  581 Training loss: 0.0001779 Validation loss = 0.0142, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  582 Training loss: 0.0001997 Validation loss = 0.01161, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  583 Training loss: 0.0001652 Validation loss = 0.0168, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  584 Training loss: 0.0001877 Validation loss = 0.01521, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  585 Training loss: 0.0001827 Validation loss = 0.01275, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  586 Training loss: 0.000167 Validation loss = 0.01132, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  587 Training loss: 0.0001917 Validation loss = 0.01324, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.010912208341640703\n",
      "Epoch:  588 Training loss: 0.0001753 Validation loss = 0.01091, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  589 Training loss: 0.0001596 Validation loss = 0.02329, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  590 Training loss: 0.0001946 Validation loss = 0.0211, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  591 Training loss: 0.0001559 Validation loss = 0.01342, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  592 Training loss: 0.0001819 Validation loss = 0.01254, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  593 Training loss: 0.0001898 Validation loss = 0.01191, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  594 Training loss: 0.0001939 Validation loss = 0.01268, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  595 Training loss: 0.0001743 Validation loss = 0.01483, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  596 Training loss: 0.0001729 Validation loss = 0.01171, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  597 Training loss: 0.0001722 Validation loss = 0.01174, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  598 Training loss: 0.0001729 Validation loss = 0.01136, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  599 Training loss: 0.000182 Validation loss = 0.01345, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  600 Training loss: 0.0001784 Validation loss = 0.01275, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  601 Training loss: 0.0002171 Validation loss = 0.01345, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  602 Training loss: 0.0001362 Validation loss = 0.01287, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  603 Training loss: 0.0001715 Validation loss = 0.0122, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  604 Training loss: 0.0001826 Validation loss = 0.01165, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  605 Training loss: 0.0001949 Validation loss = 0.01556, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  606 Training loss: 0.0001567 Validation loss = 0.01147, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  607 Training loss: 0.000178 Validation loss = 0.01418, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  608 Training loss: 0.0001713 Validation loss = 0.01109, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  609 Training loss: 0.0001709 Validation loss = 0.01105, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  610 Training loss: 0.0001665 Validation loss = 0.01124, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  611 Training loss: 0.0002361 Validation loss = 0.01471, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "########## new best ########### 0.010708609733200397\n",
      "Epoch:  612 Training loss: 0.0001278 Validation loss = 0.01071, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  613 Training loss: 0.000173 Validation loss = 0.01552, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  614 Training loss: 0.0001761 Validation loss = 0.01123, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  615 Training loss: 0.0001616 Validation loss = 0.01807, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  616 Training loss: 0.0001776 Validation loss = 0.01179, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  617 Training loss: 0.000157 Validation loss = 0.01134, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  618 Training loss: 0.0002366 Validation loss = 0.01272, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  619 Training loss: 0.0001367 Validation loss = 0.01077, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  620 Training loss: 0.0001573 Validation loss = 0.01315, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  621 Training loss: 0.0001653 Validation loss = 0.0136, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  622 Training loss: 0.0001751 Validation loss = 0.01259, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  623 Training loss: 0.0001788 Validation loss = 0.01133, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  624 Training loss: 0.0001801 Validation loss = 0.01073, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  625 Training loss: 0.0001549 Validation loss = 0.01236, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  626 Training loss: 0.0002519 Validation loss = 0.01145, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  627 Training loss: 0.0001253 Validation loss = 0.01075, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  628 Training loss: 0.0001473 Validation loss = 0.01363, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  629 Training loss: 0.0001894 Validation loss = 0.01253, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  630 Training loss: 0.0001596 Validation loss = 0.0232, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  631 Training loss: 0.0001623 Validation loss = 0.01168, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  632 Training loss: 0.0001801 Validation loss = 0.01167, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  633 Training loss: 0.0001737 Validation loss = 0.01607, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  634 Training loss: 0.0001666 Validation loss = 0.0109, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  635 Training loss: 0.0001581 Validation loss = 0.01088, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  636 Training loss: 0.0001749 Validation loss = 0.01118, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  637 Training loss: 0.0001636 Validation loss = 0.01153, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  638 Training loss: 0.0001767 Validation loss = 0.0258, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  639 Training loss: 0.0001518 Validation loss = 0.01168, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  640 Training loss: 0.0001821 Validation loss = 0.01159, time Loss: 31.8%, back: 30.7%, val: 35.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  641 Training loss: 0.0001808 Validation loss = 0.01154, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  642 Training loss: 0.0001344 Validation loss = 0.01108, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  643 Training loss: 0.0001743 Validation loss = 0.01291, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  644 Training loss: 0.0001764 Validation loss = 0.0111, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  645 Training loss: 0.00016 Validation loss = 0.01232, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  646 Training loss: 0.0001796 Validation loss = 0.01075, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  647 Training loss: 0.0001543 Validation loss = 0.01368, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  648 Training loss: 0.0001717 Validation loss = 0.01273, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.010563104995219608\n",
      "Epoch:  649 Training loss: 0.0001699 Validation loss = 0.01056, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  650 Training loss: 0.0001633 Validation loss = 0.01916, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  651 Training loss: 0.0001569 Validation loss = 0.01169, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.010467059473744964\n",
      "Epoch:  652 Training loss: 0.0001879 Validation loss = 0.01047, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  653 Training loss: 0.0001594 Validation loss = 0.01306, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  654 Training loss: 0.0001502 Validation loss = 0.01255, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  655 Training loss: 0.0001637 Validation loss = 0.01519, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  656 Training loss: 0.0001905 Validation loss = 0.01376, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  657 Training loss: 0.0001548 Validation loss = 0.01463, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  658 Training loss: 0.0001639 Validation loss = 0.01109, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  659 Training loss: 0.0001748 Validation loss = 0.01183, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  660 Training loss: 0.00014 Validation loss = 0.0119, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  661 Training loss: 0.0001722 Validation loss = 0.01202, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  662 Training loss: 0.000157 Validation loss = 0.01846, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch:  663 Training loss: 0.0001525 Validation loss = 0.01174, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  664 Training loss: 0.0001846 Validation loss = 0.01258, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  665 Training loss: 0.0001484 Validation loss = 0.01486, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  666 Training loss: 0.0001684 Validation loss = 0.01778, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  667 Training loss: 0.0001733 Validation loss = 0.01655, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  668 Training loss: 0.0001545 Validation loss = 0.0121, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  669 Training loss: 0.0001707 Validation loss = 0.01608, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  670 Training loss: 0.0001658 Validation loss = 0.01625, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  671 Training loss: 0.0001413 Validation loss =  0.013, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  672 Training loss: 0.000163 Validation loss = 0.01097, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  673 Training loss: 0.0001649 Validation loss =  0.013, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  674 Training loss: 0.0001657 Validation loss = 0.01126, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch:  675 Training loss: 0.0001553 Validation loss = 0.01395, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  676 Training loss: 0.0001797 Validation loss = 0.01146, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  677 Training loss: 0.0001332 Validation loss = 0.01271, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  678 Training loss: 0.0001665 Validation loss = 0.01299, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  679 Training loss: 0.0001917 Validation loss = 0.01073, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  680 Training loss: 0.0001245 Validation loss = 0.01392, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  681 Training loss: 0.0001661 Validation loss = 0.01061, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  682 Training loss: 0.0001596 Validation loss = 0.0114, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  683 Training loss: 0.0001641 Validation loss = 0.01177, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  684 Training loss: 0.0001491 Validation loss = 0.01052, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  685 Training loss: 0.0001823 Validation loss = 0.01206, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  686 Training loss: 0.0001274 Validation loss = 0.01352, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  687 Training loss: 0.0001804 Validation loss = 0.01097, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  688 Training loss: 0.0001574 Validation loss = 0.01089, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  689 Training loss: 0.0001557 Validation loss = 0.0128, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  690 Training loss: 0.0001813 Validation loss = 0.0112, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  691 Training loss: 0.0001398 Validation loss = 0.01333, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  692 Training loss: 0.0001471 Validation loss = 0.01088, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  693 Training loss: 0.000174 Validation loss = 0.01254, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  694 Training loss: 0.0001567 Validation loss = 0.01115, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  695 Training loss: 0.0001527 Validation loss = 0.01072, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  696 Training loss: 0.0001572 Validation loss = 0.01298, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  697 Training loss: 0.000171 Validation loss = 0.0107, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  698 Training loss: 0.0001472 Validation loss = 0.01123, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  699 Training loss: 0.0001528 Validation loss = 0.01983, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  700 Training loss: 0.0001591 Validation loss = 0.01205, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  701 Training loss: 0.0001632 Validation loss = 0.01422, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  702 Training loss: 0.0001382 Validation loss = 0.01421, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  703 Training loss: 0.00019 Validation loss = 0.01089, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  704 Training loss: 0.000136 Validation loss = 0.01266, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  705 Training loss: 0.0001534 Validation loss = 0.0112, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  706 Training loss: 0.0001686 Validation loss = 0.01526, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  707 Training loss: 0.0001317 Validation loss = 0.01079, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  708 Training loss: 0.0001656 Validation loss = 0.01155, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  709 Training loss: 0.0001467 Validation loss = 0.01149, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  710 Training loss: 0.0001587 Validation loss = 0.01185, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  711 Training loss: 0.0001568 Validation loss = 0.01399, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  712 Training loss: 0.0001486 Validation loss = 0.01852, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  713 Training loss: 0.0001643 Validation loss = 0.01217, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  714 Training loss: 0.0001576 Validation loss = 0.01139, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  715 Training loss: 0.0001414 Validation loss = 0.01355, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  716 Training loss: 0.0001679 Validation loss = 0.01747, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  717 Training loss: 0.0001374 Validation loss = 0.01241, time Loss: 31.8%, back: 30.6%, val: 35.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## new best ########### 0.010321297543301203\n",
      "Epoch:  718 Training loss: 0.0001666 Validation loss = 0.01032, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  719 Training loss: 0.0001757 Validation loss = 0.01072, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  720 Training loss: 0.0001347 Validation loss = 0.01063, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  721 Training loss: 0.0001583 Validation loss = 0.01782, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  722 Training loss: 0.0001561 Validation loss = 0.01203, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  723 Training loss: 0.0001658 Validation loss = 0.01076, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  724 Training loss: 0.0001399 Validation loss = 0.01436, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  725 Training loss: 0.0001604 Validation loss = 0.01276, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  726 Training loss: 0.000147 Validation loss = 0.01129, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  727 Training loss: 0.0001485 Validation loss = 0.0107, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  728 Training loss: 0.000145 Validation loss = 0.01047, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  729 Training loss: 0.0001677 Validation loss = 0.01186, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  730 Training loss: 0.0001438 Validation loss = 0.01981, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  731 Training loss: 0.0001485 Validation loss = 0.01078, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.010282242263310771\n",
      "Epoch:  732 Training loss: 0.000149 Validation loss = 0.01028, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  733 Training loss: 0.0001509 Validation loss = 0.01166, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  734 Training loss: 0.0001439 Validation loss = 0.01076, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  735 Training loss: 0.000152 Validation loss = 0.01421, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  736 Training loss: 0.0001436 Validation loss = 0.0141, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  737 Training loss: 0.0001761 Validation loss = 0.01518, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  738 Training loss: 0.0001279 Validation loss = 0.01448, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  739 Training loss: 0.0001619 Validation loss = 0.01126, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  740 Training loss: 0.0001385 Validation loss = 0.0177, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  741 Training loss: 0.0001517 Validation loss = 0.01081, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  742 Training loss: 0.0001537 Validation loss = 0.01047, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  743 Training loss: 0.0001563 Validation loss = 0.01149, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  744 Training loss: 0.0001429 Validation loss = 0.01093, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  745 Training loss: 0.0001535 Validation loss = 0.01518, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  746 Training loss: 0.0001572 Validation loss = 0.01111, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  747 Training loss: 0.0001402 Validation loss = 0.01219, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.010014696476271229\n",
      "Epoch:  748 Training loss: 0.0001506 Validation loss = 0.01001, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  749 Training loss: 0.0001392 Validation loss = 0.01355, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  750 Training loss: 0.0001507 Validation loss = 0.01388, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  751 Training loss: 0.0001711 Validation loss = 0.01016, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  752 Training loss: 0.000133 Validation loss = 0.01162, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  753 Training loss: 0.0001513 Validation loss = 0.01149, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  754 Training loss: 0.0001604 Validation loss = 0.01745, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  755 Training loss: 0.0001294 Validation loss = 0.01054, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  756 Training loss: 0.0001625 Validation loss = 0.01254, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  757 Training loss: 0.0001376 Validation loss = 0.01392, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  758 Training loss: 0.0001557 Validation loss = 0.01129, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  759 Training loss: 0.0001623 Validation loss = 0.01044, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  760 Training loss: 0.0001247 Validation loss = 0.01239, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  761 Training loss: 0.000154 Validation loss = 0.01102, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  762 Training loss: 0.0001468 Validation loss = 0.01284, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  763 Training loss: 0.0001608 Validation loss = 0.01178, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  764 Training loss: 0.0001426 Validation loss = 0.01284, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  765 Training loss: 0.0001546 Validation loss = 0.01091, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  766 Training loss: 0.000134 Validation loss = 0.01066, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  767 Training loss: 0.0001742 Validation loss = 0.01055, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  768 Training loss: 0.0001191 Validation loss = 0.0101, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  769 Training loss: 0.0001547 Validation loss = 0.01331, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  770 Training loss: 0.0001355 Validation loss = 0.0112, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  771 Training loss: 0.0001663 Validation loss = 0.01035, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  772 Training loss: 0.0001449 Validation loss = 0.01009, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  773 Training loss: 0.000119 Validation loss = 0.01062, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  774 Training loss: 0.0001629 Validation loss = 0.01274, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  775 Training loss: 0.0001471 Validation loss = 0.01153, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  776 Training loss: 0.0001359 Validation loss = 0.01083, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  777 Training loss: 0.0001475 Validation loss = 0.01204, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  778 Training loss: 0.0001597 Validation loss = 0.01046, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  779 Training loss: 0.000125 Validation loss = 0.02295, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  780 Training loss: 0.0001412 Validation loss = 0.01295, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.009986918807553867\n",
      "Epoch:  781 Training loss: 0.000147 Validation loss = 0.009987, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  782 Training loss: 0.0001589 Validation loss = 0.01175, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  783 Training loss: 0.0001375 Validation loss = 0.01099, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  784 Training loss: 0.0001469 Validation loss = 0.01762, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  785 Training loss: 0.0001342 Validation loss = 0.01275, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  786 Training loss: 0.0001515 Validation loss = 0.01046, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  787 Training loss: 0.0001495 Validation loss = 0.01009, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  788 Training loss: 0.0001414 Validation loss = 0.01061, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  789 Training loss: 0.0001395 Validation loss = 0.0152, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  790 Training loss: 0.0001564 Validation loss = 0.01209, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  791 Training loss: 0.0001366 Validation loss = 0.0102, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  792 Training loss: 0.0001484 Validation loss = 0.01338, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  793 Training loss: 0.0001684 Validation loss = 0.01304, time Loss: 31.8%, back: 30.6%, val: 35.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  794 Training loss: 0.0001181 Validation loss = 0.01551, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  795 Training loss: 0.0001497 Validation loss = 0.01634, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  796 Training loss: 0.0001308 Validation loss = 0.01107, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  797 Training loss: 0.00014 Validation loss = 0.01011, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  798 Training loss: 0.0001681 Validation loss = 0.01087, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  799 Training loss: 0.0001274 Validation loss = 0.01785, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  800 Training loss: 0.000144 Validation loss = 0.01303, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  801 Training loss: 0.0001421 Validation loss = 0.0113, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  802 Training loss: 0.0001454 Validation loss = 0.01273, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  803 Training loss: 0.0001579 Validation loss = 0.01073, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  804 Training loss: 0.0001292 Validation loss = 0.01139, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  805 Training loss: 0.0001393 Validation loss = 0.01389, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  806 Training loss: 0.0001443 Validation loss = 0.0115, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.009939719705450992\n",
      "Epoch:  807 Training loss: 0.0001374 Validation loss = 0.00994, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  808 Training loss: 0.0001424 Validation loss = 0.01006, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  809 Training loss: 0.0001293 Validation loss = 0.01021, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  810 Training loss: 0.0001488 Validation loss = 0.01231, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  811 Training loss: 0.0001334 Validation loss = 0.01765, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  812 Training loss: 0.0001785 Validation loss = 0.009986, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  813 Training loss: 0.0001134 Validation loss = 0.01053, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  814 Training loss: 0.0001589 Validation loss = 0.01125, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  815 Training loss: 0.0001294 Validation loss = 0.01341, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  816 Training loss: 0.0001642 Validation loss = 0.0119, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  817 Training loss: 0.0001145 Validation loss = 0.01088, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  818 Training loss: 0.0001467 Validation loss = 0.01551, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  819 Training loss: 0.0001805 Validation loss = 0.01577, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  820 Training loss: 0.0001208 Validation loss = 0.01056, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  821 Training loss: 0.0001201 Validation loss = 0.01353, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  822 Training loss: 0.0001487 Validation loss = 0.01102, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  823 Training loss: 0.0001291 Validation loss = 0.01036, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  824 Training loss: 0.0001466 Validation loss = 0.01052, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  825 Training loss: 0.0001343 Validation loss = 0.01209, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  826 Training loss: 0.0001299 Validation loss = 0.01629, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  827 Training loss: 0.0001526 Validation loss = 0.01062, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  828 Training loss: 0.000151 Validation loss = 0.01766, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  829 Training loss: 0.0001192 Validation loss = 0.01052, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  830 Training loss: 0.0001539 Validation loss = 0.0117, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  831 Training loss: 0.0001283 Validation loss = 0.01388, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  832 Training loss: 0.0001425 Validation loss = 0.01417, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  833 Training loss: 0.0001389 Validation loss = 0.01182, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  834 Training loss: 0.0001409 Validation loss = 0.01129, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  835 Training loss: 0.0001263 Validation loss = 0.01073, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.00981885722653931\n",
      "Epoch:  836 Training loss: 0.000155 Validation loss = 0.009819, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  837 Training loss: 0.000141 Validation loss = 0.01028, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  838 Training loss: 0.0001498 Validation loss = 0.01041, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  839 Training loss: 0.0001283 Validation loss = 0.01016, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  840 Training loss: 0.0001429 Validation loss = 0.01077, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  841 Training loss: 0.0001354 Validation loss = 0.01121, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  842 Training loss: 0.0001334 Validation loss = 0.01107, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  843 Training loss: 0.0001312 Validation loss = 0.01166, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  844 Training loss: 0.0001458 Validation loss = 0.01213, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  845 Training loss: 0.0001303 Validation loss = 0.01725, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  846 Training loss: 0.0001409 Validation loss = 0.01139, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  847 Training loss: 0.0001396 Validation loss = 0.0103, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  848 Training loss: 0.000141 Validation loss = 0.01538, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  849 Training loss: 0.0001421 Validation loss = 0.0109, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  850 Training loss: 0.0001379 Validation loss = 0.01308, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  851 Training loss: 0.0001312 Validation loss = 0.01047, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  852 Training loss: 0.000135 Validation loss = 0.01289, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  853 Training loss: 0.0001389 Validation loss = 0.01324, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  854 Training loss: 0.0001326 Validation loss = 0.01341, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  855 Training loss: 0.0001399 Validation loss = 0.01042, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  856 Training loss: 0.0001427 Validation loss = 0.01112, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  857 Training loss: 0.0001559 Validation loss = 0.01405, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  858 Training loss: 0.0001168 Validation loss = 0.01181, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  859 Training loss: 0.0001253 Validation loss = 0.01167, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  860 Training loss: 0.0001633 Validation loss = 0.01178, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  861 Training loss: 0.0001082 Validation loss = 0.01269, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  862 Training loss: 0.000148 Validation loss = 0.01033, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  863 Training loss: 0.0001273 Validation loss = 0.01078, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  864 Training loss: 0.0001504 Validation loss = 0.01076, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  865 Training loss: 0.0001289 Validation loss = 0.01126, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  866 Training loss: 0.0001317 Validation loss = 0.01143, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  867 Training loss: 0.0001422 Validation loss = 0.01096, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  868 Training loss: 0.000132 Validation loss = 0.01412, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  869 Training loss: 0.0001325 Validation loss = 0.01047, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  870 Training loss: 0.0001281 Validation loss = 0.01012, time Loss: 31.8%, back: 30.6%, val: 35.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  871 Training loss: 0.0001438 Validation loss = 0.01105, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  872 Training loss: 0.00013 Validation loss = 0.01228, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  873 Training loss: 0.0001527 Validation loss = 0.01165, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  874 Training loss: 0.0001257 Validation loss = 0.01292, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  875 Training loss: 0.0001288 Validation loss = 0.01061, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  876 Training loss: 0.0001416 Validation loss = 0.01504, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  877 Training loss: 0.0001341 Validation loss = 0.01049, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  878 Training loss: 0.0001224 Validation loss = 0.01635, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  879 Training loss: 0.0001417 Validation loss = 0.01069, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  880 Training loss: 0.0001326 Validation loss = 0.0103, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  881 Training loss: 0.0001347 Validation loss = 0.01052, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  882 Training loss: 0.0001251 Validation loss = 0.0105, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.009591472521238407\n",
      "Epoch:  883 Training loss: 0.0001523 Validation loss = 0.009591, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  884 Training loss: 0.000121 Validation loss = 0.01042, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  885 Training loss: 0.0001341 Validation loss = 0.01033, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  886 Training loss: 0.0001384 Validation loss = 0.01162, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  887 Training loss: 0.0001422 Validation loss = 0.01572, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  888 Training loss: 0.0001166 Validation loss = 0.0109, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  889 Training loss: 0.0001444 Validation loss = 0.01108, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  890 Training loss: 0.0001329 Validation loss = 0.01124, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  891 Training loss: 0.0001365 Validation loss = 0.01077, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  892 Training loss: 0.0001266 Validation loss = 0.01494, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  893 Training loss: 0.0001311 Validation loss = 0.01472, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  894 Training loss: 0.0001482 Validation loss = 0.01037, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  895 Training loss: 0.0001168 Validation loss = 0.01954, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  896 Training loss: 0.0001319 Validation loss = 0.01229, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  897 Training loss: 0.0001324 Validation loss = 0.01848, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  898 Training loss: 0.0001331 Validation loss = 0.01105, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  899 Training loss: 0.0001311 Validation loss = 0.01048, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  900 Training loss: 0.0001508 Validation loss = 0.009681, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  901 Training loss: 0.0001079 Validation loss = 0.01291, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  902 Training loss: 0.0001442 Validation loss = 0.01137, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  903 Training loss: 0.0001413 Validation loss = 0.01243, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  904 Training loss: 0.0001246 Validation loss = 0.01208, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  905 Training loss: 0.0001402 Validation loss = 0.01274, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  906 Training loss: 0.000117 Validation loss =   0.01, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  907 Training loss: 0.0001317 Validation loss = 0.01321, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  908 Training loss: 0.0001279 Validation loss = 0.01133, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  909 Training loss: 0.0001373 Validation loss = 0.0131, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  910 Training loss: 0.0001374 Validation loss = 0.01098, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  911 Training loss: 0.0001375 Validation loss = 0.01036, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  912 Training loss: 0.0001282 Validation loss = 0.01114, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  913 Training loss: 0.0001295 Validation loss = 0.01015, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  914 Training loss: 0.0001364 Validation loss = 0.01068, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  915 Training loss: 0.000127 Validation loss = 0.01008, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  916 Training loss: 0.0001492 Validation loss = 0.009717, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  917 Training loss: 0.0001048 Validation loss = 0.01402, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.009563862726807216\n",
      "Epoch:  918 Training loss: 0.0001276 Validation loss = 0.009564, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  919 Training loss: 0.0001608 Validation loss = 0.00989, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  920 Training loss: 0.0001085 Validation loss = 0.01255, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  921 Training loss: 0.0001276 Validation loss = 0.01016, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  922 Training loss: 0.0001272 Validation loss = 0.01111, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  923 Training loss: 0.0001365 Validation loss = 0.01141, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  924 Training loss: 0.0001385 Validation loss = 0.01154, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  925 Training loss: 0.0001206 Validation loss = 0.01119, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  926 Training loss: 0.0001361 Validation loss = 0.009821, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  927 Training loss: 0.0001302 Validation loss = 0.009673, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  928 Training loss: 0.0001262 Validation loss = 0.01143, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  929 Training loss: 0.0001253 Validation loss = 0.01024, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  930 Training loss: 0.0001347 Validation loss = 0.01068, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  931 Training loss: 0.0001245 Validation loss = 0.01669, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  932 Training loss: 0.0001175 Validation loss =  0.013, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  933 Training loss: 0.0001412 Validation loss = 0.01202, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  934 Training loss: 0.0001631 Validation loss = 0.009872, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  935 Training loss: 0.0001051 Validation loss = 0.01072, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  936 Training loss: 0.0001289 Validation loss = 0.009923, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  937 Training loss: 0.0001143 Validation loss = 0.01767, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  938 Training loss: 0.0001404 Validation loss = 0.01234, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  939 Training loss: 0.0001317 Validation loss = 0.009579, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  940 Training loss: 0.0001189 Validation loss = 0.01091, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  941 Training loss: 0.0001361 Validation loss = 0.01312, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.009498613986699142\n",
      "Epoch:  942 Training loss: 0.0001213 Validation loss = 0.009499, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  943 Training loss: 0.0001292 Validation loss = 0.01496, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  944 Training loss: 0.0001296 Validation loss = 0.01482, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.009449857639656981\n",
      "Epoch:  945 Training loss: 0.0001369 Validation loss = 0.00945, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  946 Training loss: 0.0001386 Validation loss = 0.01063, time Loss: 31.8%, back: 30.6%, val: 35.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  947 Training loss: 0.0001188 Validation loss = 0.01079, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  948 Training loss: 0.0001898 Validation loss = 0.01396, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  949 Training loss: 9.535e-05 Validation loss = 0.01022, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  950 Training loss: 0.0001072 Validation loss = 0.01255, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  951 Training loss: 0.0001239 Validation loss = 0.009626, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.009444019219474546\n",
      "Epoch:  952 Training loss: 0.0001294 Validation loss = 0.009444, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  953 Training loss: 0.0001189 Validation loss = 0.01264, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  954 Training loss: 0.0001452 Validation loss = 0.01006, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  955 Training loss: 0.0001273 Validation loss = 0.01385, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  956 Training loss: 0.000136 Validation loss = 0.01091, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  957 Training loss: 0.0001025 Validation loss = 0.01373, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  958 Training loss: 0.0001539 Validation loss = 0.01068, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  959 Training loss: 0.0001093 Validation loss = 0.01286, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  960 Training loss: 0.0001246 Validation loss = 0.01182, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  961 Training loss: 0.0001317 Validation loss = 0.01054, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  962 Training loss: 0.0001214 Validation loss = 0.01144, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  963 Training loss: 0.0001266 Validation loss = 0.009932, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  964 Training loss: 0.0001303 Validation loss = 0.01177, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  965 Training loss: 0.0001339 Validation loss = 0.00987, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  966 Training loss: 0.0001188 Validation loss = 0.009832, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  967 Training loss: 0.0001289 Validation loss = 0.01245, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  968 Training loss: 0.0001223 Validation loss = 0.01101, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  969 Training loss: 0.0001358 Validation loss = 0.01269, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  970 Training loss: 0.0001182 Validation loss = 0.01194, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  971 Training loss: 0.0001351 Validation loss = 0.01832, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  972 Training loss: 0.0001179 Validation loss = 0.01033, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  973 Training loss: 0.0001238 Validation loss = 0.009874, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  974 Training loss: 0.0001317 Validation loss = 0.009729, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  975 Training loss: 0.0001194 Validation loss =  0.013, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  976 Training loss: 0.0001295 Validation loss = 0.0116, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  977 Training loss: 0.0001174 Validation loss = 0.01057, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  978 Training loss: 0.0001657 Validation loss = 0.009696, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  979 Training loss: 0.0001003 Validation loss = 0.009501, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  980 Training loss: 0.0001311 Validation loss = 0.01095, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  981 Training loss: 0.0001257 Validation loss = 0.01436, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  982 Training loss: 0.0001207 Validation loss = 0.009498, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  983 Training loss: 0.0001598 Validation loss = 0.009744, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  984 Training loss: 9.575e-05 Validation loss = 0.01097, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  985 Training loss: 0.0001319 Validation loss = 0.01297, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  986 Training loss: 0.0001161 Validation loss = 0.02114, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  987 Training loss: 0.0001282 Validation loss = 0.01091, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  988 Training loss: 0.000123 Validation loss = 0.01145, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  989 Training loss: 0.0001219 Validation loss = 0.01029, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  990 Training loss: 0.0001261 Validation loss = 0.01019, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  991 Training loss: 0.0001208 Validation loss = 0.01148, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  992 Training loss: 0.0001271 Validation loss = 0.009924, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  993 Training loss: 0.0001294 Validation loss = 0.01311, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  994 Training loss: 0.0001275 Validation loss = 0.01037, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  995 Training loss: 0.0001126 Validation loss = 0.01388, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  996 Training loss: 0.0001389 Validation loss = 0.009891, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  997 Training loss: 0.0001198 Validation loss = 0.01028, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  998 Training loss: 0.0001274 Validation loss = 0.009664, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch:  999 Training loss: 0.0001147 Validation loss = 0.01353, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1000 Training loss: 0.000134 Validation loss = 0.0114, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1001 Training loss: 0.0001134 Validation loss = 0.01375, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1002 Training loss: 0.0001253 Validation loss = 0.01299, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1003 Training loss: 0.0001304 Validation loss = 0.01056, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1004 Training loss: 0.0001216 Validation loss = 0.01121, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1005 Training loss: 0.0001111 Validation loss = 0.01178, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1006 Training loss: 0.0001292 Validation loss = 0.01091, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1007 Training loss: 0.000136 Validation loss = 0.01116, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1008 Training loss: 0.0001179 Validation loss = 0.009986, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.009437098955997954\n",
      "Epoch: 1009 Training loss: 0.0001324 Validation loss = 0.009437, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.009301846062013133\n",
      "Epoch: 1010 Training loss: 0.0001173 Validation loss = 0.009302, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1011 Training loss: 0.0001232 Validation loss = 0.01036, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1012 Training loss: 0.0001265 Validation loss = 0.009523, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1013 Training loss: 0.000118 Validation loss = 0.01048, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1014 Training loss: 0.0001193 Validation loss = 0.01475, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1015 Training loss: 0.0001207 Validation loss = 0.0116, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1016 Training loss: 0.0001289 Validation loss = 0.009755, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1017 Training loss: 0.0001262 Validation loss = 0.009883, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1018 Training loss: 0.0001143 Validation loss = 0.01793, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1019 Training loss: 0.0001223 Validation loss = 0.01151, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1020 Training loss: 0.0001193 Validation loss = 0.0136, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1021 Training loss: 0.0001477 Validation loss = 0.01143, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1022 Training loss: 0.0001237 Validation loss = 0.01146, time Loss: 31.8%, back: 30.6%, val: 35.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1023 Training loss: 0.0001112 Validation loss = 0.01151, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1024 Training loss: 0.0001147 Validation loss = 0.01075, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1025 Training loss: 0.0001651 Validation loss = 0.009835, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1026 Training loss: 0.0001002 Validation loss = 0.009974, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1027 Training loss: 0.0001092 Validation loss = 0.009579, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1028 Training loss: 0.0001442 Validation loss = 0.009691, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1029 Training loss: 0.0001021 Validation loss = 0.01067, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1030 Training loss: 0.0001228 Validation loss = 0.01274, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1031 Training loss: 0.0001345 Validation loss = 0.01059, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1032 Training loss: 9.528e-05 Validation loss = 0.01196, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1033 Training loss: 0.0001502 Validation loss = 0.009827, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1034 Training loss: 0.000111 Validation loss = 0.01296, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1035 Training loss: 0.0001179 Validation loss = 0.00949, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1036 Training loss: 0.0001145 Validation loss = 0.01145, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1037 Training loss: 0.0001386 Validation loss = 0.009736, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1038 Training loss: 0.0001216 Validation loss = 0.01032, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1039 Training loss: 0.0001444 Validation loss = 0.01228, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1040 Training loss: 9.649e-05 Validation loss = 0.01024, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1041 Training loss: 0.0001309 Validation loss = 0.01096, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1042 Training loss: 0.0001145 Validation loss = 0.01007, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1043 Training loss: 0.0001164 Validation loss = 0.01285, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1044 Training loss: 0.0001249 Validation loss = 0.01161, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1045 Training loss: 0.0001123 Validation loss = 0.01036, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1046 Training loss: 0.0001336 Validation loss = 0.009627, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1047 Training loss: 0.0001174 Validation loss =  0.011, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1048 Training loss: 0.0001193 Validation loss = 0.01004, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1049 Training loss: 0.0001143 Validation loss = 0.009952, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1050 Training loss: 0.0001112 Validation loss = 0.01046, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1051 Training loss: 0.0001827 Validation loss = 0.009974, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1052 Training loss: 9.047e-05 Validation loss = 0.00992, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1053 Training loss: 0.0001293 Validation loss = 0.01092, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1054 Training loss: 0.0001039 Validation loss = 0.01031, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1055 Training loss: 0.0001144 Validation loss = 0.01003, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1056 Training loss: 0.0001355 Validation loss = 0.009336, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1057 Training loss: 0.0001096 Validation loss = 0.009558, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1058 Training loss: 0.0001359 Validation loss = 0.02575, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1059 Training loss: 0.000126 Validation loss = 0.009719, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1060 Training loss: 0.0001047 Validation loss = 0.01018, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1061 Training loss: 0.000112 Validation loss = 0.01154, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1062 Training loss: 0.0001229 Validation loss = 0.01073, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1063 Training loss: 0.0001208 Validation loss = 0.01349, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1064 Training loss: 0.0001285 Validation loss = 0.0142, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1065 Training loss: 0.0001017 Validation loss = 0.009684, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1066 Training loss: 0.0001302 Validation loss = 0.009531, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1067 Training loss: 0.0001263 Validation loss = 0.01303, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1068 Training loss: 0.0001133 Validation loss = 0.01138, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1069 Training loss: 0.0001162 Validation loss = 0.009507, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1070 Training loss: 0.0001349 Validation loss = 0.01127, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1071 Training loss: 0.000104 Validation loss = 0.009582, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1072 Training loss: 0.0001215 Validation loss = 0.01158, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1073 Training loss: 0.0001376 Validation loss = 0.009307, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1074 Training loss: 9.942e-05 Validation loss = 0.01029, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1075 Training loss: 0.00013 Validation loss = 0.009775, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1076 Training loss: 0.000108 Validation loss = 0.01008, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1077 Training loss: 0.0001413 Validation loss = 0.01599, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1078 Training loss: 0.0001134 Validation loss = 0.009567, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1079 Training loss: 0.0001024 Validation loss = 0.01087, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1080 Training loss: 0.0001339 Validation loss = 0.01089, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1081 Training loss: 0.0001057 Validation loss = 0.01128, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1082 Training loss: 0.0001191 Validation loss = 0.01003, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1083 Training loss: 0.0001236 Validation loss = 0.01413, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1084 Training loss: 0.0001074 Validation loss = 0.01082, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1085 Training loss: 0.0001338 Validation loss = 0.009759, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1086 Training loss: 0.0001068 Validation loss = 0.01309, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1087 Training loss: 0.0001542 Validation loss = 0.01571, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1088 Training loss: 0.0001096 Validation loss = 0.0128, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1089 Training loss: 0.000102 Validation loss = 0.009981, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1090 Training loss: 0.0001151 Validation loss = 0.009758, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1091 Training loss: 0.0001178 Validation loss = 0.01292, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1092 Training loss: 0.0001174 Validation loss = 0.01059, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1093 Training loss: 0.0001186 Validation loss = 0.01138, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1094 Training loss: 0.0001132 Validation loss = 0.01015, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1095 Training loss: 0.0001189 Validation loss = 0.01391, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1096 Training loss: 0.0001327 Validation loss = 0.01121, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1097 Training loss: 0.0001036 Validation loss = 0.009371, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1098 Training loss: 0.0001146 Validation loss = 0.009636, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1099 Training loss: 0.0001249 Validation loss = 0.009349, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1100 Training loss: 0.0001125 Validation loss = 0.01024, time Loss: 31.8%, back: 30.6%, val: 35.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1101 Training loss: 0.0001172 Validation loss = 0.01095, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1102 Training loss: 0.0001173 Validation loss = 0.0103, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1103 Training loss: 0.0001368 Validation loss = 0.01119, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.009180074514670778\n",
      "Epoch: 1104 Training loss: 0.0001132 Validation loss = 0.00918, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1105 Training loss: 0.0001095 Validation loss = 0.01637, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1106 Training loss: 0.0001246 Validation loss = 0.009247, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1107 Training loss: 0.0001069 Validation loss = 0.01221, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1108 Training loss: 0.0001236 Validation loss = 0.01155, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1109 Training loss: 0.0001091 Validation loss = 0.01115, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1110 Training loss: 0.0001103 Validation loss = 0.01018, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1111 Training loss: 0.0001138 Validation loss = 0.009812, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1112 Training loss: 0.0001297 Validation loss = 0.01044, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1113 Training loss: 0.0001147 Validation loss = 0.01206, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.00907837355744083\n",
      "Epoch: 1114 Training loss: 0.0001212 Validation loss = 0.009078, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1115 Training loss: 0.0001167 Validation loss = 0.01064, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1116 Training loss: 0.0001168 Validation loss = 0.01372, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1117 Training loss: 0.0001184 Validation loss = 0.01081, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1118 Training loss: 0.0001131 Validation loss = 0.009197, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1119 Training loss: 0.0001143 Validation loss = 0.009887, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1120 Training loss: 0.0001103 Validation loss = 0.01141, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1121 Training loss: 0.000116 Validation loss = 0.01235, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1122 Training loss: 0.000118 Validation loss = 0.0109, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1123 Training loss: 0.0001094 Validation loss = 0.01013, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1124 Training loss: 0.0001163 Validation loss = 0.01129, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1125 Training loss: 0.0001223 Validation loss = 0.01033, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1126 Training loss: 0.000132 Validation loss = 0.009269, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1127 Training loss: 9.376e-05 Validation loss = 0.01029, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1128 Training loss: 0.0001215 Validation loss = 0.02033, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1129 Training loss: 0.0001175 Validation loss = 0.0106, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1130 Training loss: 0.0001226 Validation loss = 0.01187, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1131 Training loss: 0.0001028 Validation loss = 0.01044, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1132 Training loss: 0.0001222 Validation loss = 0.01093, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1133 Training loss: 0.0001181 Validation loss = 0.01293, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1134 Training loss: 0.0001085 Validation loss = 0.01169, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1135 Training loss: 0.0001164 Validation loss = 0.009913, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1136 Training loss: 0.0001112 Validation loss = 0.009647, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1137 Training loss: 0.0001215 Validation loss = 0.009253, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1138 Training loss: 0.000154 Validation loss = 0.01413, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "########## new best ########### 0.009051086784182661\n",
      "Epoch: 1139 Training loss: 9.8e-05 Validation loss = 0.009051, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1140 Training loss: 0.0001076 Validation loss = 0.01126, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1141 Training loss: 0.0001084 Validation loss = 0.009727, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1142 Training loss: 0.000115 Validation loss = 0.01011, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1143 Training loss: 0.0001093 Validation loss = 0.01046, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1144 Training loss: 0.0001119 Validation loss = 0.01024, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1145 Training loss: 0.0001176 Validation loss = 0.009745, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1146 Training loss: 0.0001118 Validation loss = 0.01102, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1147 Training loss: 0.0001291 Validation loss = 0.01014, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1148 Training loss: 9.897e-05 Validation loss = 0.01093, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1149 Training loss: 0.0001255 Validation loss = 0.01156, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1150 Training loss: 0.0001164 Validation loss = 0.01559, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1151 Training loss: 0.0001112 Validation loss = 0.009982, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1152 Training loss: 9.802e-05 Validation loss = 0.01323, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1153 Training loss: 0.000132 Validation loss = 0.01238, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1154 Training loss: 0.0001097 Validation loss = 0.009507, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1155 Training loss: 0.0001067 Validation loss = 0.01182, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1156 Training loss: 0.000123 Validation loss = 0.01032, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1157 Training loss: 0.0001139 Validation loss = 0.009267, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1158 Training loss: 0.0001148 Validation loss = 0.009632, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1159 Training loss: 0.0001177 Validation loss = 0.00975, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1160 Training loss: 0.0001085 Validation loss = 0.01066, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1161 Training loss: 0.0001147 Validation loss = 0.01068, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1162 Training loss: 0.0001189 Validation loss = 0.009296, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1163 Training loss: 0.0001046 Validation loss = 0.01092, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1164 Training loss: 0.0001187 Validation loss = 0.009783, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1165 Training loss: 0.000126 Validation loss = 0.009284, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1166 Training loss: 0.0001033 Validation loss = 0.01144, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1167 Training loss: 0.0001071 Validation loss = 0.009183, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1168 Training loss: 0.0001328 Validation loss = 0.009745, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1169 Training loss: 9.953e-05 Validation loss = 0.01056, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1170 Training loss: 0.0001113 Validation loss = 0.01201, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1171 Training loss: 0.0001177 Validation loss = 0.009909, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1172 Training loss: 0.0001091 Validation loss = 0.01172, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1173 Training loss: 0.0001013 Validation loss = 0.01313, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1174 Training loss: 0.0001285 Validation loss = 0.01152, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1175 Training loss: 0.0001044 Validation loss = 0.01035, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1176 Training loss: 0.0001183 Validation loss = 0.009825, time Loss: 31.8%, back: 30.6%, val: 35.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1177 Training loss: 0.0001089 Validation loss = 0.01062, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1178 Training loss: 0.0001187 Validation loss = 0.01031, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1179 Training loss: 0.0001143 Validation loss = 0.01058, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1180 Training loss: 0.0001125 Validation loss = 0.01602, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1181 Training loss: 0.0001145 Validation loss = 0.009495, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1182 Training loss: 0.000114 Validation loss = 0.01031, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1183 Training loss: 0.0001051 Validation loss = 0.01056, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1184 Training loss: 0.000115 Validation loss = 0.01102, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1185 Training loss: 0.0001089 Validation loss = 0.01047, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1186 Training loss: 0.0001197 Validation loss = 0.009545, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1187 Training loss: 0.0001093 Validation loss = 0.01062, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1188 Training loss: 0.00011 Validation loss = 0.009496, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1189 Training loss: 0.0001088 Validation loss = 0.009195, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1190 Training loss: 0.0001172 Validation loss = 0.009925, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1191 Training loss: 0.0001265 Validation loss = 0.009697, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1192 Training loss: 9.653e-05 Validation loss = 0.01044, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1193 Training loss: 0.0001164 Validation loss = 0.009435, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1194 Training loss: 0.0001069 Validation loss = 0.00922, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1195 Training loss: 0.0001408 Validation loss = 0.009346, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1196 Training loss: 0.0001001 Validation loss = 0.009247, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1197 Training loss: 0.0001027 Validation loss = 0.01058, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1198 Training loss: 0.0001067 Validation loss = 0.01044, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1199 Training loss: 0.000126 Validation loss = 0.009233, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1200 Training loss: 0.0001053 Validation loss = 0.009353, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1201 Training loss: 0.0001108 Validation loss = 0.009996, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1202 Training loss: 0.000106 Validation loss = 0.009158, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1203 Training loss: 0.0001183 Validation loss = 0.00974, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1204 Training loss: 0.0001139 Validation loss = 0.009059, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1205 Training loss: 0.0001248 Validation loss = 0.01148, time Loss: 31.8%, back: 30.6%, val: 35.0%\n",
      "Epoch: 1206 Training loss: 9.706e-05 Validation loss = 0.009364, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1207 Training loss: 0.0001063 Validation loss = 0.01095, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1208 Training loss: 0.0001103 Validation loss = 0.00926, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1209 Training loss: 0.0001148 Validation loss = 0.0094, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1210 Training loss: 0.000117 Validation loss = 0.009721, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1211 Training loss: 0.0001147 Validation loss = 0.01055, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1212 Training loss: 9.937e-05 Validation loss = 0.01238, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1213 Training loss: 0.000111 Validation loss = 0.01133, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1214 Training loss: 0.0001284 Validation loss = 0.01127, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1215 Training loss: 9.578e-05 Validation loss = 0.01297, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1216 Training loss: 0.0001038 Validation loss = 0.009116, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1217 Training loss: 0.0001225 Validation loss = 0.01152, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1218 Training loss: 0.000117 Validation loss = 0.01001, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1219 Training loss: 0.0001182 Validation loss = 0.01069, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1220 Training loss: 9.75e-05 Validation loss = 0.00909, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1221 Training loss: 0.0001084 Validation loss = 0.009091, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1222 Training loss: 0.0001113 Validation loss = 0.01269, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1223 Training loss: 0.0001057 Validation loss = 0.01229, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1224 Training loss: 0.0001071 Validation loss = 0.0114, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1225 Training loss: 0.0001177 Validation loss = 0.009499, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1226 Training loss: 0.0001223 Validation loss = 0.009267, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1227 Training loss: 0.0001023 Validation loss = 0.01406, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1228 Training loss: 0.000109 Validation loss = 0.01116, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1229 Training loss: 0.0001286 Validation loss = 0.0111, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1230 Training loss: 9.805e-05 Validation loss = 0.01098, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1231 Training loss: 0.0001116 Validation loss = 0.009591, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1232 Training loss: 0.0001047 Validation loss = 0.009122, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1233 Training loss: 0.0001193 Validation loss = 0.009934, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1234 Training loss:  0.0001 Validation loss = 0.01324, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1235 Training loss: 0.0001124 Validation loss = 0.009797, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1236 Training loss: 0.0001118 Validation loss = 0.009504, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1237 Training loss: 0.0001036 Validation loss = 0.01095, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1238 Training loss: 0.0001078 Validation loss = 0.01561, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1239 Training loss: 0.0001102 Validation loss = 0.01006, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1240 Training loss: 0.0001138 Validation loss = 0.01043, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1241 Training loss: 0.0001118 Validation loss = 0.01292, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1242 Training loss: 0.0001207 Validation loss = 0.009151, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1243 Training loss: 9.854e-05 Validation loss = 0.009295, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1244 Training loss: 0.0001101 Validation loss = 0.009306, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1245 Training loss: 0.000119 Validation loss = 0.01231, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1246 Training loss: 9.841e-05 Validation loss = 0.00953, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1247 Training loss: 0.000109 Validation loss = 0.009214, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1248 Training loss: 0.0001306 Validation loss = 0.009113, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1249 Training loss: 8.913e-05 Validation loss = 0.009659, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1250 Training loss: 0.000107 Validation loss = 0.009115, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1251 Training loss: 0.000109 Validation loss = 0.01185, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1252 Training loss: 0.0001226 Validation loss = 0.01025, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1253 Training loss: 0.0001113 Validation loss = 0.009636, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1254 Training loss: 9.523e-05 Validation loss = 0.01171, time Loss: 31.8%, back: 30.7%, val: 35.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1255 Training loss: 0.0001101 Validation loss = 0.009707, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1256 Training loss: 0.000111 Validation loss = 0.009622, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1257 Training loss: 0.0001041 Validation loss = 0.01281, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1258 Training loss: 0.000111 Validation loss = 0.0115, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1259 Training loss: 0.0001111 Validation loss = 0.01417, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1260 Training loss: 0.0001201 Validation loss =  0.011, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1261 Training loss: 0.0001008 Validation loss = 0.01227, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1262 Training loss: 0.0001087 Validation loss = 0.009335, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1263 Training loss: 0.0001083 Validation loss = 0.009664, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "########## new best ########### 0.009034207544991693\n",
      "Epoch: 1264 Training loss: 0.0001125 Validation loss = 0.009034, time Loss: 31.8%, back: 30.7%, val: 35.0%\n",
      "Epoch: 1265 Training loss: 0.000105 Validation loss = 0.01049, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1266 Training loss: 0.0001025 Validation loss = 0.009173, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1267 Training loss: 0.0001112 Validation loss = 0.01057, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1268 Training loss: 0.0001238 Validation loss = 0.0097, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1269 Training loss: 8.687e-05 Validation loss = 0.01013, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1270 Training loss: 0.0001272 Validation loss = 0.0119, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1271 Training loss: 9.922e-05 Validation loss = 0.009442, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1272 Training loss: 0.0001078 Validation loss = 0.01038, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1273 Training loss: 0.0001286 Validation loss = 0.01127, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1274 Training loss: 9.474e-05 Validation loss = 0.009217, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1275 Training loss: 0.0001033 Validation loss = 0.01051, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1276 Training loss: 0.0001117 Validation loss = 0.009784, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1277 Training loss: 9.923e-05 Validation loss = 0.009343, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.008964095202911592\n",
      "Epoch: 1278 Training loss: 0.0001236 Validation loss = 0.008964, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1279 Training loss: 9.97e-05 Validation loss = 0.01393, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1280 Training loss: 0.0001157 Validation loss = 0.0117, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1281 Training loss: 0.0001097 Validation loss = 0.009193, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1282 Training loss: 0.0001052 Validation loss = 0.0129, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1283 Training loss: 9.859e-05 Validation loss = 0.01003, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1284 Training loss: 0.000111 Validation loss = 0.01028, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1285 Training loss: 0.000104 Validation loss = 0.01019, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1286 Training loss: 0.0001077 Validation loss = 0.01201, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1287 Training loss: 0.0001099 Validation loss = 0.009557, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1288 Training loss: 0.0001067 Validation loss = 0.01215, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1289 Training loss: 0.0001073 Validation loss = 0.009317, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1290 Training loss: 0.0001222 Validation loss = 0.01154, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1291 Training loss: 9.679e-05 Validation loss = 0.009251, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1292 Training loss: 0.0001049 Validation loss = 0.01028, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1293 Training loss: 0.0001051 Validation loss = 0.01443, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1294 Training loss: 0.0001168 Validation loss = 0.00972, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1295 Training loss: 0.0001087 Validation loss = 0.0147, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1296 Training loss: 0.0001027 Validation loss = 0.01027, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1297 Training loss: 9.731e-05 Validation loss = 0.009896, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1298 Training loss: 0.0001266 Validation loss = 0.009203, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1299 Training loss: 0.0001054 Validation loss = 0.01015, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1300 Training loss: 0.0001005 Validation loss = 0.009411, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1301 Training loss: 0.0001032 Validation loss = 0.01353, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1302 Training loss: 0.000111 Validation loss = 0.01074, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1303 Training loss: 9.77e-05 Validation loss =  0.011, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1304 Training loss: 0.0001086 Validation loss = 0.01046, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1305 Training loss: 0.0001112 Validation loss = 0.009384, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1306 Training loss: 0.0001065 Validation loss = 0.009621, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1307 Training loss: 0.0001099 Validation loss = 0.01078, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1308 Training loss: 0.0001067 Validation loss = 0.01088, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1309 Training loss: 0.0001088 Validation loss = 0.008977, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1310 Training loss: 0.0001086 Validation loss = 0.01144, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1311 Training loss: 0.0001095 Validation loss = 0.01727, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1312 Training loss: 0.0001037 Validation loss = 0.009643, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1313 Training loss: 0.0001148 Validation loss = 0.009919, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1314 Training loss: 9.412e-05 Validation loss = 0.00992, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1315 Training loss: 0.0001107 Validation loss = 0.009161, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1316 Training loss: 0.0001074 Validation loss = 0.01026, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1317 Training loss: 0.0001008 Validation loss = 0.01106, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1318 Training loss: 0.000113 Validation loss = 0.00936, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1319 Training loss: 9.231e-05 Validation loss = 0.01333, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1320 Training loss: 0.0001068 Validation loss = 0.01041, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1321 Training loss: 0.0001058 Validation loss = 0.009118, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1322 Training loss: 0.0001058 Validation loss = 0.01013, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1323 Training loss: 0.0001094 Validation loss = 0.009343, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1324 Training loss: 0.0001088 Validation loss = 0.00972, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1325 Training loss: 0.0001206 Validation loss = 0.01114, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.008952833188032607\n",
      "Epoch: 1326 Training loss: 8.955e-05 Validation loss = 0.008953, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1327 Training loss: 0.0001022 Validation loss = 0.009108, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1328 Training loss: 0.0001231 Validation loss = 0.009627, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1329 Training loss: 0.0001041 Validation loss = 0.01174, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1330 Training loss: 0.000101 Validation loss = 0.01022, time Loss: 31.8%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1331 Training loss: 0.0001054 Validation loss = 0.01093, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1332 Training loss: 0.0001092 Validation loss = 0.01169, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.008926800772726036\n",
      "Epoch: 1333 Training loss: 0.0001212 Validation loss = 0.008927, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1334 Training loss: 9.362e-05 Validation loss = 0.01026, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.008894495181361988\n",
      "Epoch: 1335 Training loss: 0.0001094 Validation loss = 0.008894, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1336 Training loss: 9.198e-05 Validation loss = 0.01574, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1337 Training loss: 0.0001072 Validation loss = 0.01067, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1338 Training loss: 0.0001096 Validation loss = 0.01009, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1339 Training loss: 0.0001035 Validation loss = 0.01052, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1340 Training loss: 0.0001144 Validation loss = 0.01217, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1341 Training loss: 9.593e-05 Validation loss = 0.01485, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1342 Training loss: 0.0001012 Validation loss = 0.009197, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1343 Training loss: 0.0001029 Validation loss = 0.01006, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1344 Training loss: 0.0001131 Validation loss = 0.0102, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1345 Training loss: 0.0001065 Validation loss = 0.01129, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1346 Training loss: 0.0001004 Validation loss = 0.01534, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1347 Training loss: 0.0001018 Validation loss = 0.009647, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1348 Training loss: 0.0001106 Validation loss = 0.01447, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1349 Training loss: 0.0001008 Validation loss = 0.009081, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1350 Training loss: 0.0001017 Validation loss = 0.01121, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1351 Training loss: 0.0001228 Validation loss = 0.01052, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1352 Training loss: 9.709e-05 Validation loss = 0.009404, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1353 Training loss: 9.611e-05 Validation loss = 0.01004, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1354 Training loss: 0.0001126 Validation loss = 0.01232, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1355 Training loss: 0.0001194 Validation loss = 0.01027, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1356 Training loss: 9.929e-05 Validation loss = 0.009459, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1357 Training loss: 9.878e-05 Validation loss = 0.01386, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1358 Training loss: 0.000107 Validation loss = 0.01052, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1359 Training loss: 8.949e-05 Validation loss = 0.01033, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1360 Training loss: 0.0001334 Validation loss = 0.009583, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1361 Training loss: 8.876e-05 Validation loss = 0.009239, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1362 Training loss: 0.0001067 Validation loss = 0.009417, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1363 Training loss: 0.0001034 Validation loss = 0.01125, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1364 Training loss: 0.0001041 Validation loss = 0.01008, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1365 Training loss: 0.0001015 Validation loss = 0.0109, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1366 Training loss: 0.0001033 Validation loss = 0.009585, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1367 Training loss: 0.0001035 Validation loss = 0.009112, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1368 Training loss: 0.0001189 Validation loss = 0.01117, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1369 Training loss: 9.054e-05 Validation loss = 0.01053, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1370 Training loss: 0.0001016 Validation loss = 0.00949, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.008837389490399182\n",
      "Epoch: 1371 Training loss: 0.0001034 Validation loss = 0.008837, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1372 Training loss: 0.0001052 Validation loss = 0.01046, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1373 Training loss: 0.0001006 Validation loss = 0.0119, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1374 Training loss: 0.00011 Validation loss = 0.01194, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1375 Training loss: 0.0001027 Validation loss = 0.009053, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1376 Training loss: 0.0001029 Validation loss = 0.00947, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1377 Training loss: 0.0001064 Validation loss = 0.01021, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1378 Training loss: 0.0001092 Validation loss = 0.009356, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1379 Training loss: 0.0001037 Validation loss = 0.01131, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1380 Training loss: 9.478e-05 Validation loss = 0.01177, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1381 Training loss: 0.0001086 Validation loss = 0.01016, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1382 Training loss: 0.0001081 Validation loss = 0.01072, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1383 Training loss: 9.078e-05 Validation loss = 0.01442, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1384 Training loss: 0.0001168 Validation loss = 0.009855, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1385 Training loss: 9.579e-05 Validation loss = 0.009962, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1386 Training loss: 0.0001091 Validation loss = 0.0104, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1387 Training loss: 0.0001017 Validation loss = 0.009418, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.008791859694378096\n",
      "Epoch: 1388 Training loss: 0.0001017 Validation loss = 0.008792, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1389 Training loss: 0.000107 Validation loss = 0.009978, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1390 Training loss: 9.827e-05 Validation loss = 0.009146, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1391 Training loss: 9.523e-05 Validation loss = 0.01225, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1392 Training loss: 0.0001072 Validation loss = 0.009648, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1393 Training loss: 0.0001047 Validation loss = 0.01002, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1394 Training loss: 0.0001107 Validation loss = 0.009474, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1395 Training loss: 9.249e-05 Validation loss = 0.01028, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1396 Training loss: 0.0001087 Validation loss = 0.009904, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1397 Training loss: 9.965e-05 Validation loss = 0.009559, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1398 Training loss: 0.0001035 Validation loss = 0.009219, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1399 Training loss: 0.0001076 Validation loss = 0.009105, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1400 Training loss: 0.0001003 Validation loss = 0.0107, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1401 Training loss: 0.0001022 Validation loss = 0.008821, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1402 Training loss: 0.0001111 Validation loss = 0.01333, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1403 Training loss: 9.944e-05 Validation loss = 0.009425, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1404 Training loss: 9.428e-05 Validation loss = 0.01041, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1405 Training loss: 0.0001142 Validation loss = 0.01102, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1406 Training loss: 9.796e-05 Validation loss = 0.0106, time Loss: 31.8%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1407 Training loss: 0.0001115 Validation loss = 0.01746, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1408 Training loss: 9.627e-05 Validation loss = 0.009902, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1409 Training loss:  0.0001 Validation loss = 0.01072, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1410 Training loss: 0.000113 Validation loss = 0.01006, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1411 Training loss: 0.0001002 Validation loss = 0.01038, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1412 Training loss: 9.483e-05 Validation loss = 0.00914, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1413 Training loss: 0.0001068 Validation loss = 0.009029, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1414 Training loss: 9.896e-05 Validation loss = 0.009246, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1415 Training loss: 0.0001207 Validation loss = 0.01076, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1416 Training loss: 8.829e-05 Validation loss = 0.009952, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1417 Training loss: 9.7e-05 Validation loss = 0.008991, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1418 Training loss: 0.000114 Validation loss = 0.009567, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1419 Training loss: 9.608e-05 Validation loss = 0.01659, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1420 Training loss: 0.0001022 Validation loss =   0.01, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1421 Training loss: 0.0001015 Validation loss = 0.01062, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.008733083848750097\n",
      "Epoch: 1422 Training loss: 0.000104 Validation loss = 0.008733, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1423 Training loss: 0.0001015 Validation loss = 0.01093, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1424 Training loss: 0.0001017 Validation loss = 0.01032, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1425 Training loss: 0.0001154 Validation loss = 0.01111, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1426 Training loss: 8.481e-05 Validation loss = 0.009659, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1427 Training loss: 0.0001002 Validation loss = 0.01088, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1428 Training loss: 0.0001084 Validation loss = 0.009347, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1429 Training loss: 8.943e-05 Validation loss = 0.01015, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1430 Training loss: 0.0001195 Validation loss = 0.01146, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1431 Training loss: 9.664e-05 Validation loss = 0.009467, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1432 Training loss: 0.0001013 Validation loss = 0.0102, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1433 Training loss: 0.0001002 Validation loss = 0.0107, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1434 Training loss: 0.000103 Validation loss = 0.00896, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1435 Training loss: 9.277e-05 Validation loss = 0.009418, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1436 Training loss: 0.0001046 Validation loss = 0.01099, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1437 Training loss: 9.942e-05 Validation loss = 0.01245, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1438 Training loss: 0.0001041 Validation loss = 0.01215, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1439 Training loss: 0.0001133 Validation loss = 0.009919, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1440 Training loss: 9.485e-05 Validation loss = 0.01031, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1441 Training loss: 0.0001165 Validation loss = 0.009264, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1442 Training loss: 8.933e-05 Validation loss = 0.01018, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1443 Training loss: 9.696e-05 Validation loss = 0.01124, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1444 Training loss: 0.0001009 Validation loss = 0.01052, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1445 Training loss: 0.0001076 Validation loss = 0.009399, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1446 Training loss: 9.833e-05 Validation loss = 0.01054, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1447 Training loss: 9.913e-05 Validation loss = 0.01439, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1448 Training loss: 9.315e-05 Validation loss = 0.009682, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1449 Training loss: 0.0001163 Validation loss = 0.008785, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1450 Training loss: 8.612e-05 Validation loss = 0.009282, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1451 Training loss: 0.0001156 Validation loss = 0.009516, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1452 Training loss: 9.145e-05 Validation loss = 0.008928, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1453 Training loss: 9.63e-05 Validation loss = 0.00888, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1454 Training loss: 0.0001249 Validation loss = 0.009057, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1455 Training loss: 8.67e-05 Validation loss = 0.008795, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1456 Training loss: 9.907e-05 Validation loss = 0.01025, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1457 Training loss: 0.0001015 Validation loss = 0.01006, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1458 Training loss: 9.572e-05 Validation loss = 0.01093, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1459 Training loss: 0.0001009 Validation loss = 0.009513, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1460 Training loss: 0.0001029 Validation loss = 0.01081, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1461 Training loss: 9.806e-05 Validation loss = 0.008976, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1462 Training loss: 0.0001032 Validation loss = 0.009283, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1463 Training loss: 0.0001059 Validation loss = 0.009388, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1464 Training loss: 9.694e-05 Validation loss = 0.01042, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1465 Training loss: 9.443e-05 Validation loss = 0.01206, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1466 Training loss: 0.0001105 Validation loss = 0.009524, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1467 Training loss:  0.0001 Validation loss = 0.009248, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1468 Training loss: 0.0001036 Validation loss = 0.01039, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1469 Training loss: 9.263e-05 Validation loss = 0.01241, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1470 Training loss: 0.0001144 Validation loss = 0.01083, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1471 Training loss: 8.704e-05 Validation loss = 0.009834, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1472 Training loss: 9.729e-05 Validation loss = 0.009054, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1473 Training loss: 9.705e-05 Validation loss = 0.01022, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1474 Training loss: 0.000105 Validation loss = 0.008908, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1475 Training loss: 0.0001027 Validation loss = 0.009358, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1476 Training loss: 9.603e-05 Validation loss = 0.008746, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1477 Training loss: 0.0001037 Validation loss = 0.01037, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1478 Training loss: 0.0001038 Validation loss = 0.009271, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1479 Training loss: 9.146e-05 Validation loss = 0.01095, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1480 Training loss: 0.0001004 Validation loss = 0.008745, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1481 Training loss: 0.0001135 Validation loss = 0.009358, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1482 Training loss: 8.709e-05 Validation loss = 0.00881, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1483 Training loss: 0.000102 Validation loss = 0.009212, time Loss: 31.8%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1484 Training loss: 0.0001071 Validation loss = 0.009154, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.00862597875716629\n",
      "Epoch: 1485 Training loss: 8.92e-05 Validation loss = 0.008626, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1486 Training loss: 9.755e-05 Validation loss = 0.009585, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1487 Training loss: 0.0001098 Validation loss = 0.008794, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1488 Training loss: 9.177e-05 Validation loss = 0.01115, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1489 Training loss: 0.000108 Validation loss = 0.009991, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1490 Training loss: 0.0001001 Validation loss = 0.01276, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1491 Training loss: 9.207e-05 Validation loss = 0.01098, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1492 Training loss: 9.602e-05 Validation loss = 0.009328, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1493 Training loss: 0.0001186 Validation loss = 0.01321, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1494 Training loss: 8.829e-05 Validation loss = 0.009331, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1495 Training loss: 9.443e-05 Validation loss = 0.01151, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1496 Training loss: 0.0001072 Validation loss = 0.01032, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1497 Training loss: 9.175e-05 Validation loss = 0.01103, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1498 Training loss: 9.886e-05 Validation loss = 0.01468, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1499 Training loss: 9.414e-05 Validation loss = 0.00973, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1500 Training loss: 0.0001145 Validation loss = 0.009547, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1501 Training loss: 9.655e-05 Validation loss = 0.009376, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1502 Training loss: 0.0001028 Validation loss = 0.01007, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1503 Training loss: 8.616e-05 Validation loss = 0.009297, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1504 Training loss: 0.000107 Validation loss = 0.00991, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1505 Training loss: 9.575e-05 Validation loss = 0.009751, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1506 Training loss: 0.0001055 Validation loss = 0.008778, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1507 Training loss: 8.817e-05 Validation loss = 0.009596, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1508 Training loss: 0.0001009 Validation loss = 0.009724, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1509 Training loss: 0.0001009 Validation loss = 0.009067, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1510 Training loss: 9.751e-05 Validation loss = 0.008779, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1511 Training loss:  0.0001 Validation loss = 0.008854, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1512 Training loss: 9.708e-05 Validation loss = 0.01118, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1513 Training loss: 0.0001021 Validation loss = 0.008948, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1514 Training loss: 9.826e-05 Validation loss = 0.01031, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1515 Training loss: 0.0001083 Validation loss = 0.009121, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1516 Training loss: 9.362e-05 Validation loss = 0.01241, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.008610167780320588\n",
      "Epoch: 1517 Training loss: 9.828e-05 Validation loss = 0.00861, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1518 Training loss: 9.999e-05 Validation loss = 0.01089, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1519 Training loss: 9.556e-05 Validation loss = 0.01461, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1520 Training loss: 9.86e-05 Validation loss = 0.01052, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1521 Training loss: 9.998e-05 Validation loss = 0.009524, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1522 Training loss: 9.993e-05 Validation loss = 0.009485, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1523 Training loss: 8.92e-05 Validation loss = 0.009374, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1524 Training loss: 0.000109 Validation loss = 0.008765, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1525 Training loss: 9.138e-05 Validation loss = 0.01049, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1526 Training loss: 0.0001083 Validation loss = 0.009856, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1527 Training loss: 8.798e-05 Validation loss = 0.008883, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1528 Training loss: 0.0001029 Validation loss = 0.009729, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1529 Training loss: 9.106e-05 Validation loss = 0.009628, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1530 Training loss: 0.0001083 Validation loss = 0.009016, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1531 Training loss: 9.242e-05 Validation loss = 0.01013, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1532 Training loss: 0.0001045 Validation loss = 0.01054, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1533 Training loss: 9.157e-05 Validation loss = 0.01094, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1534 Training loss: 9.297e-05 Validation loss = 0.008655, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1535 Training loss: 9.734e-05 Validation loss = 0.009177, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1536 Training loss: 9.737e-05 Validation loss = 0.009981, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1537 Training loss: 0.0001004 Validation loss = 0.008979, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1538 Training loss: 0.000107 Validation loss = 0.008822, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1539 Training loss: 9.624e-05 Validation loss = 0.009554, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1540 Training loss: 0.0001028 Validation loss = 0.009932, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1541 Training loss: 9.687e-05 Validation loss = 0.009944, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1542 Training loss: 9.237e-05 Validation loss = 0.01416, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1543 Training loss: 9.44e-05 Validation loss = 0.009497, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1544 Training loss: 9.47e-05 Validation loss = 0.009976, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1545 Training loss: 0.000107 Validation loss = 0.009342, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1546 Training loss: 8.866e-05 Validation loss = 0.01092, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1547 Training loss: 9.7e-05 Validation loss = 0.008981, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1548 Training loss:  0.0001 Validation loss = 0.01019, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1549 Training loss: 9.702e-05 Validation loss = 0.008748, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1550 Training loss: 9.548e-05 Validation loss = 0.0122, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.008601951137250508\n",
      "Epoch: 1551 Training loss: 9.202e-05 Validation loss = 0.008602, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1552 Training loss: 0.0001134 Validation loss = 0.009189, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1553 Training loss: 8.426e-05 Validation loss = 0.009604, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1554 Training loss: 0.0001001 Validation loss = 0.01305, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1555 Training loss: 0.0001051 Validation loss = 0.009583, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1556 Training loss: 8.486e-05 Validation loss = 0.009075, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1557 Training loss: 0.000106 Validation loss = 0.01601, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1558 Training loss: 9.543e-05 Validation loss = 0.0101, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1559 Training loss: 0.0001027 Validation loss = 0.008836, time Loss: 31.8%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1560 Training loss: 9.187e-05 Validation loss = 0.01071, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1561 Training loss: 9.134e-05 Validation loss = 0.01072, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1562 Training loss: 0.0001062 Validation loss = 0.0103, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.008559475737577614\n",
      "Epoch: 1563 Training loss: 9.382e-05 Validation loss = 0.008559, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1564 Training loss: 9.432e-05 Validation loss = 0.00868, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1565 Training loss: 0.0001293 Validation loss = 0.008914, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1566 Training loss: 8.113e-05 Validation loss = 0.009039, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1567 Training loss: 9.78e-05 Validation loss = 0.009686, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1568 Training loss: 9.134e-05 Validation loss = 0.01124, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1569 Training loss: 9.386e-05 Validation loss = 0.01237, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1570 Training loss: 9.129e-05 Validation loss = 0.009906, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1571 Training loss: 9.435e-05 Validation loss = 0.01005, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1572 Training loss: 0.0001034 Validation loss = 0.009081, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1573 Training loss: 9.925e-05 Validation loss = 0.01454, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1574 Training loss: 9.342e-05 Validation loss = 0.008603, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1575 Training loss: 9.294e-05 Validation loss = 0.01006, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1576 Training loss: 9.956e-05 Validation loss = 0.0106, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1577 Training loss: 9.769e-05 Validation loss = 0.008657, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1578 Training loss: 8.743e-05 Validation loss = 0.01065, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1579 Training loss: 0.0001004 Validation loss = 0.009829, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1580 Training loss: 9.754e-05 Validation loss = 0.009617, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1581 Training loss: 0.0001038 Validation loss = 0.008661, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1582 Training loss: 8.573e-05 Validation loss = 0.008588, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1583 Training loss: 9.948e-05 Validation loss = 0.01273, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1584 Training loss: 0.0001047 Validation loss = 0.008929, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1585 Training loss: 8.437e-05 Validation loss = 0.008962, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1586 Training loss: 0.0001094 Validation loss = 0.008986, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1587 Training loss: 8.368e-05 Validation loss = 0.01953, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1588 Training loss: 0.000104 Validation loss = 0.01021, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1589 Training loss: 9.335e-05 Validation loss = 0.01017, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1590 Training loss: 0.0001051 Validation loss = 0.009317, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1591 Training loss: 8.218e-05 Validation loss = 0.01187, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1592 Training loss: 0.0001015 Validation loss = 0.009733, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1593 Training loss: 8.627e-05 Validation loss = 0.00878, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1594 Training loss: 0.0001117 Validation loss = 0.008885, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1595 Training loss: 8.637e-05 Validation loss = 0.01118, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1596 Training loss: 9.769e-05 Validation loss = 0.009381, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1597 Training loss: 9.406e-05 Validation loss = 0.009451, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1598 Training loss: 0.0001001 Validation loss = 0.01144, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1599 Training loss: 9.339e-05 Validation loss = 0.008696, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1600 Training loss: 0.0001029 Validation loss = 0.01394, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1601 Training loss: 8.476e-05 Validation loss = 0.009116, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1602 Training loss: 9.664e-05 Validation loss = 0.01156, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1603 Training loss: 0.0001052 Validation loss = 0.01036, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1604 Training loss: 8.733e-05 Validation loss = 0.009144, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1605 Training loss: 0.0001032 Validation loss = 0.01314, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1606 Training loss: 8.791e-05 Validation loss = 0.009756, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1607 Training loss: 9.323e-05 Validation loss = 0.01289, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1608 Training loss: 0.0001023 Validation loss = 0.009307, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1609 Training loss: 9.735e-05 Validation loss = 0.008946, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1610 Training loss: 8.542e-05 Validation loss = 0.00991, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1611 Training loss: 9.144e-05 Validation loss = 0.01151, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1612 Training loss: 0.0001065 Validation loss = 0.009404, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1613 Training loss: 8.984e-05 Validation loss = 0.01059, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1614 Training loss: 9.348e-05 Validation loss = 0.009553, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1615 Training loss: 0.0001044 Validation loss = 0.008963, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1616 Training loss: 8.977e-05 Validation loss = 0.01781, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1617 Training loss: 0.0001048 Validation loss = 0.008972, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1618 Training loss: 8.562e-05 Validation loss = 0.01416, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1619 Training loss: 9.2e-05 Validation loss = 0.0114, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1620 Training loss: 9.832e-05 Validation loss = 0.009037, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1621 Training loss: 9.444e-05 Validation loss = 0.01495, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1622 Training loss: 9.88e-05 Validation loss = 0.009591, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1623 Training loss: 8.919e-05 Validation loss = 0.01061, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1624 Training loss: 9.924e-05 Validation loss = 0.01014, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1625 Training loss: 8.749e-05 Validation loss = 0.009404, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1626 Training loss: 0.0001032 Validation loss = 0.009204, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1627 Training loss: 9.213e-05 Validation loss = 0.01172, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1628 Training loss: 9.559e-05 Validation loss = 0.01176, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1629 Training loss: 8.658e-05 Validation loss = 0.01261, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1630 Training loss: 0.0001063 Validation loss = 0.01204, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1631 Training loss: 9.24e-05 Validation loss = 0.01036, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1632 Training loss: 9.915e-05 Validation loss = 0.008722, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1633 Training loss: 9.235e-05 Validation loss = 0.01562, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1634 Training loss:  0.0001 Validation loss = 0.008569, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1635 Training loss: 9.492e-05 Validation loss = 0.01004, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1636 Training loss: 8.598e-05 Validation loss = 0.009318, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1637 Training loss: 9.523e-05 Validation loss = 0.00886, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1638 Training loss: 9.151e-05 Validation loss = 0.01061, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1639 Training loss: 0.0001033 Validation loss = 0.008978, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1640 Training loss: 8.394e-05 Validation loss = 0.008772, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1641 Training loss: 0.0001039 Validation loss = 0.0118, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1642 Training loss: 8.675e-05 Validation loss = 0.009007, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1643 Training loss: 9.45e-05 Validation loss = 0.009607, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1644 Training loss: 9.575e-05 Validation loss = 0.01028, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1645 Training loss: 9.599e-05 Validation loss = 0.009396, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1646 Training loss: 0.0001037 Validation loss = 0.009241, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.008519026420008109\n",
      "Epoch: 1647 Training loss: 8.691e-05 Validation loss = 0.008519, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1648 Training loss: 9.567e-05 Validation loss = 0.009099, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1649 Training loss: 9.086e-05 Validation loss = 0.01075, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1650 Training loss: 9.517e-05 Validation loss = 0.009923, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1651 Training loss: 9.07e-05 Validation loss = 0.009487, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1652 Training loss: 9.108e-05 Validation loss = 0.01018, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1653 Training loss: 9.622e-05 Validation loss = 0.01024, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1654 Training loss: 9.317e-05 Validation loss = 0.009322, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1655 Training loss: 9.863e-05 Validation loss = 0.01241, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1656 Training loss: 9.078e-05 Validation loss = 0.008674, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1657 Training loss: 0.0001046 Validation loss = 0.01146, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1658 Training loss: 8.236e-05 Validation loss = 0.009425, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1659 Training loss: 0.0001057 Validation loss = 0.0115, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1660 Training loss: 8.826e-05 Validation loss = 0.009823, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1661 Training loss: 0.0001104 Validation loss = 0.01123, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1662 Training loss: 8.458e-05 Validation loss = 0.008949, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1663 Training loss: 8.176e-05 Validation loss = 0.01036, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1664 Training loss: 9.589e-05 Validation loss = 0.009749, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1665 Training loss: 9.145e-05 Validation loss = 0.009428, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1666 Training loss: 9.2e-05 Validation loss = 0.009803, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1667 Training loss: 9.756e-05 Validation loss = 0.0108, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1668 Training loss: 9.965e-05 Validation loss = 0.008741, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1669 Training loss: 8.926e-05 Validation loss = 0.009206, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1670 Training loss: 9.342e-05 Validation loss = 0.009869, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1671 Training loss: 8.603e-05 Validation loss = 0.009362, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1672 Training loss: 8.998e-05 Validation loss = 0.01046, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1673 Training loss: 0.000103 Validation loss = 0.009397, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1674 Training loss: 9.185e-05 Validation loss = 0.009391, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1675 Training loss: 8.663e-05 Validation loss = 0.009296, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1676 Training loss: 0.0001092 Validation loss = 0.009612, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1677 Training loss: 8.839e-05 Validation loss = 0.009881, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1678 Training loss: 9.018e-05 Validation loss = 0.009293, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1679 Training loss: 9.208e-05 Validation loss = 0.01006, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1680 Training loss: 9.589e-05 Validation loss = 0.01003, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.008448431822041181\n",
      "Epoch: 1681 Training loss: 9.054e-05 Validation loss = 0.008448, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1682 Training loss: 8.736e-05 Validation loss = 0.01065, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1683 Training loss: 9.469e-05 Validation loss = 0.008689, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1684 Training loss: 9.356e-05 Validation loss = 0.01173, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1685 Training loss: 0.0001048 Validation loss = 0.009335, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1686 Training loss: 8.898e-05 Validation loss = 0.01096, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1687 Training loss: 9.719e-05 Validation loss = 0.009956, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1688 Training loss: 8.428e-05 Validation loss = 0.009981, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1689 Training loss: 8.849e-05 Validation loss = 0.009753, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1690 Training loss: 9.499e-05 Validation loss = 0.008725, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1691 Training loss: 9.559e-05 Validation loss = 0.01346, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1692 Training loss: 8.932e-05 Validation loss = 0.009252, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1693 Training loss: 9.438e-05 Validation loss = 0.01044, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1694 Training loss: 8.296e-05 Validation loss = 0.008548, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1695 Training loss: 0.0001029 Validation loss = 0.009895, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1696 Training loss: 9.755e-05 Validation loss = 0.009441, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1697 Training loss: 8.52e-05 Validation loss = 0.01051, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1698 Training loss: 8.892e-05 Validation loss = 0.01283, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1699 Training loss: 8.899e-05 Validation loss = 0.008495, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1700 Training loss: 0.0001056 Validation loss = 0.0108, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1701 Training loss: 8.456e-05 Validation loss = 0.009909, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1702 Training loss: 9.034e-05 Validation loss = 0.01221, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1703 Training loss: 9.585e-05 Validation loss = 0.008929, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1704 Training loss: 9.423e-05 Validation loss = 0.01006, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1705 Training loss: 8.787e-05 Validation loss = 0.01005, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1706 Training loss: 9.649e-05 Validation loss = 0.009546, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1707 Training loss: 9.097e-05 Validation loss = 0.01097, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1708 Training loss: 9.364e-05 Validation loss = 0.008931, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1709 Training loss: 8.803e-05 Validation loss = 0.009168, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1710 Training loss: 9.063e-05 Validation loss = 0.01341, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1711 Training loss: 8.886e-05 Validation loss = 0.00986, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1712 Training loss: 0.000102 Validation loss = 0.009305, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1713 Training loss: 8.503e-05 Validation loss = 0.009139, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1714 Training loss: 9.662e-05 Validation loss = 0.009306, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1715 Training loss: 8.811e-05 Validation loss = 0.01022, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1716 Training loss: 8.892e-05 Validation loss = 0.009125, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1717 Training loss: 9.028e-05 Validation loss = 0.008545, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1718 Training loss: 9.354e-05 Validation loss = 0.01071, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1719 Training loss: 9.317e-05 Validation loss = 0.008912, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1720 Training loss: 8.672e-05 Validation loss = 0.008505, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1721 Training loss: 9.094e-05 Validation loss = 0.009562, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1722 Training loss: 0.0001005 Validation loss = 0.01061, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1723 Training loss: 9.701e-05 Validation loss = 0.01156, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1724 Training loss: 8.969e-05 Validation loss = 0.00894, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1725 Training loss: 9.504e-05 Validation loss = 0.008451, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1726 Training loss: 7.84e-05 Validation loss = 0.009295, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1727 Training loss: 9.815e-05 Validation loss = 0.009263, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1728 Training loss: 9.503e-05 Validation loss = 0.009885, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1729 Training loss: 8.319e-05 Validation loss = 0.008979, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1730 Training loss: 9.805e-05 Validation loss = 0.01112, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1731 Training loss: 8.899e-05 Validation loss = 0.009815, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1732 Training loss: 9.027e-05 Validation loss = 0.009362, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1733 Training loss: 9.113e-05 Validation loss = 0.008922, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1734 Training loss: 9.281e-05 Validation loss = 0.01385, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1735 Training loss: 9.446e-05 Validation loss = 0.009605, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1736 Training loss: 8.754e-05 Validation loss = 0.009626, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1737 Training loss: 9.206e-05 Validation loss = 0.008696, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1738 Training loss: 8.934e-05 Validation loss = 0.009535, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1739 Training loss: 9.209e-05 Validation loss = 0.008887, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1740 Training loss: 8.697e-05 Validation loss = 0.00988, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1741 Training loss: 9.016e-05 Validation loss = 0.008646, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1742 Training loss: 9.251e-05 Validation loss = 0.009873, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1743 Training loss: 9.203e-05 Validation loss = 0.009987, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1744 Training loss: 9.048e-05 Validation loss = 0.01087, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1745 Training loss: 8.809e-05 Validation loss = 0.01092, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1746 Training loss: 9.812e-05 Validation loss = 0.009875, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1747 Training loss: 8.256e-05 Validation loss = 0.008917, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1748 Training loss: 0.0001006 Validation loss = 0.0092, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1749 Training loss: 8.177e-05 Validation loss = 0.01214, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1750 Training loss: 8.908e-05 Validation loss = 0.009563, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1751 Training loss: 9.05e-05 Validation loss = 0.00904, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1752 Training loss: 9.338e-05 Validation loss = 0.009642, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1753 Training loss: 8.927e-05 Validation loss = 0.01011, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1754 Training loss: 9.145e-05 Validation loss = 0.008531, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1755 Training loss: 9.45e-05 Validation loss = 0.01097, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1756 Training loss: 9.564e-05 Validation loss = 0.01016, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1757 Training loss: 8.265e-05 Validation loss = 0.009034, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1758 Training loss: 8.964e-05 Validation loss = 0.01042, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1759 Training loss: 8.851e-05 Validation loss = 0.00942, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1760 Training loss: 9.615e-05 Validation loss = 0.01031, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1761 Training loss: 9.768e-05 Validation loss = 0.01031, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1762 Training loss: 8.41e-05 Validation loss = 0.008674, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1763 Training loss: 0.0001028 Validation loss = 0.009824, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1764 Training loss: 7.355e-05 Validation loss = 0.00952, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1765 Training loss: 9.506e-05 Validation loss = 0.0106, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1766 Training loss: 8.347e-05 Validation loss = 0.01061, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1767 Training loss: 9.751e-05 Validation loss = 0.009208, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1768 Training loss: 0.0001017 Validation loss = 0.008762, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1769 Training loss: 7.571e-05 Validation loss = 0.01086, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1770 Training loss: 9.289e-05 Validation loss = 0.01052, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1771 Training loss: 8.765e-05 Validation loss = 0.01057, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1772 Training loss: 9.174e-05 Validation loss = 0.009257, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1773 Training loss: 8.776e-05 Validation loss = 0.01058, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1774 Training loss: 8.637e-05 Validation loss = 0.009012, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1775 Training loss: 9.03e-05 Validation loss = 0.01153, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1776 Training loss: 9.407e-05 Validation loss = 0.01124, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1777 Training loss: 8.993e-05 Validation loss = 0.008982, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1778 Training loss: 9.35e-05 Validation loss = 0.01001, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1779 Training loss: 8.419e-05 Validation loss = 0.008743, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1780 Training loss: 9.385e-05 Validation loss = 0.008465, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1781 Training loss: 9.172e-05 Validation loss = 0.008866, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1782 Training loss: 8.36e-05 Validation loss = 0.009114, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1783 Training loss: 9.312e-05 Validation loss = 0.009831, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1784 Training loss: 9.592e-05 Validation loss = 0.009559, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1785 Training loss: 8.44e-05 Validation loss = 0.01044, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1786 Training loss: 8.254e-05 Validation loss = 0.009186, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1787 Training loss: 9.945e-05 Validation loss = 0.009598, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1788 Training loss: 8.024e-05 Validation loss = 0.009968, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1789 Training loss: 9.783e-05 Validation loss = 0.009916, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1790 Training loss: 8.565e-05 Validation loss = 0.009529, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1791 Training loss: 8.79e-05 Validation loss = 0.009166, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1792 Training loss: 8.855e-05 Validation loss = 0.009187, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1793 Training loss: 8.896e-05 Validation loss = 0.01039, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1794 Training loss: 9.143e-05 Validation loss = 0.008987, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1795 Training loss: 9.508e-05 Validation loss = 0.009254, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1796 Training loss: 8.305e-05 Validation loss = 0.01025, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1797 Training loss: 8.928e-05 Validation loss = 0.009873, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1798 Training loss: 8.863e-05 Validation loss = 0.0142, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1799 Training loss: 8.953e-05 Validation loss = 0.009143, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1800 Training loss: 9.206e-05 Validation loss = 0.009712, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1801 Training loss: 8.877e-05 Validation loss = 0.01017, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1802 Training loss: 8.429e-05 Validation loss = 0.009773, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1803 Training loss: 9.907e-05 Validation loss = 0.009509, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.008321241224141365\n",
      "Epoch: 1804 Training loss: 8.163e-05 Validation loss = 0.008321, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.00829805384790384\n",
      "Epoch: 1805 Training loss: 9.45e-05 Validation loss = 0.008298, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1806 Training loss: 8.232e-05 Validation loss = 0.008814, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1807 Training loss: 9.284e-05 Validation loss = 0.01114, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1808 Training loss: 8.923e-05 Validation loss = 0.01007, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1809 Training loss: 9.508e-05 Validation loss = 0.008384, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1810 Training loss: 8.164e-05 Validation loss = 0.009534, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1811 Training loss: 9.001e-05 Validation loss = 0.009043, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1812 Training loss: 8.563e-05 Validation loss = 0.009207, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1813 Training loss: 8.791e-05 Validation loss = 0.01157, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1814 Training loss: 9.409e-05 Validation loss = 0.01179, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1815 Training loss: 7.999e-05 Validation loss = 0.01161, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1816 Training loss: 9.914e-05 Validation loss = 0.00942, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1817 Training loss: 8.142e-05 Validation loss = 0.009765, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1818 Training loss: 8.614e-05 Validation loss = 0.008819, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1819 Training loss: 9.04e-05 Validation loss = 0.01036, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1820 Training loss: 8.938e-05 Validation loss = 0.008509, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1821 Training loss: 8.562e-05 Validation loss = 0.01139, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1822 Training loss: 8.299e-05 Validation loss = 0.008844, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1823 Training loss: 0.0001026 Validation loss = 0.008992, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1824 Training loss: 8.416e-05 Validation loss = 0.008721, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1825 Training loss: 9.663e-05 Validation loss = 0.01044, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1826 Training loss: 7.983e-05 Validation loss = 0.008773, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1827 Training loss: 8.831e-05 Validation loss = 0.01019, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1828 Training loss: 8.79e-05 Validation loss = 0.01124, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1829 Training loss: 9.213e-05 Validation loss = 0.01074, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1830 Training loss: 9.12e-05 Validation loss = 0.01077, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1831 Training loss: 8.013e-05 Validation loss = 0.01025, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1832 Training loss: 8.626e-05 Validation loss = 0.009341, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1833 Training loss: 9.071e-05 Validation loss = 0.008554, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1834 Training loss: 9.455e-05 Validation loss = 0.008709, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1835 Training loss: 8.644e-05 Validation loss = 0.009598, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1836 Training loss: 8.603e-05 Validation loss = 0.008858, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1837 Training loss: 8.897e-05 Validation loss = 0.009836, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1838 Training loss: 8.473e-05 Validation loss = 0.008438, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1839 Training loss: 9.291e-05 Validation loss = 0.009285, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1840 Training loss: 8.338e-05 Validation loss = 0.009724, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1841 Training loss: 8.922e-05 Validation loss = 0.0201, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1842 Training loss: 9.443e-05 Validation loss = 0.009644, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1843 Training loss: 7.814e-05 Validation loss = 0.008552, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1844 Training loss: 8.552e-05 Validation loss = 0.009345, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.008285689314632852\n",
      "Epoch: 1845 Training loss: 9.141e-05 Validation loss = 0.008286, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1846 Training loss: 8.519e-05 Validation loss = 0.008663, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1847 Training loss: 8.925e-05 Validation loss = 0.008551, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1848 Training loss: 8.672e-05 Validation loss = 0.01114, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1849 Training loss: 8.739e-05 Validation loss = 0.009197, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1850 Training loss: 9.07e-05 Validation loss = 0.008767, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1851 Training loss: 8.332e-05 Validation loss = 0.008805, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1852 Training loss: 8.206e-05 Validation loss = 0.0103, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1853 Training loss: 9.817e-05 Validation loss = 0.01067, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1854 Training loss: 8.094e-05 Validation loss = 0.009737, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1855 Training loss: 9.288e-05 Validation loss = 0.008881, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1856 Training loss: 9.671e-05 Validation loss = 0.008554, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1857 Training loss: 9.241e-05 Validation loss = 0.0114, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1858 Training loss: 8.288e-05 Validation loss = 0.008713, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1859 Training loss: 8.801e-05 Validation loss = 0.01016, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1860 Training loss: 7.874e-05 Validation loss = 0.008667, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1861 Training loss: 8.843e-05 Validation loss = 0.01139, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1862 Training loss: 8.291e-05 Validation loss = 0.01058, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1863 Training loss: 9.299e-05 Validation loss = 0.00991, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1864 Training loss: 8.246e-05 Validation loss = 0.009267, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1865 Training loss: 8.51e-05 Validation loss = 0.00833, time Loss: 31.8%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1866 Training loss: 8.816e-05 Validation loss = 0.01106, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1867 Training loss: 8.992e-05 Validation loss = 0.01049, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1868 Training loss: 8.297e-05 Validation loss = 0.01034, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1869 Training loss: 9.676e-05 Validation loss = 0.008495, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1870 Training loss: 8.279e-05 Validation loss = 0.01126, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1871 Training loss: 8.46e-05 Validation loss = 0.009269, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1872 Training loss: 9.146e-05 Validation loss = 0.008516, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1873 Training loss: 8.12e-05 Validation loss = 0.009567, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1874 Training loss: 8.696e-05 Validation loss = 0.008394, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1875 Training loss: 9.12e-05 Validation loss = 0.008531, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1876 Training loss: 8.37e-05 Validation loss = 0.009604, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1877 Training loss: 8.551e-05 Validation loss = 0.01024, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1878 Training loss: 8.264e-05 Validation loss = 0.008924, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1879 Training loss: 9.066e-05 Validation loss = 0.0085, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1880 Training loss: 8.745e-05 Validation loss = 0.01056, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1881 Training loss: 8.302e-05 Validation loss = 0.009142, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1882 Training loss: 8.774e-05 Validation loss = 0.009229, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1883 Training loss: 8.912e-05 Validation loss = 0.008929, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1884 Training loss: 8.581e-05 Validation loss = 0.008595, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1885 Training loss: 8.477e-05 Validation loss = 0.01042, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1886 Training loss: 9.85e-05 Validation loss = 0.008289, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1887 Training loss: 8.179e-05 Validation loss = 0.008561, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1888 Training loss: 8.14e-05 Validation loss = 0.01199, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1889 Training loss: 8.57e-05 Validation loss = 0.01209, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1890 Training loss: 8.685e-05 Validation loss = 0.008824, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1891 Training loss: 8.762e-05 Validation loss = 0.008934, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1892 Training loss: 8.665e-05 Validation loss = 0.009666, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1893 Training loss: 8.09e-05 Validation loss = 0.01032, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1894 Training loss: 8.844e-05 Validation loss = 0.008769, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1895 Training loss: 8.256e-05 Validation loss = 0.009247, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1896 Training loss: 8.598e-05 Validation loss = 0.00871, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1897 Training loss: 9.8e-05 Validation loss = 0.009381, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1898 Training loss: 7.952e-05 Validation loss = 0.0121, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.008162805669904988\n",
      "Epoch: 1899 Training loss: 8.332e-05 Validation loss = 0.008163, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1900 Training loss: 8.164e-05 Validation loss = 0.009009, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1901 Training loss: 9.237e-05 Validation loss = 0.008644, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1902 Training loss: 8.386e-05 Validation loss = 0.008684, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1903 Training loss: 8.325e-05 Validation loss = 0.009596, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1904 Training loss: 8.669e-05 Validation loss = 0.009053, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1905 Training loss: 8.084e-05 Validation loss = 0.0111, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1906 Training loss: 9.347e-05 Validation loss = 0.008604, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1907 Training loss: 8.059e-05 Validation loss = 0.008839, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1908 Training loss: 8.847e-05 Validation loss = 0.01271, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1909 Training loss: 8.45e-05 Validation loss = 0.01136, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1910 Training loss: 8.958e-05 Validation loss = 0.009981, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1911 Training loss: 8.099e-05 Validation loss = 0.01086, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1912 Training loss: 8.932e-05 Validation loss = 0.01037, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1913 Training loss: 8.394e-05 Validation loss = 0.01007, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1914 Training loss: 8.58e-05 Validation loss = 0.008911, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1915 Training loss: 8.455e-05 Validation loss = 0.008604, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1916 Training loss: 8.004e-05 Validation loss = 0.008266, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1917 Training loss: 9.168e-05 Validation loss = 0.01029, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1918 Training loss: 8.208e-05 Validation loss = 0.01127, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1919 Training loss: 8.952e-05 Validation loss = 0.009036, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1920 Training loss: 8.066e-05 Validation loss = 0.01027, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1921 Training loss: 9.05e-05 Validation loss = 0.01061, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1922 Training loss: 7.806e-05 Validation loss = 0.008253, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1923 Training loss: 9.037e-05 Validation loss = 0.009003, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1924 Training loss: 8.733e-05 Validation loss = 0.008368, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1925 Training loss: 8.033e-05 Validation loss = 0.01065, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1926 Training loss: 9.545e-05 Validation loss = 0.02111, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1927 Training loss: 8.725e-05 Validation loss = 0.008744, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1928 Training loss: 7.807e-05 Validation loss = 0.008292, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1929 Training loss: 8.098e-05 Validation loss = 0.00942, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1930 Training loss: 8.718e-05 Validation loss = 0.009822, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1931 Training loss: 8.505e-05 Validation loss = 0.008807, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1932 Training loss: 8.165e-05 Validation loss = 0.01204, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1933 Training loss: 8.639e-05 Validation loss = 0.00976, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1934 Training loss: 8.923e-05 Validation loss = 0.009261, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1935 Training loss: 8.522e-05 Validation loss = 0.0109, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1936 Training loss: 8.408e-05 Validation loss = 0.00839, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1937 Training loss: 7.676e-05 Validation loss = 0.008678, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1938 Training loss: 9.581e-05 Validation loss = 0.008308, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1939 Training loss: 7.485e-05 Validation loss = 0.00845, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1940 Training loss: 9.472e-05 Validation loss = 0.008592, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1941 Training loss: 7.748e-05 Validation loss = 0.009443, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1942 Training loss: 8.644e-05 Validation loss = 0.01246, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1943 Training loss: 8.19e-05 Validation loss = 0.01024, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1944 Training loss: 8.372e-05 Validation loss = 0.009886, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1945 Training loss: 8.385e-05 Validation loss = 0.008399, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1946 Training loss: 8.534e-05 Validation loss = 0.00848, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1947 Training loss: 8.533e-05 Validation loss = 0.01035, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1948 Training loss: 7.973e-05 Validation loss = 0.008443, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1949 Training loss: 9.423e-05 Validation loss = 0.009492, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1950 Training loss: 7.488e-05 Validation loss = 0.00971, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1951 Training loss: 9.024e-05 Validation loss = 0.01134, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1952 Training loss: 7.996e-05 Validation loss = 0.01001, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1953 Training loss: 8.825e-05 Validation loss = 0.009939, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1954 Training loss: 8.457e-05 Validation loss = 0.00876, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1955 Training loss: 7.591e-05 Validation loss = 0.0107, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1956 Training loss: 9.054e-05 Validation loss = 0.008766, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1957 Training loss: 8.251e-05 Validation loss = 0.01039, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1958 Training loss: 8.132e-05 Validation loss = 0.00992, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1959 Training loss: 8.499e-05 Validation loss = 0.009888, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1960 Training loss: 8.713e-05 Validation loss = 0.01095, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1961 Training loss: 8.303e-05 Validation loss = 0.009595, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1962 Training loss: 8.275e-05 Validation loss = 0.009704, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1963 Training loss: 8.763e-05 Validation loss = 0.008329, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1964 Training loss: 8.226e-05 Validation loss = 0.009772, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1965 Training loss: 7.253e-05 Validation loss = 0.01066, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1966 Training loss: 9.707e-05 Validation loss = 0.01156, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1967 Training loss: 7.942e-05 Validation loss = 0.008552, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1968 Training loss: 8.06e-05 Validation loss = 0.008631, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1969 Training loss: 8.315e-05 Validation loss = 0.008464, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1970 Training loss: 9.143e-05 Validation loss = 0.008956, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1971 Training loss: 7.913e-05 Validation loss = 0.0106, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1972 Training loss: 9.289e-05 Validation loss = 0.008947, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1973 Training loss: 7.182e-05 Validation loss = 0.008297, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1974 Training loss: 8.81e-05 Validation loss = 0.01124, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1975 Training loss: 8.067e-05 Validation loss = 0.01013, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1976 Training loss: 9.141e-05 Validation loss = 0.01178, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1977 Training loss: 7.997e-05 Validation loss = 0.009533, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1978 Training loss: 7.902e-05 Validation loss = 0.008374, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1979 Training loss: 8.376e-05 Validation loss = 0.008879, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.008086281582843512\n",
      "Epoch: 1980 Training loss: 8.169e-05 Validation loss = 0.008086, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1981 Training loss: 8.098e-05 Validation loss = 0.00934, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1982 Training loss: 9.467e-05 Validation loss = 0.009177, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1983 Training loss: 7.489e-05 Validation loss = 0.01033, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1984 Training loss: 7.808e-05 Validation loss = 0.009464, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.008085980097538725\n",
      "Epoch: 1985 Training loss: 9.787e-05 Validation loss = 0.008086, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1986 Training loss: 8.486e-05 Validation loss = 0.0086, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1987 Training loss: 8.431e-05 Validation loss = 0.0101, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1988 Training loss: 7.243e-05 Validation loss = 0.01012, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1989 Training loss: 8.29e-05 Validation loss = 0.009879, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1990 Training loss: 9.028e-05 Validation loss = 0.01023, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1991 Training loss: 7.684e-05 Validation loss = 0.01523, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1992 Training loss: 8.596e-05 Validation loss = 0.008768, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1993 Training loss: 7.538e-05 Validation loss = 0.009444, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1994 Training loss: 8.461e-05 Validation loss = 0.008484, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1995 Training loss: 9.067e-05 Validation loss = 0.0088, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1996 Training loss: 7.443e-05 Validation loss = 0.008361, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1997 Training loss: 8.017e-05 Validation loss = 0.01021, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1998 Training loss: 9.176e-05 Validation loss = 0.01275, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 1999 Training loss: 7.914e-05 Validation loss = 0.00842, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2000 Training loss: 7.703e-05 Validation loss = 0.008659, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2001 Training loss: 8.267e-05 Validation loss = 0.01189, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2002 Training loss: 8.373e-05 Validation loss = 0.008678, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2003 Training loss: 8.518e-05 Validation loss = 0.01211, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2004 Training loss: 8.57e-05 Validation loss = 0.008741, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2005 Training loss: 7.8e-05 Validation loss = 0.008168, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2006 Training loss: 8.997e-05 Validation loss = 0.009278, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2007 Training loss: 7.767e-05 Validation loss = 0.0148, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2008 Training loss: 8.607e-05 Validation loss = 0.008343, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2009 Training loss: 8.055e-05 Validation loss = 0.01208, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2010 Training loss: 7.939e-05 Validation loss = 0.008481, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2011 Training loss: 8.663e-05 Validation loss = 0.01002, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2012 Training loss: 7.5e-05 Validation loss = 0.00982, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2013 Training loss: 7.897e-05 Validation loss = 0.009314, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2014 Training loss: 8.656e-05 Validation loss = 0.009032, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2015 Training loss: 8.147e-05 Validation loss = 0.009034, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2016 Training loss: 7.7e-05 Validation loss = 0.008289, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2017 Training loss: 9.476e-05 Validation loss = 0.009163, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2018 Training loss: 7.039e-05 Validation loss = 0.008298, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2019 Training loss: 9.441e-05 Validation loss = 0.01112, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2020 Training loss: 7.153e-05 Validation loss = 0.01047, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2021 Training loss: 9.436e-05 Validation loss = 0.008448, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2022 Training loss: 7.186e-05 Validation loss = 0.009903, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.008006934299751436\n",
      "Epoch: 2023 Training loss: 8.069e-05 Validation loss = 0.008007, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2024 Training loss: 8.395e-05 Validation loss = 0.008923, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2025 Training loss: 7.761e-05 Validation loss = 0.008479, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2026 Training loss: 8.278e-05 Validation loss = 0.01157, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2027 Training loss: 8.153e-05 Validation loss = 0.009382, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2028 Training loss: 8.013e-05 Validation loss = 0.008761, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2029 Training loss: 8.396e-05 Validation loss = 0.009189, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2030 Training loss: 7.911e-05 Validation loss = 0.008349, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2031 Training loss: 9.285e-05 Validation loss = 0.01006, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2032 Training loss: 7.293e-05 Validation loss = 0.009965, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2033 Training loss: 8.115e-05 Validation loss = 0.009049, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2034 Training loss: 7.836e-05 Validation loss = 0.009466, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2035 Training loss: 9.024e-05 Validation loss = 0.008487, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2036 Training loss: 7.857e-05 Validation loss = 0.008253, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0080024302262079\n",
      "Epoch: 2037 Training loss: 8.332e-05 Validation loss = 0.008002, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2038 Training loss: 7.647e-05 Validation loss = 0.008021, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2039 Training loss: 7.683e-05 Validation loss = 0.009901, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2040 Training loss: 8.27e-05 Validation loss = 0.009366, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2041 Training loss: 7.975e-05 Validation loss = 0.01167, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2042 Training loss: 8.966e-05 Validation loss = 0.009106, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2043 Training loss: 7.284e-05 Validation loss = 0.008366, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2044 Training loss: 8.675e-05 Validation loss = 0.01052, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2045 Training loss: 7.289e-05 Validation loss = 0.009181, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2046 Training loss: 9.636e-05 Validation loss = 0.008529, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2047 Training loss: 7.576e-05 Validation loss = 0.009059, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2048 Training loss: 7.171e-05 Validation loss = 0.008143, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2049 Training loss: 8.026e-05 Validation loss = 0.008132, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.007991216311480673\n",
      "Epoch: 2050 Training loss: 9.008e-05 Validation loss = 0.007991, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2051 Training loss: 8.289e-05 Validation loss = 0.008971, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2052 Training loss: 7.095e-05 Validation loss = 0.009017, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2053 Training loss: 8.684e-05 Validation loss = 0.008695, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2054 Training loss: 7.299e-05 Validation loss = 0.01046, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2055 Training loss: 8.326e-05 Validation loss = 0.009193, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2056 Training loss: 7.574e-05 Validation loss = 0.008729, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2057 Training loss: 8.379e-05 Validation loss = 0.008353, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2058 Training loss: 8.046e-05 Validation loss = 0.008559, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2059 Training loss: 8.309e-05 Validation loss = 0.008649, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2060 Training loss: 7.628e-05 Validation loss = 0.009074, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2061 Training loss: 8.083e-05 Validation loss = 0.01014, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.007899483044809696\n",
      "Epoch: 2062 Training loss: 7.972e-05 Validation loss = 0.007899, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2063 Training loss: 7.727e-05 Validation loss = 0.009372, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2064 Training loss: 8.354e-05 Validation loss = 0.008881, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2065 Training loss: 8.286e-05 Validation loss = 0.009063, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2066 Training loss: 8.316e-05 Validation loss = 0.009089, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2067 Training loss: 7.197e-05 Validation loss = 0.008664, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2068 Training loss: 8.13e-05 Validation loss = 0.01062, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2069 Training loss: 8.05e-05 Validation loss = 0.008064, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2070 Training loss: 8.188e-05 Validation loss = 0.009108, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2071 Training loss: 7.897e-05 Validation loss = 0.008891, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2072 Training loss: 8.309e-05 Validation loss = 0.01069, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2073 Training loss: 7.721e-05 Validation loss = 0.009739, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2074 Training loss: 7.972e-05 Validation loss = 0.00945, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2075 Training loss: 7.916e-05 Validation loss = 0.008808, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2076 Training loss: 8.297e-05 Validation loss = 0.008336, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2077 Training loss: 7.676e-05 Validation loss = 0.009276, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2078 Training loss: 8.248e-05 Validation loss = 0.008641, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2079 Training loss: 7.41e-05 Validation loss = 0.009223, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2080 Training loss: 7.463e-05 Validation loss = 0.008969, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2081 Training loss: 9.466e-05 Validation loss = 0.008671, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2082 Training loss: 7.179e-05 Validation loss = 0.009927, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2083 Training loss: 7.716e-05 Validation loss = 0.009653, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2084 Training loss: 8.151e-05 Validation loss = 0.01015, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2085 Training loss: 7.43e-05 Validation loss = 0.008947, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2086 Training loss: 8.493e-05 Validation loss = 0.00835, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2087 Training loss: 7.88e-05 Validation loss = 0.009135, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2088 Training loss: 8.502e-05 Validation loss = 0.008689, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2089 Training loss: 7.068e-05 Validation loss = 0.008195, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2090 Training loss: 8.056e-05 Validation loss = 0.00913, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.007881483213626712\n",
      "Epoch: 2091 Training loss: 7.27e-05 Validation loss = 0.007881, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2092 Training loss: 8.799e-05 Validation loss = 0.008149, time Loss: 31.8%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.007848580236169092\n",
      "Epoch: 2093 Training loss: 7.415e-05 Validation loss = 0.007849, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2094 Training loss: 7.983e-05 Validation loss = 0.007906, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2095 Training loss: 7.495e-05 Validation loss = 0.009797, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2096 Training loss: 7.605e-05 Validation loss = 0.007912, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2097 Training loss: 8.093e-05 Validation loss = 0.008704, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2098 Training loss: 8.594e-05 Validation loss = 0.008987, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2099 Training loss: 7.459e-05 Validation loss = 0.01033, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2100 Training loss: 8.16e-05 Validation loss = 0.008045, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2101 Training loss: 7.583e-05 Validation loss = 0.008347, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2102 Training loss: 7.479e-05 Validation loss = 0.008036, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2103 Training loss: 8.338e-05 Validation loss = 0.007971, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2104 Training loss: 7.119e-05 Validation loss = 0.008122, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2105 Training loss: 8.56e-05 Validation loss = 0.01013, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2106 Training loss: 8.665e-05 Validation loss = 0.01372, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2107 Training loss: 7.402e-05 Validation loss = 0.00836, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2108 Training loss: 8.449e-05 Validation loss = 0.008204, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2109 Training loss: 6.332e-05 Validation loss = 0.00857, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2110 Training loss: 9.702e-05 Validation loss = 0.009418, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2111 Training loss: 8.69e-05 Validation loss = 0.009484, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2112 Training loss: 6.858e-05 Validation loss = 0.008828, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2113 Training loss: 6.914e-05 Validation loss = 0.01232, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2114 Training loss: 7.14e-05 Validation loss = 0.009089, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2115 Training loss: 8.227e-05 Validation loss = 0.008308, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2116 Training loss: 7.621e-05 Validation loss = 0.008765, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2117 Training loss: 8.456e-05 Validation loss = 0.0104, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2118 Training loss: 6.939e-05 Validation loss = 0.00854, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2119 Training loss: 8.214e-05 Validation loss = 0.00827, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.007791262655081719\n",
      "Epoch: 2120 Training loss: 7.693e-05 Validation loss = 0.007791, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2121 Training loss: 7.35e-05 Validation loss = 0.008356, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2122 Training loss: 8.149e-05 Validation loss = 0.008889, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2123 Training loss: 7.922e-05 Validation loss = 0.008929, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2124 Training loss: 7.531e-05 Validation loss = 0.008021, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2125 Training loss: 7.648e-05 Validation loss = 0.00836, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2126 Training loss: 8.195e-05 Validation loss = 0.01169, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2127 Training loss: 7.36e-05 Validation loss = 0.008263, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2128 Training loss: 7.909e-05 Validation loss = 0.008682, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2129 Training loss: 7.495e-05 Validation loss = 0.007879, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2130 Training loss: 7.436e-05 Validation loss = 0.008078, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0077345379569770415\n",
      "Epoch: 2131 Training loss: 7.966e-05 Validation loss = 0.007735, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2132 Training loss: 7.593e-05 Validation loss = 0.008117, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2133 Training loss: 7.623e-05 Validation loss = 0.008603, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2134 Training loss: 7.654e-05 Validation loss = 0.007957, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2135 Training loss: 7.125e-05 Validation loss = 0.009092, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2136 Training loss: 7.401e-05 Validation loss = 0.009778, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2137 Training loss: 8.924e-05 Validation loss = 0.008471, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2138 Training loss: 6.872e-05 Validation loss = 0.009739, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2139 Training loss: 7.439e-05 Validation loss = 0.008021, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2140 Training loss: 7.438e-05 Validation loss = 0.008076, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2141 Training loss: 8.613e-05 Validation loss = 0.01084, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2142 Training loss: 7.504e-05 Validation loss = 0.008101, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2143 Training loss: 7.707e-05 Validation loss = 0.009529, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2144 Training loss: 7.596e-05 Validation loss = 0.009001, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2145 Training loss: 7.671e-05 Validation loss = 0.009779, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2146 Training loss: 7.72e-05 Validation loss = 0.008847, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2147 Training loss: 7.141e-05 Validation loss = 0.008457, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2148 Training loss: 7.282e-05 Validation loss = 0.008395, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2149 Training loss: 8.201e-05 Validation loss = 0.008947, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2150 Training loss: 7.415e-05 Validation loss = 0.00786, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2151 Training loss: 7.754e-05 Validation loss = 0.009098, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2152 Training loss: 7.399e-05 Validation loss = 0.007827, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2153 Training loss: 7.679e-05 Validation loss = 0.007848, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2154 Training loss: 7.397e-05 Validation loss = 0.008167, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2155 Training loss: 7.748e-05 Validation loss = 0.009242, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2156 Training loss: 7.583e-05 Validation loss = 0.007738, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2157 Training loss: 7.218e-05 Validation loss = 0.007822, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2158 Training loss: 7.444e-05 Validation loss = 0.009306, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2159 Training loss: 7.759e-05 Validation loss = 0.009694, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.007705990045507216\n",
      "Epoch: 2160 Training loss: 7.29e-05 Validation loss = 0.007706, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2161 Training loss: 7.219e-05 Validation loss = 0.008893, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2162 Training loss: 8.517e-05 Validation loss = 0.007801, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2163 Training loss: 6.922e-05 Validation loss = 0.008703, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2164 Training loss: 7.382e-05 Validation loss = 0.008072, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2165 Training loss: 7.488e-05 Validation loss = 0.00785, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2166 Training loss: 7.248e-05 Validation loss = 0.008683, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2167 Training loss: 7.649e-05 Validation loss = 0.008084, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2168 Training loss: 7.144e-05 Validation loss = 0.007859, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2169 Training loss: 8.425e-05 Validation loss = 0.008457, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2170 Training loss: 6.596e-05 Validation loss = 0.008411, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2171 Training loss: 7.582e-05 Validation loss = 0.008784, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2172 Training loss: 7.042e-05 Validation loss = 0.009757, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2173 Training loss: 8.03e-05 Validation loss = 0.00827, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2174 Training loss: 7.592e-05 Validation loss = 0.008763, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2175 Training loss: 7.48e-05 Validation loss = 0.01094, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.007614073691174552\n",
      "Epoch: 2176 Training loss: 7.127e-05 Validation loss = 0.007614, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2177 Training loss: 7.151e-05 Validation loss = 0.007863, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2178 Training loss: 7.034e-05 Validation loss = 0.007655, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2179 Training loss: 7.416e-05 Validation loss = 0.009175, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2180 Training loss: 8.341e-05 Validation loss = 0.008584, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2181 Training loss: 6.482e-05 Validation loss = 0.009397, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2182 Training loss: 7.743e-05 Validation loss = 0.0081, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2183 Training loss: 7.467e-05 Validation loss = 0.008908, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.007475624882633441\n",
      "Epoch: 2184 Training loss: 6.889e-05 Validation loss = 0.007476, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2185 Training loss: 7.588e-05 Validation loss = 0.008605, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2186 Training loss: 7.437e-05 Validation loss = 0.009744, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2187 Training loss: 6.711e-05 Validation loss = 0.01178, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2188 Training loss: 7.042e-05 Validation loss = 0.007704, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2189 Training loss: 8.129e-05 Validation loss = 0.008011, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2190 Training loss: 6.753e-05 Validation loss = 0.008173, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2191 Training loss: 8.196e-05 Validation loss = 0.009573, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2192 Training loss: 7.24e-05 Validation loss = 0.008062, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2193 Training loss: 6.48e-05 Validation loss = 0.008689, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2194 Training loss: 6.723e-05 Validation loss = 0.009088, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2195 Training loss: 8.473e-05 Validation loss = 0.007851, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2196 Training loss: 6.776e-05 Validation loss = 0.009595, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2197 Training loss: 7.729e-05 Validation loss = 0.008892, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2198 Training loss: 7.822e-05 Validation loss = 0.01008, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2199 Training loss: 6.858e-05 Validation loss = 0.007615, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2200 Training loss: 6.307e-05 Validation loss = 0.007614, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2201 Training loss: 7.64e-05 Validation loss = 0.01007, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2202 Training loss: 7.044e-05 Validation loss = 0.009266, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2203 Training loss: 6.979e-05 Validation loss = 0.009352, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2204 Training loss: 7.212e-05 Validation loss = 0.01017, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2205 Training loss: 7.605e-05 Validation loss = 0.008039, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.007451872665213675\n",
      "Epoch: 2206 Training loss: 7.086e-05 Validation loss = 0.007452, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2207 Training loss: 6.565e-05 Validation loss = 0.008716, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2208 Training loss: 7.103e-05 Validation loss = 0.008875, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2209 Training loss: 6.721e-05 Validation loss = 0.00818, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2210 Training loss: 7.783e-05 Validation loss = 0.009685, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2211 Training loss: 6.618e-05 Validation loss = 0.007618, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2212 Training loss: 7.404e-05 Validation loss = 0.008965, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2213 Training loss: 7.383e-05 Validation loss = 0.01014, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2214 Training loss: 6.972e-05 Validation loss = 0.00864, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2215 Training loss: 6.773e-05 Validation loss = 0.01028, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2216 Training loss: 7.118e-05 Validation loss = 0.01047, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.007273226428705714\n",
      "Epoch: 2217 Training loss: 7.599e-05 Validation loss = 0.007273, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2218 Training loss: 6.671e-05 Validation loss = 0.007757, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2219 Training loss: 6.892e-05 Validation loss = 0.007651, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2220 Training loss: 7.158e-05 Validation loss = 0.00815, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2221 Training loss: 6.568e-05 Validation loss = 0.007604, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.007243766118864783\n",
      "Epoch: 2222 Training loss: 6.86e-05 Validation loss = 0.007244, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2223 Training loss: 6.819e-05 Validation loss = 0.008833, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2224 Training loss: 6.713e-05 Validation loss = 0.01306, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2225 Training loss: 7.905e-05 Validation loss = 0.009432, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2226 Training loss: 6.237e-05 Validation loss = 0.008696, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2227 Training loss: 7.248e-05 Validation loss = 0.009028, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.007168209631848532\n",
      "Epoch: 2228 Training loss: 6.913e-05 Validation loss = 0.007168, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2229 Training loss: 6.614e-05 Validation loss = 0.007428, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2230 Training loss: 6.75e-05 Validation loss = 0.008488, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2231 Training loss: 6.929e-05 Validation loss = 0.009686, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2232 Training loss: 7.05e-05 Validation loss = 0.01081, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2233 Training loss: 7.831e-05 Validation loss = 0.008034, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2234 Training loss: 5.467e-05 Validation loss = 0.007273, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2235 Training loss: 6.975e-05 Validation loss = 0.007732, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2236 Training loss: 6.585e-05 Validation loss = 0.007718, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2237 Training loss: 6.99e-05 Validation loss = 0.01001, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2238 Training loss: 7.059e-05 Validation loss = 0.007833, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2239 Training loss: 6.661e-05 Validation loss = 0.009406, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.007135178467234915\n",
      "Epoch: 2240 Training loss: 6.439e-05 Validation loss = 0.007135, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2241 Training loss: 7.453e-05 Validation loss = 0.008488, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2242 Training loss: 6.468e-05 Validation loss = 0.009792, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2243 Training loss: 6.298e-05 Validation loss = 0.007317, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2244 Training loss: 6.836e-05 Validation loss = 0.008253, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2245 Training loss: 6.26e-05 Validation loss = 0.009382, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2246 Training loss: 6.757e-05 Validation loss = 0.01066, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2247 Training loss: 6.924e-05 Validation loss = 0.007399, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2248 Training loss: 6.416e-05 Validation loss = 0.008736, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2249 Training loss: 6.935e-05 Validation loss = 0.009628, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2250 Training loss: 6.833e-05 Validation loss = 0.008025, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2251 Training loss: 5.853e-05 Validation loss = 0.009008, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2252 Training loss: 7.393e-05 Validation loss = 0.007971, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2253 Training loss: 5.863e-05 Validation loss = 0.008197, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2254 Training loss: 6.772e-05 Validation loss = 0.007572, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2255 Training loss: 7.536e-05 Validation loss = 0.008331, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2256 Training loss: 6.218e-05 Validation loss = 0.01058, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2257 Training loss: 5.85e-05 Validation loss = 0.009664, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.007090836293343976\n",
      "Epoch: 2258 Training loss: 6.886e-05 Validation loss = 0.007091, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2259 Training loss: 6.394e-05 Validation loss = 0.008596, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2260 Training loss: 6.295e-05 Validation loss = 0.007865, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2261 Training loss: 6.795e-05 Validation loss = 0.007777, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2262 Training loss: 6.532e-05 Validation loss = 0.009669, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2263 Training loss: 6.081e-05 Validation loss = 0.007635, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.006966460621398601\n",
      "Epoch: 2264 Training loss: 6.273e-05 Validation loss = 0.006966, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2265 Training loss: 6.605e-05 Validation loss = 0.007056, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.006793689774060109\n",
      "Epoch: 2266 Training loss: 6.84e-05 Validation loss = 0.006794, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2267 Training loss: 5.526e-05 Validation loss = 0.01124, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2268 Training loss: 7.258e-05 Validation loss = 0.00839, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2269 Training loss: 5.815e-05 Validation loss = 0.007321, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2270 Training loss: 7.09e-05 Validation loss = 0.008145, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2271 Training loss: 5.74e-05 Validation loss = 0.009898, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2272 Training loss: 7.032e-05 Validation loss = 0.007317, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2273 Training loss: 6.517e-05 Validation loss = 0.008469, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2274 Training loss: 5.554e-05 Validation loss = 0.009782, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2275 Training loss: 7.043e-05 Validation loss = 0.008508, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2276 Training loss: 5.7e-05 Validation loss = 0.00704, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2277 Training loss: 6.247e-05 Validation loss = 0.007758, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2278 Training loss: 6.515e-05 Validation loss = 0.007713, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2279 Training loss: 5.863e-05 Validation loss = 0.008498, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2280 Training loss: 6.65e-05 Validation loss = 0.007428, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2281 Training loss: 6.744e-05 Validation loss = 0.009046, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2282 Training loss: 5.793e-05 Validation loss = 0.006888, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2283 Training loss: 6.241e-05 Validation loss = 0.008498, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2284 Training loss: 6.3e-05 Validation loss = 0.009375, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2285 Training loss: 5.929e-05 Validation loss = 0.006865, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2286 Training loss: 5.962e-05 Validation loss = 0.006975, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2287 Training loss: 6.49e-05 Validation loss = 0.008205, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2288 Training loss: 5.677e-05 Validation loss = 0.00854, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2289 Training loss: 6.303e-05 Validation loss = 0.009552, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2290 Training loss: 6.799e-05 Validation loss = 0.008093, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2291 Training loss: 5.686e-05 Validation loss = 0.008618, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0066324011704598064\n",
      "Epoch: 2292 Training loss: 6.211e-05 Validation loss = 0.006632, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2293 Training loss: 6.432e-05 Validation loss =  0.013, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.006480189036093947\n",
      "Epoch: 2294 Training loss: 5.709e-05 Validation loss = 0.00648, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2295 Training loss: 6.22e-05 Validation loss = 0.009273, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2296 Training loss: 6.147e-05 Validation loss = 0.008461, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2297 Training loss: 6.61e-05 Validation loss = 0.01097, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2298 Training loss: 5.316e-05 Validation loss = 0.008995, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2299 Training loss: 5.936e-05 Validation loss = 0.006861, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2300 Training loss: 6.074e-05 Validation loss = 0.006792, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2301 Training loss: 5.89e-05 Validation loss = 0.008575, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2302 Training loss: 6.369e-05 Validation loss = 0.006824, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2303 Training loss: 5.993e-05 Validation loss = 0.008911, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2304 Training loss: 5.409e-05 Validation loss = 0.01004, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2305 Training loss: 6.825e-05 Validation loss = 0.007528, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2306 Training loss: 5.613e-05 Validation loss = 0.009613, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2307 Training loss: 6.42e-05 Validation loss = 0.009945, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2308 Training loss: 5.448e-05 Validation loss = 0.007206, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2309 Training loss: 7.415e-05 Validation loss = 0.006889, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2310 Training loss: 4.909e-05 Validation loss = 0.007389, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2311 Training loss: 5.879e-05 Validation loss = 0.007942, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2312 Training loss: 6.202e-05 Validation loss = 0.006557, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2313 Training loss: 5.61e-05 Validation loss = 0.007537, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2314 Training loss: 5.503e-05 Validation loss = 0.00712, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2315 Training loss: 6.057e-05 Validation loss = 0.007205, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2316 Training loss: 6.683e-05 Validation loss = 0.00721, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2317 Training loss: 5.07e-05 Validation loss = 0.007232, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2318 Training loss: 6.561e-05 Validation loss = 0.006979, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2319 Training loss: 5.855e-05 Validation loss = 0.006743, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2320 Training loss: 5.892e-05 Validation loss = 0.007169, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2321 Training loss: 5.906e-05 Validation loss = 0.008286, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2322 Training loss: 5.682e-05 Validation loss = 0.007143, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2323 Training loss: 5.79e-05 Validation loss = 0.007274, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2324 Training loss: 5.724e-05 Validation loss = 0.006646, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2325 Training loss: 5.43e-05 Validation loss = 0.007262, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.006384666590921971\n",
      "Epoch: 2326 Training loss: 7.033e-05 Validation loss = 0.006385, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2327 Training loss: 5.791e-05 Validation loss = 0.006964, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2328 Training loss: 4.998e-05 Validation loss = 0.008892, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2329 Training loss: 6.111e-05 Validation loss = 0.009463, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2330 Training loss: 5.826e-05 Validation loss = 0.007726, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2331 Training loss: 5.926e-05 Validation loss = 0.007446, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2332 Training loss: 5.517e-05 Validation loss = 0.0066, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2333 Training loss: 5.859e-05 Validation loss = 0.007663, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2334 Training loss: 6.247e-05 Validation loss = 0.007372, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2335 Training loss: 5.677e-05 Validation loss = 0.006506, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2336 Training loss: 5.322e-05 Validation loss = 0.008242, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2337 Training loss: 5.927e-05 Validation loss = 0.009398, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2338 Training loss: 5.341e-05 Validation loss = 0.007296, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2339 Training loss: 6.295e-05 Validation loss = 0.007101, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2340 Training loss: 5.006e-05 Validation loss = 0.009812, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2341 Training loss: 5.693e-05 Validation loss = 0.006504, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2342 Training loss: 5.681e-05 Validation loss = 0.008368, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2343 Training loss: 6.058e-05 Validation loss = 0.006871, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2344 Training loss: 5.254e-05 Validation loss = 0.009541, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2345 Training loss: 6.366e-05 Validation loss = 0.009145, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.006313482951733184\n",
      "Epoch: 2346 Training loss: 5.429e-05 Validation loss = 0.006313, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2347 Training loss: 5.859e-05 Validation loss = 0.008867, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2348 Training loss: 5.383e-05 Validation loss = 0.006729, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2349 Training loss: 5.981e-05 Validation loss = 0.01456, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.006240009545313483\n",
      "Epoch: 2350 Training loss: 5.635e-05 Validation loss = 0.00624, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2351 Training loss: 5.631e-05 Validation loss = 0.008282, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2352 Training loss: 5.562e-05 Validation loss = 0.008232, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2353 Training loss: 6.552e-05 Validation loss = 0.007336, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.006050250680921521\n",
      "Epoch: 2354 Training loss: 5.042e-05 Validation loss = 0.00605, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2355 Training loss: 5.486e-05 Validation loss = 0.007365, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2356 Training loss: 5.479e-05 Validation loss = 0.007482, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2357 Training loss: 5.753e-05 Validation loss = 0.007175, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2358 Training loss: 5.901e-05 Validation loss = 0.007997, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2359 Training loss: 5.627e-05 Validation loss = 0.006562, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2360 Training loss: 5.047e-05 Validation loss =  0.011, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2361 Training loss: 5.754e-05 Validation loss = 0.007917, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2362 Training loss: 5.765e-05 Validation loss = 0.00897, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2363 Training loss: 4.784e-05 Validation loss = 0.008577, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2364 Training loss: 7.318e-05 Validation loss = 0.00636, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2365 Training loss: 4.795e-05 Validation loss = 0.006325, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2366 Training loss: 5.97e-05 Validation loss = 0.006138, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2367 Training loss: 5.086e-05 Validation loss = 0.009068, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2368 Training loss: 5.222e-05 Validation loss = 0.008952, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2369 Training loss: 5.857e-05 Validation loss = 0.006455, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2370 Training loss: 5.179e-05 Validation loss = 0.009084, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2371 Training loss: 5.973e-05 Validation loss = 0.006464, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2372 Training loss: 4.718e-05 Validation loss = 0.006919, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2373 Training loss: 6.856e-05 Validation loss = 0.00646, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2374 Training loss: 4.536e-05 Validation loss = 0.008625, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2375 Training loss: 5.735e-05 Validation loss = 0.01001, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2376 Training loss: 5.431e-05 Validation loss = 0.006651, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2377 Training loss: 5.311e-05 Validation loss = 0.009165, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2378 Training loss: 5.691e-05 Validation loss = 0.006433, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2379 Training loss: 5.228e-05 Validation loss = 0.007323, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2380 Training loss: 6.718e-05 Validation loss = 0.007107, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2381 Training loss: 4.762e-05 Validation loss = 0.006053, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2382 Training loss: 5.195e-05 Validation loss = 0.007328, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2383 Training loss: 5.586e-05 Validation loss = 0.006565, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2384 Training loss: 5.634e-05 Validation loss = 0.008721, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2385 Training loss: 5.318e-05 Validation loss = 0.01864, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2386 Training loss: 6.07e-05 Validation loss = 0.006167, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2387 Training loss: 4.907e-05 Validation loss = 0.00635, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2388 Training loss: 5.279e-05 Validation loss = 0.006993, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2389 Training loss: 5.243e-05 Validation loss = 0.006746, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2390 Training loss: 5.972e-05 Validation loss = 0.007757, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2391 Training loss: 5.457e-05 Validation loss = 0.008088, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2392 Training loss: 5.519e-05 Validation loss = 0.007494, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2393 Training loss: 5.296e-05 Validation loss = 0.007467, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2394 Training loss: 5.883e-05 Validation loss = 0.009743, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2395 Training loss: 5.037e-05 Validation loss = 0.006534, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2396 Training loss: 5.166e-05 Validation loss = 0.008091, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0060462595392125305\n",
      "Epoch: 2397 Training loss: 5.306e-05 Validation loss = 0.006046, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2398 Training loss: 5.933e-05 Validation loss = 0.006553, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2399 Training loss: 5.411e-05 Validation loss = 0.006934, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2400 Training loss: 5.371e-05 Validation loss = 0.008571, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2401 Training loss: 5.91e-05 Validation loss = 0.01041, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2402 Training loss: 4.793e-05 Validation loss = 0.007365, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2403 Training loss: 5.349e-05 Validation loss = 0.007212, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2404 Training loss: 5.599e-05 Validation loss = 0.006592, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.006011499972725802\n",
      "Epoch: 2405 Training loss: 5.011e-05 Validation loss = 0.006011, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2406 Training loss: 5.105e-05 Validation loss = 0.006704, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0057966294176512705\n",
      "Epoch: 2407 Training loss: 5.257e-05 Validation loss = 0.005797, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2408 Training loss: 6.416e-05 Validation loss = 0.007137, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2409 Training loss: 4.734e-05 Validation loss = 0.005883, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2410 Training loss: 6.062e-05 Validation loss = 0.008108, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2411 Training loss: 4.456e-05 Validation loss = 0.007316, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2412 Training loss: 6.133e-05 Validation loss = 0.006982, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2413 Training loss: 5.147e-05 Validation loss = 0.00641, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2414 Training loss: 6.027e-05 Validation loss = 0.008067, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2415 Training loss: 4.799e-05 Validation loss = 0.007106, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2416 Training loss: 5.298e-05 Validation loss = 0.007232, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2417 Training loss: 5.217e-05 Validation loss = 0.00776, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2418 Training loss: 5.01e-05 Validation loss = 0.007129, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2419 Training loss: 5.83e-05 Validation loss = 0.007577, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2420 Training loss: 5.11e-05 Validation loss = 0.006446, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2421 Training loss: 4.969e-05 Validation loss = 0.006276, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2422 Training loss: 5.133e-05 Validation loss = 0.006067, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2423 Training loss: 5.167e-05 Validation loss = 0.006144, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2424 Training loss: 6.359e-05 Validation loss = 0.006321, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2425 Training loss: 4.619e-05 Validation loss = 0.006157, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2426 Training loss: 5.429e-05 Validation loss = 0.005881, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2427 Training loss: 5.021e-05 Validation loss = 0.006061, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2428 Training loss: 5.736e-05 Validation loss = 0.007235, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2429 Training loss: 5.06e-05 Validation loss = 0.009134, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2430 Training loss: 4.925e-05 Validation loss = 0.007813, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2431 Training loss: 5.813e-05 Validation loss = 0.00945, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2432 Training loss: 5.272e-05 Validation loss = 0.008902, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2433 Training loss: 4.389e-05 Validation loss = 0.006272, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2434 Training loss: 6.075e-05 Validation loss = 0.006115, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2435 Training loss: 4.914e-05 Validation loss = 0.006334, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2436 Training loss: 5.709e-05 Validation loss = 0.00685, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2437 Training loss: 4.492e-05 Validation loss = 0.008446, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2438 Training loss: 5.46e-05 Validation loss = 0.007324, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2439 Training loss: 5.714e-05 Validation loss = 0.005842, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2440 Training loss: 5.43e-05 Validation loss = 0.006343, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2441 Training loss: 4.557e-05 Validation loss = 0.006312, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2442 Training loss: 5.167e-05 Validation loss = 0.006108, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2443 Training loss: 5.531e-05 Validation loss = 0.007868, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2444 Training loss: 5.559e-05 Validation loss = 0.006138, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2445 Training loss: 4.716e-05 Validation loss = 0.006279, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2446 Training loss: 5.567e-05 Validation loss = 0.009509, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2447 Training loss: 5.193e-05 Validation loss = 0.005859, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2448 Training loss: 5.695e-05 Validation loss = 0.006398, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2449 Training loss: 4.474e-05 Validation loss = 0.005836, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2450 Training loss: 5.284e-05 Validation loss = 0.007089, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2451 Training loss: 4.633e-05 Validation loss = 0.01454, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2452 Training loss: 6.391e-05 Validation loss = 0.006148, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2453 Training loss: 4.398e-05 Validation loss = 0.007221, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2454 Training loss: 5.416e-05 Validation loss = 0.006039, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2455 Training loss: 5.724e-05 Validation loss = 0.00644, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2456 Training loss: 4.819e-05 Validation loss = 0.006145, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2457 Training loss: 4.884e-05 Validation loss = 0.006347, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2458 Training loss: 5.929e-05 Validation loss = 0.006658, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2459 Training loss: 4.695e-05 Validation loss = 0.006187, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2460 Training loss: 6.736e-05 Validation loss = 0.008319, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2461 Training loss: 4.21e-05 Validation loss = 0.006746, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2462 Training loss: 5.3e-05 Validation loss = 0.006739, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2463 Training loss: 4.793e-05 Validation loss = 0.008306, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2464 Training loss: 4.722e-05 Validation loss = 0.008461, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2465 Training loss: 5.585e-05 Validation loss = 0.006339, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2466 Training loss: 5.144e-05 Validation loss = 0.009278, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2467 Training loss: 4.785e-05 Validation loss = 0.006754, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2468 Training loss: 5.391e-05 Validation loss = 0.008904, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2469 Training loss: 4.481e-05 Validation loss = 0.007703, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2470 Training loss: 5.994e-05 Validation loss = 0.006184, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.00576546160949493\n",
      "Epoch: 2471 Training loss: 4.508e-05 Validation loss = 0.005765, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2472 Training loss: 5.246e-05 Validation loss = 0.006816, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2473 Training loss: 6.048e-05 Validation loss = 0.007376, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2474 Training loss: 4.082e-05 Validation loss = 0.006691, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2475 Training loss: 5.307e-05 Validation loss = 0.006729, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2476 Training loss: 4.876e-05 Validation loss = 0.00722, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2477 Training loss: 5.221e-05 Validation loss = 0.007489, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2478 Training loss: 5.1e-05 Validation loss = 0.007123, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2479 Training loss: 5.528e-05 Validation loss = 0.00757, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2480 Training loss: 4.645e-05 Validation loss = 0.006265, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2481 Training loss: 4.887e-05 Validation loss = 0.009527, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2482 Training loss: 5.43e-05 Validation loss = 0.008228, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2483 Training loss: 4.898e-05 Validation loss = 0.006051, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2484 Training loss: 5.81e-05 Validation loss = 0.006514, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2485 Training loss: 4.584e-05 Validation loss = 0.006496, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2486 Training loss: 5.879e-05 Validation loss = 0.008139, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2487 Training loss: 4.619e-05 Validation loss = 0.00604, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2488 Training loss: 5.417e-05 Validation loss = 0.008432, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2489 Training loss: 4.873e-05 Validation loss = 0.008249, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2490 Training loss: 4.388e-05 Validation loss = 0.006671, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2491 Training loss: 5.123e-05 Validation loss = 0.006404, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2492 Training loss: 6.024e-05 Validation loss = 0.006667, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2493 Training loss: 4.601e-05 Validation loss = 0.006156, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0056581917290304765\n",
      "Epoch: 2494 Training loss: 4.647e-05 Validation loss = 0.005658, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2495 Training loss: 4.981e-05 Validation loss = 0.006021, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2496 Training loss: 5.184e-05 Validation loss = 0.008487, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2497 Training loss: 5.613e-05 Validation loss = 0.009536, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2498 Training loss: 5.374e-05 Validation loss = 0.008325, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2499 Training loss: 4.304e-05 Validation loss = 0.00583, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2500 Training loss: 5.35e-05 Validation loss = 0.006949, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2501 Training loss: 4.976e-05 Validation loss = 0.006313, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2502 Training loss: 4.598e-05 Validation loss = 0.006565, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2503 Training loss: 5.435e-05 Validation loss = 0.00675, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2504 Training loss: 4.9e-05 Validation loss = 0.006475, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.00565061033721739\n",
      "Epoch: 2505 Training loss: 5.522e-05 Validation loss = 0.005651, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2506 Training loss: 4.966e-05 Validation loss = 0.006376, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2507 Training loss: 4.874e-05 Validation loss = 0.006911, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2508 Training loss: 4.867e-05 Validation loss = 0.00618, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2509 Training loss: 4.659e-05 Validation loss = 0.006793, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2510 Training loss: 5.37e-05 Validation loss = 0.005752, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2511 Training loss: 5.116e-05 Validation loss = 0.007489, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2512 Training loss: 4.241e-05 Validation loss = 0.008094, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2513 Training loss: 5.937e-05 Validation loss = 0.007852, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2514 Training loss: 4.838e-05 Validation loss = 0.006599, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2515 Training loss: 4.771e-05 Validation loss = 0.006695, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2516 Training loss: 4.616e-05 Validation loss = 0.006939, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2517 Training loss: 5.742e-05 Validation loss = 0.006251, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2518 Training loss: 4.381e-05 Validation loss = 0.00803, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2519 Training loss: 5.377e-05 Validation loss = 0.006443, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2520 Training loss: 5.234e-05 Validation loss = 0.006313, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2521 Training loss: 4.823e-05 Validation loss = 0.006015, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2522 Training loss: 4.658e-05 Validation loss = 0.005953, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2523 Training loss: 5.028e-05 Validation loss = 0.007817, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2524 Training loss: 5.604e-05 Validation loss = 0.007841, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2525 Training loss: 4.41e-05 Validation loss = 0.005739, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2526 Training loss: 5.32e-05 Validation loss = 0.006281, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2527 Training loss: 4.675e-05 Validation loss = 0.007029, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2528 Training loss: 4.896e-05 Validation loss = 0.01118, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2529 Training loss: 5.198e-05 Validation loss = 0.006188, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.005615054609056676\n",
      "Epoch: 2530 Training loss: 4.964e-05 Validation loss = 0.005615, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2531 Training loss: 4.51e-05 Validation loss = 0.01036, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2532 Training loss: 5.324e-05 Validation loss = 0.007497, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2533 Training loss: 4.815e-05 Validation loss = 0.007921, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2534 Training loss: 5.919e-05 Validation loss = 0.006478, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2535 Training loss: 4.384e-05 Validation loss = 0.007015, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2536 Training loss: 5.061e-05 Validation loss = 0.006698, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2537 Training loss: 4.804e-05 Validation loss = 0.0117, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2538 Training loss: 4.701e-05 Validation loss = 0.007837, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2539 Training loss: 4.981e-05 Validation loss = 0.006586, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2540 Training loss: 5.343e-05 Validation loss = 0.005699, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2541 Training loss: 4.528e-05 Validation loss = 0.006522, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2542 Training loss: 5.813e-05 Validation loss = 0.008161, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2543 Training loss: 4.284e-05 Validation loss = 0.0133, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2544 Training loss: 5.217e-05 Validation loss = 0.009401, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2545 Training loss: 4.62e-05 Validation loss = 0.006996, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.005604587000832252\n",
      "Epoch: 2546 Training loss: 5.013e-05 Validation loss = 0.005605, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2547 Training loss: 5.18e-05 Validation loss = 0.00591, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2548 Training loss: 5.47e-05 Validation loss = 0.00571, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2549 Training loss: 3.694e-05 Validation loss = 0.006163, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2550 Training loss: 5.435e-05 Validation loss = 0.007297, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2551 Training loss: 5.019e-05 Validation loss = 0.006771, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2552 Training loss: 5.234e-05 Validation loss = 0.009171, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2553 Training loss: 4.408e-05 Validation loss = 0.006483, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2554 Training loss: 4.817e-05 Validation loss = 0.007775, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2555 Training loss: 4.609e-05 Validation loss = 0.005613, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2556 Training loss: 5.837e-05 Validation loss = 0.007689, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2557 Training loss: 5.136e-05 Validation loss = 0.006766, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2558 Training loss: 4.338e-05 Validation loss = 0.007499, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2559 Training loss: 5.052e-05 Validation loss = 0.006424, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2560 Training loss: 4.344e-05 Validation loss = 0.01004, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2561 Training loss: 5.008e-05 Validation loss = 0.01038, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2562 Training loss: 5.312e-05 Validation loss = 0.006809, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2563 Training loss: 5.035e-05 Validation loss = 0.006077, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2564 Training loss: 4.435e-05 Validation loss = 0.005641, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2565 Training loss: 5.319e-05 Validation loss = 0.008492, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2566 Training loss: 4.719e-05 Validation loss = 0.006558, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2567 Training loss: 5.237e-05 Validation loss = 0.005646, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2568 Training loss: 4.745e-05 Validation loss = 0.006798, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2569 Training loss: 4.421e-05 Validation loss = 0.008656, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2570 Training loss: 4.535e-05 Validation loss = 0.005983, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2571 Training loss: 5.403e-05 Validation loss = 0.006663, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2572 Training loss: 5.215e-05 Validation loss = 0.005712, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2573 Training loss: 4.111e-05 Validation loss = 0.006166, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2574 Training loss: 5.622e-05 Validation loss = 0.008646, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2575 Training loss: 4.006e-05 Validation loss = 0.005756, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2576 Training loss: 5.051e-05 Validation loss = 0.006597, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2577 Training loss: 5.318e-05 Validation loss = 0.008067, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2578 Training loss: 4.446e-05 Validation loss = 0.006013, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2579 Training loss: 5.153e-05 Validation loss = 0.009156, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0055742407397070155\n",
      "Epoch: 2580 Training loss: 4.833e-05 Validation loss = 0.005574, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2581 Training loss: 4.685e-05 Validation loss = 0.005914, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.00544989159294956\n",
      "Epoch: 2582 Training loss: 4.701e-05 Validation loss = 0.00545, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2583 Training loss: 4.846e-05 Validation loss = 0.006068, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2584 Training loss: 5.005e-05 Validation loss = 0.005976, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2585 Training loss: 4.744e-05 Validation loss = 0.006098, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2586 Training loss: 5.099e-05 Validation loss = 0.00732, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2587 Training loss: 4.718e-05 Validation loss = 0.007693, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2588 Training loss: 5.048e-05 Validation loss = 0.006764, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2589 Training loss: 5.132e-05 Validation loss = 0.006672, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2590 Training loss: 4.075e-05 Validation loss = 0.006249, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2591 Training loss: 5.028e-05 Validation loss = 0.007045, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2592 Training loss: 4.953e-05 Validation loss = 0.006104, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2593 Training loss: 4.575e-05 Validation loss = 0.007757, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2594 Training loss: 5.136e-05 Validation loss = 0.008351, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2595 Training loss: 4.492e-05 Validation loss = 0.006447, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.005405689577189796\n",
      "Epoch: 2596 Training loss: 5.093e-05 Validation loss = 0.005406, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2597 Training loss: 6.126e-05 Validation loss = 0.006911, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2598 Training loss: 4.046e-05 Validation loss = 0.005693, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2599 Training loss: 4.381e-05 Validation loss = 0.007838, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2600 Training loss: 4.823e-05 Validation loss = 0.008497, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2601 Training loss: 4.589e-05 Validation loss = 0.007155, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2602 Training loss: 5.301e-05 Validation loss = 0.006234, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2603 Training loss: 4.384e-05 Validation loss = 0.009699, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2604 Training loss: 4.524e-05 Validation loss = 0.006516, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2605 Training loss: 5.235e-05 Validation loss = 0.006588, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2606 Training loss: 4.813e-05 Validation loss = 0.006088, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2607 Training loss: 4.052e-05 Validation loss = 0.005623, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2608 Training loss: 5.636e-05 Validation loss = 0.007955, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2609 Training loss: 4.32e-05 Validation loss = 0.00683, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2610 Training loss: 5.482e-05 Validation loss = 0.008234, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2611 Training loss: 4.117e-05 Validation loss = 0.007509, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2612 Training loss: 4.813e-05 Validation loss = 0.005667, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2613 Training loss: 4.407e-05 Validation loss = 0.008942, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2614 Training loss: 5.329e-05 Validation loss = 0.005585, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2615 Training loss: 4.531e-05 Validation loss = 0.006056, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2616 Training loss: 4.42e-05 Validation loss = 0.006247, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2617 Training loss: 5.771e-05 Validation loss = 0.00723, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2618 Training loss: 4.296e-05 Validation loss = 0.005719, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2619 Training loss: 5.13e-05 Validation loss = 0.006851, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2620 Training loss: 4.687e-05 Validation loss = 0.00813, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2621 Training loss: 4.581e-05 Validation loss = 0.006367, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2622 Training loss: 4.842e-05 Validation loss = 0.005719, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2623 Training loss: 4.141e-05 Validation loss = 0.006363, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2624 Training loss: 5.115e-05 Validation loss = 0.008384, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2625 Training loss: 4.67e-05 Validation loss = 0.00692, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2626 Training loss: 4.754e-05 Validation loss = 0.00885, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2627 Training loss: 4.814e-05 Validation loss = 0.005429, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2628 Training loss: 4.904e-05 Validation loss = 0.005776, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2629 Training loss: 4.117e-05 Validation loss = 0.006266, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2630 Training loss: 4.789e-05 Validation loss = 0.009259, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2631 Training loss: 4.655e-05 Validation loss = 0.009938, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2632 Training loss: 5.467e-05 Validation loss = 0.007628, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2633 Training loss: 4.735e-05 Validation loss = 0.005853, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2634 Training loss: 4.212e-05 Validation loss = 0.005809, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2635 Training loss: 5.357e-05 Validation loss = 0.008155, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2636 Training loss: 4.255e-05 Validation loss = 0.006711, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2637 Training loss: 5.205e-05 Validation loss = 0.009398, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2638 Training loss: 4.086e-05 Validation loss = 0.006946, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2639 Training loss: 5.199e-05 Validation loss = 0.007769, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2640 Training loss: 4.734e-05 Validation loss = 0.005852, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2641 Training loss: 4.791e-05 Validation loss = 0.005941, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2642 Training loss: 5.789e-05 Validation loss = 0.007843, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2643 Training loss: 3.811e-05 Validation loss = 0.005561, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2644 Training loss: 3.988e-05 Validation loss = 0.005767, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2645 Training loss: 5.577e-05 Validation loss = 0.005605, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2646 Training loss: 4.064e-05 Validation loss = 0.006384, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2647 Training loss: 4.797e-05 Validation loss = 0.008984, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2648 Training loss: 4.664e-05 Validation loss = 0.005537, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2649 Training loss: 4.339e-05 Validation loss = 0.009588, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.00539094042190367\n",
      "Epoch: 2650 Training loss: 5.277e-05 Validation loss = 0.005391, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2651 Training loss: 4.65e-05 Validation loss = 0.007795, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2652 Training loss: 4.291e-05 Validation loss = 0.005833, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2653 Training loss: 4.866e-05 Validation loss = 0.009212, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2654 Training loss: 4.575e-05 Validation loss = 0.009298, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2655 Training loss: 4.902e-05 Validation loss = 0.006139, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2656 Training loss: 5.36e-05 Validation loss = 0.008374, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2657 Training loss: 3.97e-05 Validation loss = 0.006744, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2658 Training loss: 4.653e-05 Validation loss = 0.00573, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2659 Training loss: 4.894e-05 Validation loss = 0.006264, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2660 Training loss: 5.012e-05 Validation loss = 0.005409, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2661 Training loss: 4.276e-05 Validation loss = 0.006136, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2662 Training loss: 4.934e-05 Validation loss = 0.005824, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2663 Training loss: 4.5e-05 Validation loss = 0.006032, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2664 Training loss: 4.527e-05 Validation loss = 0.006171, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2665 Training loss: 4.91e-05 Validation loss = 0.006271, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2666 Training loss: 5.141e-05 Validation loss = 0.006947, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2667 Training loss: 3.709e-05 Validation loss = 0.008143, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2668 Training loss: 5.172e-05 Validation loss = 0.007375, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2669 Training loss: 4.476e-05 Validation loss = 0.007233, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2670 Training loss: 5.237e-05 Validation loss = 0.008652, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2671 Training loss: 4.926e-05 Validation loss = 0.006904, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2672 Training loss: 4.039e-05 Validation loss = 0.007108, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2673 Training loss: 5.008e-05 Validation loss = 0.006104, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2674 Training loss: 3.845e-05 Validation loss = 0.006992, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2675 Training loss: 4.52e-05 Validation loss = 0.00671, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2676 Training loss: 5.827e-05 Validation loss = 0.006349, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2677 Training loss: 3.909e-05 Validation loss = 0.008204, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2678 Training loss: 4.651e-05 Validation loss = 0.006556, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.005386365443215761\n",
      "Epoch: 2679 Training loss: 4.434e-05 Validation loss = 0.005386, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2680 Training loss: 5.046e-05 Validation loss = 0.007449, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2681 Training loss: 4.927e-05 Validation loss = 0.005642, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2682 Training loss: 3.773e-05 Validation loss = 0.009587, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2683 Training loss: 5.492e-05 Validation loss = 0.007805, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2684 Training loss: 4.139e-05 Validation loss = 0.007285, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2685 Training loss: 4.685e-05 Validation loss = 0.007612, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2686 Training loss: 4.712e-05 Validation loss = 0.006105, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2687 Training loss: 5.127e-05 Validation loss = 0.006094, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2688 Training loss: 3.909e-05 Validation loss = 0.006603, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2689 Training loss: 4.925e-05 Validation loss = 0.006043, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2690 Training loss: 4.659e-05 Validation loss = 0.005752, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2691 Training loss: 4.602e-05 Validation loss = 0.006517, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2692 Training loss: 4.32e-05 Validation loss = 0.005883, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2693 Training loss: 5.443e-05 Validation loss = 0.005919, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2694 Training loss: 4.048e-05 Validation loss = 0.005511, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2695 Training loss: 4.412e-05 Validation loss = 0.00635, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2696 Training loss: 4.692e-05 Validation loss = 0.006377, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2697 Training loss: 4.681e-05 Validation loss = 0.006209, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2698 Training loss: 4.542e-05 Validation loss = 0.005719, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2699 Training loss: 4.831e-05 Validation loss = 0.006179, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2700 Training loss: 4.371e-05 Validation loss = 0.005775, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2701 Training loss: 4.372e-05 Validation loss = 0.006043, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2702 Training loss: 4.592e-05 Validation loss = 0.01129, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2703 Training loss: 4.975e-05 Validation loss = 0.00614, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2704 Training loss: 4.377e-05 Validation loss = 0.006856, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2705 Training loss: 4.702e-05 Validation loss = 0.006525, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2706 Training loss: 4.32e-05 Validation loss = 0.005713, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2707 Training loss: 4.574e-05 Validation loss = 0.006937, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2708 Training loss: 4.473e-05 Validation loss = 0.005654, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2709 Training loss: 6.098e-05 Validation loss = 0.009179, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2710 Training loss: 3.505e-05 Validation loss = 0.006175, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2711 Training loss: 4.258e-05 Validation loss = 0.006431, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2712 Training loss: 4.675e-05 Validation loss = 0.007822, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2713 Training loss: 4.334e-05 Validation loss = 0.00602, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2714 Training loss: 5.571e-05 Validation loss = 0.007374, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2715 Training loss: 4.154e-05 Validation loss = 0.005398, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2716 Training loss: 4.43e-05 Validation loss = 0.007561, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2717 Training loss: 4.43e-05 Validation loss = 0.007838, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2718 Training loss: 4.319e-05 Validation loss = 0.01036, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2719 Training loss: 4.816e-05 Validation loss = 0.006474, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2720 Training loss: 4.421e-05 Validation loss = 0.006074, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2721 Training loss: 4.446e-05 Validation loss = 0.009093, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2722 Training loss: 5.233e-05 Validation loss = 0.00608, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2723 Training loss: 4.96e-05 Validation loss = 0.01178, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2724 Training loss: 3.883e-05 Validation loss = 0.005736, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2725 Training loss: 4.81e-05 Validation loss = 0.005542, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2726 Training loss: 3.904e-05 Validation loss = 0.005532, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2727 Training loss: 5.258e-05 Validation loss = 0.006409, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2728 Training loss: 4.15e-05 Validation loss = 0.009309, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2729 Training loss: 4.345e-05 Validation loss = 0.006825, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2730 Training loss: 5.84e-05 Validation loss = 0.007686, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2731 Training loss: 3.536e-05 Validation loss = 0.006621, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2732 Training loss: 4.533e-05 Validation loss = 0.009552, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2733 Training loss: 5.436e-05 Validation loss = 0.006173, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2734 Training loss: 3.931e-05 Validation loss = 0.006463, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2735 Training loss: 4.345e-05 Validation loss = 0.00639, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2736 Training loss: 3.97e-05 Validation loss = 0.008313, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2737 Training loss: 4.688e-05 Validation loss = 0.006259, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2738 Training loss: 4.975e-05 Validation loss = 0.00639, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2739 Training loss: 4.46e-05 Validation loss = 0.007685, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2740 Training loss: 4.775e-05 Validation loss = 0.0057, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2741 Training loss: 4.091e-05 Validation loss = 0.006423, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2742 Training loss: 4.318e-05 Validation loss = 0.005678, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2743 Training loss: 4.504e-05 Validation loss = 0.005786, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.005377013825433149\n",
      "Epoch: 2744 Training loss: 5.42e-05 Validation loss = 0.005377, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2745 Training loss: 3.674e-05 Validation loss = 0.005408, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2746 Training loss: 5.598e-05 Validation loss = 0.006831, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2747 Training loss: 3.669e-05 Validation loss = 0.006896, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2748 Training loss: 4.519e-05 Validation loss = 0.005944, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2749 Training loss: 4.894e-05 Validation loss = 0.006627, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2750 Training loss: 4.125e-05 Validation loss = 0.008312, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2751 Training loss: 4.636e-05 Validation loss = 0.006327, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2752 Training loss: 4.942e-05 Validation loss = 0.006439, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2753 Training loss: 4.756e-05 Validation loss = 0.006595, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2754 Training loss: 4.154e-05 Validation loss = 0.008793, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2755 Training loss: 4.255e-05 Validation loss = 0.008871, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2756 Training loss: 4.691e-05 Validation loss = 0.00693, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2757 Training loss: 4.064e-05 Validation loss = 0.006276, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2758 Training loss: 4.875e-05 Validation loss = 0.006073, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2759 Training loss: 4.054e-05 Validation loss = 0.00625, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2760 Training loss: 4.408e-05 Validation loss = 0.005467, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.005327647281686479\n",
      "Epoch: 2761 Training loss: 4.86e-05 Validation loss = 0.005328, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2762 Training loss: 4.53e-05 Validation loss = 0.009287, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2763 Training loss: 4.653e-05 Validation loss = 0.005659, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2764 Training loss: 4.727e-05 Validation loss = 0.005755, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2765 Training loss: 3.674e-05 Validation loss = 0.005791, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2766 Training loss: 4.825e-05 Validation loss = 0.007715, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2767 Training loss: 4.504e-05 Validation loss = 0.005408, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2768 Training loss: 4.33e-05 Validation loss = 0.006312, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2769 Training loss: 4.678e-05 Validation loss = 0.005355, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2770 Training loss: 4.09e-05 Validation loss = 0.00549, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.005303256946071896\n",
      "Epoch: 2771 Training loss: 4.884e-05 Validation loss = 0.005303, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2772 Training loss: 4.078e-05 Validation loss = 0.00587, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2773 Training loss: 5.077e-05 Validation loss = 0.006019, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2774 Training loss: 3.876e-05 Validation loss = 0.009537, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2775 Training loss: 4.14e-05 Validation loss = 0.005792, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2776 Training loss: 5.687e-05 Validation loss = 0.005936, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2777 Training loss: 3.842e-05 Validation loss = 0.00855, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2778 Training loss: 4.216e-05 Validation loss = 0.005452, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2779 Training loss: 4.949e-05 Validation loss = 0.00788, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2780 Training loss: 4.322e-05 Validation loss = 0.00597, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2781 Training loss: 4.551e-05 Validation loss = 0.01156, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2782 Training loss: 4.28e-05 Validation loss = 0.009018, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2783 Training loss: 4.347e-05 Validation loss = 0.007343, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2784 Training loss: 4.351e-05 Validation loss = 0.005589, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2785 Training loss: 5.136e-05 Validation loss = 0.005869, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.005286528734180142\n",
      "Epoch: 2786 Training loss: 3.9e-05 Validation loss = 0.005287, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2787 Training loss: 4.491e-05 Validation loss = 0.005861, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2788 Training loss: 4.051e-05 Validation loss = 0.006093, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2789 Training loss: 5.201e-05 Validation loss = 0.005883, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2790 Training loss: 3.84e-05 Validation loss = 0.006208, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2791 Training loss: 5.294e-05 Validation loss = 0.005893, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2792 Training loss: 3.557e-05 Validation loss = 0.006717, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2793 Training loss: 4.972e-05 Validation loss = 0.006932, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2794 Training loss: 4.245e-05 Validation loss = 0.007982, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2795 Training loss: 4.669e-05 Validation loss = 0.006302, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.005132475763182282\n",
      "Epoch: 2796 Training loss: 3.777e-05 Validation loss = 0.005132, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2797 Training loss: 4.621e-05 Validation loss = 0.006719, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2798 Training loss: 4.61e-05 Validation loss = 0.007345, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2799 Training loss: 4.441e-05 Validation loss = 0.006959, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2800 Training loss: 4.442e-05 Validation loss = 0.006285, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2801 Training loss: 3.973e-05 Validation loss = 0.00522, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2802 Training loss: 5.076e-05 Validation loss = 0.008431, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2803 Training loss: 3.933e-05 Validation loss = 0.005962, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2804 Training loss: 4.744e-05 Validation loss = 0.005855, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2805 Training loss: 4.466e-05 Validation loss = 0.005942, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2806 Training loss: 4.489e-05 Validation loss = 0.005555, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2807 Training loss: 3.999e-05 Validation loss = 0.006414, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2808 Training loss: 4.871e-05 Validation loss = 0.005792, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2809 Training loss: 4.056e-05 Validation loss = 0.00599, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2810 Training loss: 4.554e-05 Validation loss = 0.005841, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2811 Training loss: 4.731e-05 Validation loss = 0.01046, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2812 Training loss: 3.426e-05 Validation loss = 0.005563, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2813 Training loss: 5.125e-05 Validation loss = 0.006764, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2814 Training loss: 3.929e-05 Validation loss = 0.006936, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2815 Training loss: 4.311e-05 Validation loss = 0.006799, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2816 Training loss: 4.676e-05 Validation loss = 0.005433, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2817 Training loss: 4.984e-05 Validation loss = 0.006828, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2818 Training loss: 3.912e-05 Validation loss = 0.006584, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2819 Training loss: 4.739e-05 Validation loss = 0.006823, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2820 Training loss: 3.772e-05 Validation loss = 0.007914, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2821 Training loss: 4.651e-05 Validation loss = 0.005248, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2822 Training loss: 4.34e-05 Validation loss = 0.007427, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2823 Training loss: 4.725e-05 Validation loss = 0.007869, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2824 Training loss: 4.024e-05 Validation loss = 0.00578, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2825 Training loss: 4.369e-05 Validation loss = 0.006302, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2826 Training loss: 4.712e-05 Validation loss = 0.006476, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2827 Training loss: 4.248e-05 Validation loss = 0.006744, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2828 Training loss: 3.73e-05 Validation loss = 0.006561, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2829 Training loss: 4.533e-05 Validation loss = 0.007997, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2830 Training loss: 4.672e-05 Validation loss = 0.00628, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2831 Training loss: 5.44e-05 Validation loss = 0.01019, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2832 Training loss: 3.872e-05 Validation loss = 0.006666, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2833 Training loss: 4.177e-05 Validation loss = 0.01103, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2834 Training loss: 4.499e-05 Validation loss = 0.0102, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2835 Training loss: 4.215e-05 Validation loss = 0.005947, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.005094515594136151\n",
      "Epoch: 2836 Training loss: 4.323e-05 Validation loss = 0.005095, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2837 Training loss: 4.489e-05 Validation loss = 0.005276, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2838 Training loss: 3.646e-05 Validation loss = 0.006941, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2839 Training loss: 4.907e-05 Validation loss = 0.005451, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2840 Training loss: 4.543e-05 Validation loss = 0.008999, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2841 Training loss: 4.317e-05 Validation loss = 0.005541, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2842 Training loss: 3.816e-05 Validation loss = 0.007394, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2843 Training loss: 4.548e-05 Validation loss = 0.005161, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2844 Training loss: 5.466e-05 Validation loss = 0.007923, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2845 Training loss: 3.546e-05 Validation loss = 0.005849, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2846 Training loss: 4.193e-05 Validation loss = 0.005584, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2847 Training loss: 4.336e-05 Validation loss = 0.007213, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2848 Training loss: 3.942e-05 Validation loss = 0.006111, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2849 Training loss: 4.99e-05 Validation loss = 0.008026, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2850 Training loss: 4.34e-05 Validation loss = 0.006741, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2851 Training loss: 4.039e-05 Validation loss = 0.005555, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2852 Training loss: 3.846e-05 Validation loss = 0.006367, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2853 Training loss: 4.84e-05 Validation loss = 0.006494, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2854 Training loss: 4.922e-05 Validation loss = 0.006215, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2855 Training loss: 3.781e-05 Validation loss = 0.005937, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2856 Training loss: 3.866e-05 Validation loss = 0.007245, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2857 Training loss: 4.838e-05 Validation loss = 0.006196, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2858 Training loss: 4.168e-05 Validation loss = 0.005402, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2859 Training loss: 4.14e-05 Validation loss = 0.005776, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2860 Training loss: 5.036e-05 Validation loss = 0.005934, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2861 Training loss: 3.351e-05 Validation loss = 0.0064, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2862 Training loss: 4.82e-05 Validation loss = 0.005325, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2863 Training loss: 4.308e-05 Validation loss = 0.005232, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2864 Training loss: 4.67e-05 Validation loss = 0.00975, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2865 Training loss: 4.022e-05 Validation loss = 0.008639, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2866 Training loss: 4.139e-05 Validation loss = 0.005553, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2867 Training loss: 3.855e-05 Validation loss = 0.005426, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2868 Training loss: 4.523e-05 Validation loss = 0.005487, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2869 Training loss: 4.658e-05 Validation loss = 0.005304, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2870 Training loss: 3.838e-05 Validation loss = 0.005152, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2871 Training loss: 4.959e-05 Validation loss = 0.006976, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2872 Training loss: 3.635e-05 Validation loss = 0.00701, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2873 Training loss: 4.714e-05 Validation loss = 0.006006, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2874 Training loss: 4.038e-05 Validation loss = 0.005994, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2875 Training loss: 4.066e-05 Validation loss = 0.009145, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2876 Training loss: 4.658e-05 Validation loss = 0.00659, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2877 Training loss: 4.692e-05 Validation loss = 0.006196, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2878 Training loss: 3.34e-05 Validation loss = 0.005531, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2879 Training loss: 5.367e-05 Validation loss = 0.00576, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2880 Training loss: 4.331e-05 Validation loss = 0.006275, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2881 Training loss: 3.71e-05 Validation loss = 0.005481, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2882 Training loss: 3.881e-05 Validation loss = 0.005248, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2883 Training loss: 4.922e-05 Validation loss = 0.00683, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2884 Training loss: 4.218e-05 Validation loss = 0.005993, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2885 Training loss: 4.038e-05 Validation loss = 0.006089, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2886 Training loss: 4.331e-05 Validation loss = 0.005575, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2887 Training loss: 4.177e-05 Validation loss = 0.006362, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2888 Training loss: 4.03e-05 Validation loss = 0.005713, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2889 Training loss: 4.329e-05 Validation loss = 0.007595, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2890 Training loss: 4.087e-05 Validation loss = 0.005697, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2891 Training loss: 4.672e-05 Validation loss = 0.006542, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2892 Training loss: 3.976e-05 Validation loss = 0.005887, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2893 Training loss: 4.486e-05 Validation loss = 0.008573, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2894 Training loss: 4.371e-05 Validation loss = 0.006101, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2895 Training loss: 4.205e-05 Validation loss = 0.00594, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2896 Training loss: 3.773e-05 Validation loss = 0.005389, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2897 Training loss: 4.639e-05 Validation loss = 0.007651, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2898 Training loss: 4.337e-05 Validation loss = 0.006808, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2899 Training loss: 4.393e-05 Validation loss = 0.006165, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2900 Training loss: 3.874e-05 Validation loss = 0.01145, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2901 Training loss: 4.135e-05 Validation loss = 0.005718, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2902 Training loss: 4.632e-05 Validation loss = 0.005728, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2903 Training loss: 3.865e-05 Validation loss = 0.005551, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2904 Training loss: 4.165e-05 Validation loss = 0.007506, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2905 Training loss: 3.787e-05 Validation loss = 0.005423, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2906 Training loss: 4.473e-05 Validation loss = 0.005251, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2907 Training loss: 4.007e-05 Validation loss = 0.006085, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2908 Training loss: 5.089e-05 Validation loss = 0.005181, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2909 Training loss: 3.567e-05 Validation loss = 0.006455, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2910 Training loss: 4.717e-05 Validation loss = 0.008107, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2911 Training loss: 4.774e-05 Validation loss = 0.007016, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2912 Training loss: 3.51e-05 Validation loss = 0.006794, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2913 Training loss: 4.203e-05 Validation loss = 0.005437, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2914 Training loss: 4.33e-05 Validation loss = 0.005746, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2915 Training loss: 3.987e-05 Validation loss = 0.00576, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0050387874285702005\n",
      "Epoch: 2916 Training loss: 5.674e-05 Validation loss = 0.005039, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2917 Training loss: 3.404e-05 Validation loss = 0.005723, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2918 Training loss: 4.098e-05 Validation loss = 0.005535, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2919 Training loss: 4.038e-05 Validation loss = 0.006138, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2920 Training loss: 3.931e-05 Validation loss = 0.008464, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2921 Training loss: 4.013e-05 Validation loss = 0.005103, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2922 Training loss: 4.437e-05 Validation loss = 0.005142, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2923 Training loss: 4.286e-05 Validation loss = 0.008004, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2924 Training loss: 4.18e-05 Validation loss = 0.00575, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2925 Training loss: 4.014e-05 Validation loss = 0.007147, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2926 Training loss: 4.537e-05 Validation loss = 0.005852, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2927 Training loss: 3.98e-05 Validation loss = 0.007779, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2928 Training loss: 3.917e-05 Validation loss = 0.008575, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2929 Training loss: 4.495e-05 Validation loss = 0.007708, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2930 Training loss: 4.821e-05 Validation loss = 0.006955, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2931 Training loss: 3.586e-05 Validation loss = 0.00631, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2932 Training loss: 4.08e-05 Validation loss = 0.005501, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2933 Training loss: 4.28e-05 Validation loss = 0.008544, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2934 Training loss: 4.052e-05 Validation loss = 0.006447, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2935 Training loss: 4.736e-05 Validation loss = 0.005136, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2936 Training loss: 3.814e-05 Validation loss = 0.006052, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2937 Training loss: 4.03e-05 Validation loss = 0.005244, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2938 Training loss: 4.298e-05 Validation loss = 0.006248, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2939 Training loss: 4.137e-05 Validation loss = 0.006012, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2940 Training loss: 4.222e-05 Validation loss = 0.007241, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2941 Training loss: 4.413e-05 Validation loss = 0.005408, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2942 Training loss: 3.661e-05 Validation loss = 0.008087, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2943 Training loss: 4.248e-05 Validation loss = 0.006647, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2944 Training loss: 4.109e-05 Validation loss = 0.006786, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2945 Training loss: 4.272e-05 Validation loss = 0.008675, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2946 Training loss: 4.203e-05 Validation loss = 0.00608, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2947 Training loss: 4.403e-05 Validation loss = 0.005879, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2948 Training loss: 4.157e-05 Validation loss = 0.005596, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2949 Training loss: 3.414e-05 Validation loss = 0.00668, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2950 Training loss: 5.924e-05 Validation loss = 0.007388, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2951 Training loss: 3.427e-05 Validation loss = 0.005771, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2952 Training loss: 3.618e-05 Validation loss = 0.005228, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2953 Training loss: 5.09e-05 Validation loss = 0.005438, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2954 Training loss: 3.378e-05 Validation loss = 0.005555, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2955 Training loss: 5.429e-05 Validation loss = 0.005479, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2956 Training loss: 2.905e-05 Validation loss = 0.00568, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2957 Training loss: 3.863e-05 Validation loss = 0.006883, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2958 Training loss: 4.353e-05 Validation loss = 0.005479, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2959 Training loss: 4.443e-05 Validation loss = 0.006899, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2960 Training loss: 3.696e-05 Validation loss = 0.006228, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2961 Training loss: 3.745e-05 Validation loss = 0.005612, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2962 Training loss: 5.122e-05 Validation loss = 0.005801, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2963 Training loss: 3.486e-05 Validation loss = 0.007345, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2964 Training loss: 4.055e-05 Validation loss = 0.00797, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2965 Training loss: 4.214e-05 Validation loss = 0.006375, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2966 Training loss: 4.627e-05 Validation loss = 0.005138, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2967 Training loss: 3.528e-05 Validation loss = 0.005414, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2968 Training loss: 4.651e-05 Validation loss = 0.005551, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2969 Training loss: 3.657e-05 Validation loss = 0.006663, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2970 Training loss: 4.311e-05 Validation loss = 0.005712, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2971 Training loss: 4.356e-05 Validation loss = 0.006832, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2972 Training loss: 3.794e-05 Validation loss = 0.005338, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2973 Training loss: 4.219e-05 Validation loss = 0.009827, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2974 Training loss: 4.571e-05 Validation loss = 0.00682, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.005021025041524721\n",
      "Epoch: 2975 Training loss: 4.13e-05 Validation loss = 0.005021, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2976 Training loss: 4.258e-05 Validation loss = 0.005723, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2977 Training loss: 3.63e-05 Validation loss = 0.005808, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2978 Training loss: 3.936e-05 Validation loss = 0.005858, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2979 Training loss: 4.256e-05 Validation loss = 0.0072, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2980 Training loss: 3.65e-05 Validation loss = 0.005427, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2981 Training loss: 4.441e-05 Validation loss = 0.008835, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2982 Training loss: 4.696e-05 Validation loss = 0.005826, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2983 Training loss: 3.761e-05 Validation loss = 0.005742, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2984 Training loss: 3.977e-05 Validation loss = 0.006789, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2985 Training loss: 4.062e-05 Validation loss = 0.00591, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2986 Training loss: 4.596e-05 Validation loss = 0.005248, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2987 Training loss: 3.511e-05 Validation loss = 0.006151, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2988 Training loss: 4.073e-05 Validation loss = 0.005566, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2989 Training loss: 5.02e-05 Validation loss = 0.006596, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2990 Training loss: 3.603e-05 Validation loss = 0.00547, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2991 Training loss: 3.869e-05 Validation loss = 0.006092, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2992 Training loss: 3.875e-05 Validation loss = 0.006895, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2993 Training loss: 4.275e-05 Validation loss = 0.005533, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2994 Training loss: 3.883e-05 Validation loss = 0.005354, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2995 Training loss: 3.588e-05 Validation loss = 0.005726, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2996 Training loss: 4.967e-05 Validation loss = 0.006596, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2997 Training loss: 3.776e-05 Validation loss = 0.005273, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2998 Training loss: 3.631e-05 Validation loss = 0.006356, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 2999 Training loss: 4.172e-05 Validation loss = 0.005566, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.00483614286892005\n",
      "Epoch: 3000 Training loss: 4.768e-05 Validation loss = 0.004836, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3001 Training loss: 3.665e-05 Validation loss = 0.006487, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3002 Training loss: 3.94e-05 Validation loss = 0.005731, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3003 Training loss: 4.097e-05 Validation loss = 0.005495, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3004 Training loss: 4.618e-05 Validation loss = 0.005681, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3005 Training loss: 3.815e-05 Validation loss = 0.005135, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3006 Training loss: 3.747e-05 Validation loss = 0.008261, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3007 Training loss: 3.86e-05 Validation loss = 0.006568, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3008 Training loss: 4.392e-05 Validation loss = 0.005633, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3009 Training loss: 4.191e-05 Validation loss = 0.007263, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3010 Training loss: 4.083e-05 Validation loss = 0.00664, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3011 Training loss: 3.517e-05 Validation loss = 0.009821, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3012 Training loss: 4.364e-05 Validation loss = 0.005955, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3013 Training loss: 5.142e-05 Validation loss = 0.005532, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3014 Training loss: 3.083e-05 Validation loss = 0.005323, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3015 Training loss: 5.531e-05 Validation loss = 0.006567, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3016 Training loss: 2.995e-05 Validation loss = 0.00493, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3017 Training loss: 3.921e-05 Validation loss = 0.005385, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3018 Training loss: 3.55e-05 Validation loss = 0.005014, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3019 Training loss: 4.237e-05 Validation loss = 0.006512, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3020 Training loss: 4.271e-05 Validation loss = 0.00599, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3021 Training loss: 3.435e-05 Validation loss = 0.006426, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3022 Training loss: 4.024e-05 Validation loss = 0.006762, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3023 Training loss: 4.608e-05 Validation loss = 0.005273, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3024 Training loss: 4.666e-05 Validation loss = 0.008229, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3025 Training loss: 3.24e-05 Validation loss = 0.006586, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3026 Training loss: 4.113e-05 Validation loss = 0.005217, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3027 Training loss: 3.7e-05 Validation loss = 0.006072, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3028 Training loss: 4.337e-05 Validation loss = 0.005093, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3029 Training loss: 3.968e-05 Validation loss = 0.005118, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3030 Training loss: 4.179e-05 Validation loss = 0.005995, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3031 Training loss: 3.541e-05 Validation loss = 0.005319, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3032 Training loss: 4.538e-05 Validation loss = 0.005452, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3033 Training loss: 3.637e-05 Validation loss = 0.006703, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3034 Training loss: 4.549e-05 Validation loss = 0.005405, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3035 Training loss: 3.417e-05 Validation loss = 0.006083, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3036 Training loss: 4.524e-05 Validation loss = 0.006211, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3037 Training loss: 3.775e-05 Validation loss = 0.005559, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3038 Training loss: 4.485e-05 Validation loss = 0.005894, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3039 Training loss: 3.572e-05 Validation loss = 0.006294, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3040 Training loss: 3.645e-05 Validation loss = 0.005435, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3041 Training loss: 4.329e-05 Validation loss = 0.006031, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3042 Training loss: 3.946e-05 Validation loss = 0.005706, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3043 Training loss: 4.307e-05 Validation loss = 0.007345, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3044 Training loss: 3.597e-05 Validation loss = 0.005079, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3045 Training loss: 4.254e-05 Validation loss = 0.007235, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3046 Training loss: 3.856e-05 Validation loss = 0.009188, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3047 Training loss: 4.018e-05 Validation loss = 0.006373, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3048 Training loss: 4.168e-05 Validation loss = 0.0068, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3049 Training loss: 4.129e-05 Validation loss = 0.006487, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3050 Training loss: 3.702e-05 Validation loss = 0.005658, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3051 Training loss: 4.635e-05 Validation loss = 0.006751, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3052 Training loss: 3.519e-05 Validation loss = 0.00491, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3053 Training loss: 4.162e-05 Validation loss = 0.00744, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3054 Training loss: 4.142e-05 Validation loss = 0.007279, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3055 Training loss: 3.618e-05 Validation loss = 0.005021, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3056 Training loss: 3.767e-05 Validation loss = 0.008681, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3057 Training loss: 3.987e-05 Validation loss = 0.006733, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3058 Training loss: 4.149e-05 Validation loss = 0.006732, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3059 Training loss: 3.658e-05 Validation loss = 0.005844, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3060 Training loss: 3.781e-05 Validation loss = 0.006303, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0048105167911062134\n",
      "Epoch: 3061 Training loss: 5.881e-05 Validation loss = 0.004811, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3062 Training loss: 2.86e-05 Validation loss = 0.005708, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3063 Training loss: 3.551e-05 Validation loss = 0.005502, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3064 Training loss: 3.707e-05 Validation loss = 0.00668, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3065 Training loss: 4.122e-05 Validation loss = 0.005106, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.004776539735280309\n",
      "Epoch: 3066 Training loss: 3.985e-05 Validation loss = 0.004777, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3067 Training loss: 4.299e-05 Validation loss = 0.005657, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3068 Training loss: 3.682e-05 Validation loss = 0.005402, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3069 Training loss: 3.872e-05 Validation loss = 0.006357, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3070 Training loss: 4.048e-05 Validation loss = 0.006638, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3071 Training loss: 3.645e-05 Validation loss = 0.005574, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3072 Training loss: 4.028e-05 Validation loss = 0.005387, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3073 Training loss: 4.211e-05 Validation loss = 0.006919, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3074 Training loss: 3.657e-05 Validation loss = 0.008256, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3075 Training loss: 4.14e-05 Validation loss = 0.009677, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3076 Training loss: 3.736e-05 Validation loss = 0.005024, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3077 Training loss: 3.946e-05 Validation loss = 0.00838, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3078 Training loss: 4.766e-05 Validation loss = 0.01184, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3079 Training loss: 3.595e-05 Validation loss = 0.006199, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3080 Training loss: 4.052e-05 Validation loss = 0.00882, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3081 Training loss: 3.784e-05 Validation loss = 0.006157, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3082 Training loss: 3.416e-05 Validation loss = 0.008023, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3083 Training loss: 4.667e-05 Validation loss = 0.004843, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.004741373451692918\n",
      "Epoch: 3084 Training loss: 3.605e-05 Validation loss = 0.004741, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3085 Training loss: 5.137e-05 Validation loss = 0.005094, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3086 Training loss: 2.942e-05 Validation loss = 0.006856, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3087 Training loss: 3.787e-05 Validation loss = 0.006527, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3088 Training loss: 3.851e-05 Validation loss = 0.01091, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3089 Training loss: 3.885e-05 Validation loss = 0.004888, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3090 Training loss: 4.893e-05 Validation loss = 0.004865, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3091 Training loss: 2.809e-05 Validation loss = 0.007581, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3092 Training loss: 4.715e-05 Validation loss = 0.006238, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3093 Training loss: 3.397e-05 Validation loss = 0.006854, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3094 Training loss: 4.864e-05 Validation loss = 0.006485, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3095 Training loss: 3.203e-05 Validation loss = 0.005806, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3096 Training loss: 3.782e-05 Validation loss = 0.006515, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3097 Training loss: 3.774e-05 Validation loss = 0.006656, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3098 Training loss: 3.634e-05 Validation loss = 0.005784, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3099 Training loss: 4.386e-05 Validation loss = 0.008972, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3100 Training loss: 3.319e-05 Validation loss = 0.006813, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3101 Training loss: 4.282e-05 Validation loss = 0.006001, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3102 Training loss: 3.991e-05 Validation loss = 0.006601, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3103 Training loss: 3.804e-05 Validation loss = 0.005078, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3104 Training loss: 3.956e-05 Validation loss = 0.005981, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3105 Training loss: 3.769e-05 Validation loss = 0.004938, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3106 Training loss: 3.812e-05 Validation loss = 0.006746, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3107 Training loss: 4.465e-05 Validation loss = 0.00673, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3108 Training loss: 3.829e-05 Validation loss = 0.005287, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3109 Training loss: 3.772e-05 Validation loss = 0.007043, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3110 Training loss: 4.175e-05 Validation loss = 0.005423, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3111 Training loss: 3.461e-05 Validation loss = 0.007222, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3112 Training loss: 3.718e-05 Validation loss = 0.008804, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3113 Training loss: 4.305e-05 Validation loss = 0.004795, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3114 Training loss: 3.757e-05 Validation loss = 0.006178, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3115 Training loss: 3.959e-05 Validation loss = 0.005351, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3116 Training loss: 3.7e-05 Validation loss = 0.007606, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3117 Training loss: 4.016e-05 Validation loss = 0.005362, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3118 Training loss: 4.325e-05 Validation loss = 0.00672, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3119 Training loss: 3.447e-05 Validation loss = 0.00628, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3120 Training loss: 3.32e-05 Validation loss = 0.00561, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3121 Training loss: 5.043e-05 Validation loss = 0.005656, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3122 Training loss: 2.845e-05 Validation loss = 0.005888, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3123 Training loss: 3.657e-05 Validation loss = 0.006225, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3124 Training loss: 4.234e-05 Validation loss = 0.007252, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3125 Training loss: 3.705e-05 Validation loss = 0.005498, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3126 Training loss: 3.848e-05 Validation loss = 0.00617, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3127 Training loss: 4.01e-05 Validation loss = 0.006543, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3128 Training loss: 3.832e-05 Validation loss = 0.006133, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3129 Training loss: 3.853e-05 Validation loss = 0.008064, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3130 Training loss: 3.484e-05 Validation loss = 0.00515, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3131 Training loss: 4.448e-05 Validation loss = 0.006063, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3132 Training loss: 3.986e-05 Validation loss = 0.006153, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3133 Training loss: 3.127e-05 Validation loss = 0.005084, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3134 Training loss: 4.114e-05 Validation loss = 0.005993, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3135 Training loss: 3.873e-05 Validation loss = 0.006024, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3136 Training loss: 3.303e-05 Validation loss = 0.004999, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3137 Training loss: 5.239e-05 Validation loss = 0.005087, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3138 Training loss: 3.378e-05 Validation loss = 0.005112, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3139 Training loss: 3.428e-05 Validation loss = 0.006837, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3140 Training loss: 3.64e-05 Validation loss = 0.006371, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3141 Training loss: 3.974e-05 Validation loss = 0.006527, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3142 Training loss: 3.748e-05 Validation loss = 0.009898, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3143 Training loss: 4.422e-05 Validation loss = 0.004902, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3144 Training loss: 3.341e-05 Validation loss = 0.004945, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3145 Training loss: 4.109e-05 Validation loss = 0.0056, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3146 Training loss: 3.447e-05 Validation loss = 0.005039, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3147 Training loss: 3.901e-05 Validation loss = 0.007236, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3148 Training loss: 3.324e-05 Validation loss = 0.007248, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3149 Training loss: 4.809e-05 Validation loss = 0.01149, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3150 Training loss: 3.247e-05 Validation loss = 0.005414, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3151 Training loss: 3.958e-05 Validation loss = 0.006274, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3152 Training loss: 3.389e-05 Validation loss = 0.0049, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.004611572099380859\n",
      "Epoch: 3153 Training loss: 3.734e-05 Validation loss = 0.004612, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3154 Training loss: 5.088e-05 Validation loss = 0.005086, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3155 Training loss: 2.953e-05 Validation loss = 0.005425, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3156 Training loss: 3.345e-05 Validation loss = 0.008242, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3157 Training loss: 4.172e-05 Validation loss = 0.005395, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3158 Training loss: 3.427e-05 Validation loss = 0.005128, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3159 Training loss: 4.16e-05 Validation loss = 0.00649, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3160 Training loss: 5.369e-05 Validation loss = 0.004714, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3161 Training loss: 2.804e-05 Validation loss = 0.005553, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3162 Training loss: 3.912e-05 Validation loss = 0.005358, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3163 Training loss: 3.098e-05 Validation loss = 0.005387, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3164 Training loss: 4.315e-05 Validation loss = 0.005583, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3165 Training loss: 3.314e-05 Validation loss = 0.006713, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3166 Training loss: 3.633e-05 Validation loss = 0.006833, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3167 Training loss: 4.603e-05 Validation loss = 0.004948, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3168 Training loss: 3.327e-05 Validation loss = 0.005168, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3169 Training loss: 3.366e-05 Validation loss = 0.005847, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3170 Training loss: 4.437e-05 Validation loss = 0.005927, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3171 Training loss: 3.639e-05 Validation loss = 0.00495, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3172 Training loss: 3.22e-05 Validation loss = 0.006584, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3173 Training loss: 4.009e-05 Validation loss = 0.005767, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3174 Training loss: 3.791e-05 Validation loss = 0.007247, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3175 Training loss: 3.674e-05 Validation loss = 0.005107, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3176 Training loss: 3.875e-05 Validation loss = 0.006958, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3177 Training loss: 4.001e-05 Validation loss = 0.004768, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3178 Training loss: 3.726e-05 Validation loss = 0.005132, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3179 Training loss: 4.026e-05 Validation loss = 0.005693, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3180 Training loss: 3.338e-05 Validation loss = 0.005636, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3181 Training loss: 3.877e-05 Validation loss = 0.005069, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3182 Training loss: 3.203e-05 Validation loss = 0.004658, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3183 Training loss: 4.214e-05 Validation loss = 0.007511, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3184 Training loss: 3.729e-05 Validation loss = 0.005142, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3185 Training loss: 4.443e-05 Validation loss = 0.008382, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3186 Training loss: 3.68e-05 Validation loss = 0.004828, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3187 Training loss: 3.097e-05 Validation loss = 0.004774, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3188 Training loss: 3.576e-05 Validation loss = 0.006477, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3189 Training loss: 3.766e-05 Validation loss = 0.008228, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3190 Training loss: 4.104e-05 Validation loss = 0.006305, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3191 Training loss: 3.266e-05 Validation loss = 0.004794, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3192 Training loss: 4.361e-05 Validation loss = 0.006905, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3193 Training loss: 3.17e-05 Validation loss = 0.008217, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3194 Training loss: 4.245e-05 Validation loss = 0.006043, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3195 Training loss: 3.577e-05 Validation loss = 0.004614, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3196 Training loss: 4.193e-05 Validation loss = 0.006295, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3197 Training loss: 3.398e-05 Validation loss = 0.007028, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3198 Training loss: 3.288e-05 Validation loss = 0.005095, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3199 Training loss: 4.618e-05 Validation loss = 0.005175, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3200 Training loss: 3.242e-05 Validation loss = 0.005779, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3201 Training loss: 3.09e-05 Validation loss = 0.008898, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0045479238658556\n",
      "Epoch: 3202 Training loss: 5.161e-05 Validation loss = 0.004548, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3203 Training loss: 2.752e-05 Validation loss = 0.005694, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3204 Training loss: 4.127e-05 Validation loss = 0.004894, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3205 Training loss: 3.507e-05 Validation loss = 0.004719, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3206 Training loss: 3.872e-05 Validation loss = 0.01318, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3207 Training loss: 3.707e-05 Validation loss = 0.00468, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3208 Training loss: 4.373e-05 Validation loss = 0.005775, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3209 Training loss: 2.701e-05 Validation loss = 0.005141, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3210 Training loss: 3.546e-05 Validation loss = 0.007751, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3211 Training loss: 4.142e-05 Validation loss = 0.006006, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3212 Training loss: 3.865e-05 Validation loss = 0.006503, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3213 Training loss: 3.148e-05 Validation loss = 0.005565, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3214 Training loss: 4.425e-05 Validation loss = 0.004635, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3215 Training loss: 3.214e-05 Validation loss = 0.005247, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3216 Training loss: 3.349e-05 Validation loss = 0.005742, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3217 Training loss: 3.867e-05 Validation loss = 0.006419, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3218 Training loss: 3.445e-05 Validation loss = 0.004805, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3219 Training loss: 4.636e-05 Validation loss = 0.004862, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3220 Training loss: 2.857e-05 Validation loss = 0.006104, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3221 Training loss: 3.73e-05 Validation loss = 0.004947, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3222 Training loss: 4.071e-05 Validation loss = 0.006226, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3223 Training loss: 3.804e-05 Validation loss = 0.005326, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3224 Training loss: 3.3e-05 Validation loss = 0.005935, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3225 Training loss: 3.386e-05 Validation loss = 0.005525, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3226 Training loss: 4.577e-05 Validation loss = 0.004763, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3227 Training loss: 2.988e-05 Validation loss = 0.004747, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3228 Training loss: 3.612e-05 Validation loss = 0.006427, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3229 Training loss: 4.052e-05 Validation loss = 0.00692, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3230 Training loss: 3.2e-05 Validation loss = 0.00735, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3231 Training loss: 3.548e-05 Validation loss = 0.004932, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3232 Training loss: 4.458e-05 Validation loss = 0.006407, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3233 Training loss: 3.755e-05 Validation loss = 0.007453, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3234 Training loss: 3.551e-05 Validation loss =  0.006, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3235 Training loss: 3.598e-05 Validation loss = 0.006784, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3236 Training loss: 3.25e-05 Validation loss = 0.005036, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3237 Training loss: 4.066e-05 Validation loss = 0.006343, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3238 Training loss: 4.061e-05 Validation loss = 0.005639, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3239 Training loss: 2.827e-05 Validation loss = 0.005841, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3240 Training loss: 3.424e-05 Validation loss = 0.0064, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3241 Training loss: 3.818e-05 Validation loss = 0.00664, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3242 Training loss: 3.789e-05 Validation loss = 0.004811, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3243 Training loss: 3.63e-05 Validation loss = 0.004841, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3244 Training loss: 3.674e-05 Validation loss = 0.005814, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3245 Training loss: 4.414e-05 Validation loss = 0.007093, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3246 Training loss: 3.105e-05 Validation loss = 0.005765, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3247 Training loss: 3.801e-05 Validation loss = 0.004668, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3248 Training loss: 3.186e-05 Validation loss = 0.004556, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3249 Training loss: 3.148e-05 Validation loss = 0.005548, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3250 Training loss: 4.929e-05 Validation loss = 0.005418, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3251 Training loss: 3.234e-05 Validation loss = 0.004889, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3252 Training loss: 3.147e-05 Validation loss = 0.006029, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3253 Training loss: 3.666e-05 Validation loss = 0.005343, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3254 Training loss: 4.493e-05 Validation loss = 0.005892, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3255 Training loss: 2.903e-05 Validation loss = 0.00551, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3256 Training loss: 3.387e-05 Validation loss = 0.006997, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3257 Training loss: 3.235e-05 Validation loss = 0.006268, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3258 Training loss: 4.476e-05 Validation loss = 0.005336, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.004521139902310056\n",
      "Epoch: 3259 Training loss: 3.396e-05 Validation loss = 0.004521, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3260 Training loss: 3.713e-05 Validation loss = 0.004584, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.004467255570017291\n",
      "Epoch: 3261 Training loss: 3.31e-05 Validation loss = 0.004467, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3262 Training loss: 3.704e-05 Validation loss = 0.005473, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3263 Training loss: 3.66e-05 Validation loss = 0.005225, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3264 Training loss: 3.32e-05 Validation loss = 0.007271, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3265 Training loss: 4.097e-05 Validation loss = 0.004766, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3266 Training loss: 3.572e-05 Validation loss = 0.00472, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3267 Training loss: 3.398e-05 Validation loss = 0.005287, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3268 Training loss: 4.319e-05 Validation loss = 0.005254, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3269 Training loss: 2.93e-05 Validation loss = 0.004835, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3270 Training loss: 4.284e-05 Validation loss = 0.005443, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3271 Training loss: 2.676e-05 Validation loss = 0.006897, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3272 Training loss: 4.406e-05 Validation loss = 0.005597, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3273 Training loss: 3.425e-05 Validation loss = 0.006509, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3274 Training loss: 2.806e-05 Validation loss = 0.006807, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3275 Training loss: 3.808e-05 Validation loss = 0.006757, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3276 Training loss: 3.786e-05 Validation loss = 0.005071, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3277 Training loss: 3.396e-05 Validation loss = 0.005171, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3278 Training loss: 4.621e-05 Validation loss = 0.00462, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3279 Training loss: 2.708e-05 Validation loss = 0.004769, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3280 Training loss: 3.796e-05 Validation loss = 0.007211, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3281 Training loss: 3.086e-05 Validation loss = 0.005257, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3282 Training loss: 3.35e-05 Validation loss = 0.005646, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3283 Training loss: 3.717e-05 Validation loss = 0.005357, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3284 Training loss: 4.626e-05 Validation loss = 0.004621, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3285 Training loss: 2.805e-05 Validation loss = 0.01112, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3286 Training loss: 3.242e-05 Validation loss = 0.007157, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3287 Training loss: 3.536e-05 Validation loss = 0.006548, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3288 Training loss: 3.977e-05 Validation loss = 0.009555, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3289 Training loss: 3.168e-05 Validation loss = 0.008189, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3290 Training loss: 4.051e-05 Validation loss = 0.00644, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3291 Training loss: 3.356e-05 Validation loss = 0.005094, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3292 Training loss: 3.942e-05 Validation loss = 0.00559, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3293 Training loss: 3.068e-05 Validation loss = 0.006588, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3294 Training loss: 3.948e-05 Validation loss = 0.008851, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3295 Training loss: 3.171e-05 Validation loss = 0.004958, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3296 Training loss: 3.35e-05 Validation loss = 0.006053, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3297 Training loss: 3.733e-05 Validation loss = 0.007039, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3298 Training loss: 3.669e-05 Validation loss = 0.005207, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3299 Training loss: 2.966e-05 Validation loss = 0.005459, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3300 Training loss: 3.696e-05 Validation loss = 0.005625, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3301 Training loss: 3.768e-05 Validation loss = 0.005869, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3302 Training loss: 3.984e-05 Validation loss = 0.005425, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3303 Training loss: 3.661e-05 Validation loss = 0.00631, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3304 Training loss: 3.181e-05 Validation loss = 0.006235, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3305 Training loss: 2.983e-05 Validation loss = 0.006046, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3306 Training loss: 3.749e-05 Validation loss = 0.005867, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3307 Training loss: 4.045e-05 Validation loss = 0.004941, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3308 Training loss: 2.688e-05 Validation loss = 0.007118, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3309 Training loss: 4.592e-05 Validation loss = 0.006374, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3310 Training loss: 2.789e-05 Validation loss = 0.004529, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3311 Training loss: 3.756e-05 Validation loss = 0.006121, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3312 Training loss: 3.274e-05 Validation loss = 0.005466, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3313 Training loss: 3.356e-05 Validation loss =  0.005, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3314 Training loss: 3.658e-05 Validation loss = 0.004575, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3315 Training loss: 3.743e-05 Validation loss = 0.006296, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3316 Training loss: 3.612e-05 Validation loss = 0.005174, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3317 Training loss: 4.081e-05 Validation loss = 0.004575, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3318 Training loss: 2.854e-05 Validation loss = 0.00788, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3319 Training loss: 4.104e-05 Validation loss = 0.004888, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3320 Training loss: 3.365e-05 Validation loss = 0.00476, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0044547500125911325\n",
      "Epoch: 3321 Training loss: 2.671e-05 Validation loss = 0.004455, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3322 Training loss: 3.777e-05 Validation loss = 0.004589, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3323 Training loss: 3.944e-05 Validation loss = 0.005348, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3324 Training loss: 3.317e-05 Validation loss = 0.005968, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3325 Training loss: 3.242e-05 Validation loss = 0.005903, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3326 Training loss: 3.294e-05 Validation loss = 0.004947, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3327 Training loss: 3.521e-05 Validation loss = 0.004798, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3328 Training loss: 3.52e-05 Validation loss = 0.005083, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3329 Training loss: 3.809e-05 Validation loss = 0.00621, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3330 Training loss: 3.521e-05 Validation loss = 0.00769, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3331 Training loss: 3.128e-05 Validation loss = 0.005605, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3332 Training loss: 4.565e-05 Validation loss = 0.004936, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3333 Training loss: 2.327e-05 Validation loss = 0.005837, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3334 Training loss: 3.755e-05 Validation loss = 0.004766, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3335 Training loss: 3.061e-05 Validation loss = 0.006077, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3336 Training loss: 4.993e-05 Validation loss = 0.005125, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3337 Training loss: 2.672e-05 Validation loss = 0.004862, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3338 Training loss: 3.631e-05 Validation loss = 0.005148, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3339 Training loss: 2.937e-05 Validation loss = 0.007594, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3340 Training loss: 3.321e-05 Validation loss = 0.004697, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3341 Training loss: 3.988e-05 Validation loss = 0.004563, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3342 Training loss: 3.744e-05 Validation loss = 0.004816, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3343 Training loss: 2.738e-05 Validation loss = 0.005028, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3344 Training loss: 3.673e-05 Validation loss = 0.005586, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3345 Training loss: 4.254e-05 Validation loss = 0.004711, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3346 Training loss: 2.668e-05 Validation loss = 0.006281, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3347 Training loss: 3.239e-05 Validation loss = 0.005343, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3348 Training loss: 3.854e-05 Validation loss = 0.009316, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3349 Training loss: 3.544e-05 Validation loss = 0.006113, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3350 Training loss: 3.332e-05 Validation loss = 0.005517, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3351 Training loss: 3.74e-05 Validation loss = 0.006982, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.004328031095916705\n",
      "Epoch: 3352 Training loss: 2.87e-05 Validation loss = 0.004328, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3353 Training loss: 3.595e-05 Validation loss = 0.006011, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3354 Training loss: 3.333e-05 Validation loss = 0.00502, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3355 Training loss: 3.704e-05 Validation loss = 0.005407, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3356 Training loss: 3.265e-05 Validation loss = 0.006898, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3357 Training loss: 3.666e-05 Validation loss = 0.004478, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3358 Training loss: 3.071e-05 Validation loss = 0.00679, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3359 Training loss: 3.813e-05 Validation loss = 0.005572, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3360 Training loss: 3.229e-05 Validation loss = 0.004828, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3361 Training loss: 3.175e-05 Validation loss = 0.004671, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3362 Training loss: 3.638e-05 Validation loss = 0.004367, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3363 Training loss: 3.346e-05 Validation loss = 0.004512, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3364 Training loss: 4.863e-05 Validation loss = 0.009942, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3365 Training loss: 2.985e-05 Validation loss = 0.00624, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3366 Training loss: 2.702e-05 Validation loss = 0.004667, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3367 Training loss: 3.231e-05 Validation loss = 0.00816, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3368 Training loss: 3.814e-05 Validation loss = 0.004886, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3369 Training loss: 3.054e-05 Validation loss = 0.005993, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3370 Training loss: 3.506e-05 Validation loss = 0.005275, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3371 Training loss: 3.356e-05 Validation loss = 0.006892, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3372 Training loss: 3.504e-05 Validation loss = 0.005129, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3373 Training loss: 3.932e-05 Validation loss = 0.006878, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3374 Training loss: 2.75e-05 Validation loss = 0.005994, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3375 Training loss: 3.129e-05 Validation loss = 0.005339, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3376 Training loss: 3.526e-05 Validation loss = 0.005671, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3377 Training loss: 3.065e-05 Validation loss = 0.007515, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3378 Training loss: 4.185e-05 Validation loss = 0.007616, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3379 Training loss: 2.774e-05 Validation loss = 0.005517, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3380 Training loss: 3.617e-05 Validation loss = 0.006365, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3381 Training loss: 3.214e-05 Validation loss = 0.004977, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3382 Training loss: 3.693e-05 Validation loss = 0.004807, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3383 Training loss: 3.638e-05 Validation loss = 0.004391, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3384 Training loss: 2.729e-05 Validation loss = 0.005327, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3385 Training loss: 4.12e-05 Validation loss = 0.006013, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3386 Training loss: 2.544e-05 Validation loss = 0.004894, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3387 Training loss: 3.494e-05 Validation loss = 0.005608, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3388 Training loss: 3.259e-05 Validation loss = 0.004358, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3389 Training loss: 3.686e-05 Validation loss = 0.004426, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3390 Training loss: 3.571e-05 Validation loss = 0.004558, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3391 Training loss: 3.112e-05 Validation loss = 0.005047, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3392 Training loss: 3.134e-05 Validation loss = 0.005382, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3393 Training loss: 3.59e-05 Validation loss = 0.00621, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3394 Training loss: 4.1e-05 Validation loss = 0.008727, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.004190867401415398\n",
      "Epoch: 3395 Training loss: 2.665e-05 Validation loss = 0.004191, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3396 Training loss: 3.39e-05 Validation loss = 0.005028, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3397 Training loss: 2.99e-05 Validation loss = 0.00442, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3398 Training loss: 3.819e-05 Validation loss = 0.005809, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3399 Training loss: 3.627e-05 Validation loss = 0.008422, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3400 Training loss: 2.636e-05 Validation loss = 0.005326, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3401 Training loss: 3.879e-05 Validation loss = 0.005964, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3402 Training loss: 3.758e-05 Validation loss = 0.006724, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3403 Training loss: 2.708e-05 Validation loss = 0.005384, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3404 Training loss: 3.841e-05 Validation loss = 0.006149, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3405 Training loss: 3.043e-05 Validation loss = 0.007675, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3406 Training loss: 3.305e-05 Validation loss = 0.006618, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3407 Training loss: 3.163e-05 Validation loss = 0.00461, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3408 Training loss: 3.92e-05 Validation loss = 0.004469, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3409 Training loss: 2.724e-05 Validation loss = 0.005033, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3410 Training loss: 4.008e-05 Validation loss = 0.00556, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3411 Training loss: 2.659e-05 Validation loss = 0.004675, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3412 Training loss: 3.521e-05 Validation loss = 0.006427, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3413 Training loss: 3.442e-05 Validation loss = 0.007309, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3414 Training loss: 3.329e-05 Validation loss = 0.007738, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3415 Training loss: 4.422e-05 Validation loss = 0.007118, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3416 Training loss: 2.475e-05 Validation loss = 0.005472, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3417 Training loss: 2.918e-05 Validation loss = 0.007667, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3418 Training loss: 3.258e-05 Validation loss = 0.00457, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3419 Training loss: 3.586e-05 Validation loss = 0.005349, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3420 Training loss: 2.841e-05 Validation loss = 0.004866, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3421 Training loss: 3.35e-05 Validation loss = 0.01199, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3422 Training loss: 4.179e-05 Validation loss = 0.005567, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3423 Training loss: 3.289e-05 Validation loss = 0.004455, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3424 Training loss: 2.667e-05 Validation loss = 0.00692, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3425 Training loss: 3.571e-05 Validation loss = 0.005132, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3426 Training loss: 3.51e-05 Validation loss = 0.007947, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3427 Training loss: 3.095e-05 Validation loss = 0.004968, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3428 Training loss: 3.44e-05 Validation loss = 0.009583, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3429 Training loss: 3.223e-05 Validation loss = 0.005363, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3430 Training loss: 3.56e-05 Validation loss = 0.006277, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3431 Training loss: 2.739e-05 Validation loss = 0.0044, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3432 Training loss: 3.41e-05 Validation loss = 0.005483, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3433 Training loss: 3.196e-05 Validation loss = 0.005711, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3434 Training loss: 3.687e-05 Validation loss = 0.007463, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3435 Training loss: 3.558e-05 Validation loss = 0.004918, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3436 Training loss: 3.133e-05 Validation loss = 0.005309, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3437 Training loss: 3.615e-05 Validation loss = 0.01086, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3438 Training loss: 2.968e-05 Validation loss = 0.00461, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3439 Training loss: 2.99e-05 Validation loss = 0.004576, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3440 Training loss: 3.432e-05 Validation loss = 0.006097, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3441 Training loss: 2.993e-05 Validation loss = 0.007267, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3442 Training loss: 3.423e-05 Validation loss = 0.004856, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3443 Training loss: 3.474e-05 Validation loss = 0.00755, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3444 Training loss: 3.525e-05 Validation loss = 0.008043, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3445 Training loss: 2.894e-05 Validation loss = 0.004209, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3446 Training loss: 3.195e-05 Validation loss = 0.004758, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3447 Training loss: 3.442e-05 Validation loss = 0.007594, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3448 Training loss: 3.032e-05 Validation loss = 0.004487, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3449 Training loss: 4.546e-05 Validation loss = 0.004922, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.00416986400728832\n",
      "Epoch: 3450 Training loss: 2.171e-05 Validation loss = 0.00417, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3451 Training loss: 3.944e-05 Validation loss = 0.005876, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3452 Training loss: 2.596e-05 Validation loss = 0.00474, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3453 Training loss: 3.312e-05 Validation loss = 0.008735, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3454 Training loss: 3.552e-05 Validation loss = 0.007274, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3455 Training loss: 3.535e-05 Validation loss = 0.005226, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3456 Training loss: 2.405e-05 Validation loss = 0.004243, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.004141861036088951\n",
      "Epoch: 3457 Training loss: 3.911e-05 Validation loss = 0.004142, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3458 Training loss: 2.758e-05 Validation loss = 0.005606, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3459 Training loss: 4.37e-05 Validation loss = 0.004197, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3460 Training loss: 2.409e-05 Validation loss = 0.00454, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3461 Training loss: 3.178e-05 Validation loss = 0.006162, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3462 Training loss: 3.414e-05 Validation loss = 0.005895, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3463 Training loss: 3.175e-05 Validation loss = 0.004492, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3464 Training loss: 2.86e-05 Validation loss = 0.006933, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3465 Training loss: 3.716e-05 Validation loss = 0.005205, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3466 Training loss: 3.023e-05 Validation loss = 0.005975, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3467 Training loss: 3.169e-05 Validation loss = 0.005079, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3468 Training loss: 3.156e-05 Validation loss = 0.004594, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3469 Training loss: 3.537e-05 Validation loss = 0.005133, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3470 Training loss: 3.445e-05 Validation loss = 0.005312, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3471 Training loss: 2.969e-05 Validation loss = 0.004866, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.004095468182850552\n",
      "Epoch: 3472 Training loss: 2.935e-05 Validation loss = 0.004095, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3473 Training loss: 3.172e-05 Validation loss = 0.004822, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3474 Training loss: 3.667e-05 Validation loss = 0.004183, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3475 Training loss: 2.633e-05 Validation loss = 0.005745, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3476 Training loss: 3.474e-05 Validation loss = 0.006929, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3477 Training loss: 3.536e-05 Validation loss = 0.01284, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.004053589947517567\n",
      "Epoch: 3478 Training loss: 3.237e-05 Validation loss = 0.004054, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3479 Training loss: 2.865e-05 Validation loss = 0.00461, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3480 Training loss: 3.598e-05 Validation loss = 0.004409, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3481 Training loss: 3.412e-05 Validation loss = 0.005893, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3482 Training loss: 2.511e-05 Validation loss = 0.004632, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3483 Training loss: 3.63e-05 Validation loss = 0.004592, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3484 Training loss: 3.087e-05 Validation loss = 0.007678, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3485 Training loss: 3.129e-05 Validation loss = 0.004612, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3486 Training loss: 3.326e-05 Validation loss = 0.005346, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3487 Training loss: 4.105e-05 Validation loss = 0.00639, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3488 Training loss: 2.168e-05 Validation loss = 0.004896, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3489 Training loss: 3.239e-05 Validation loss = 0.004199, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3490 Training loss: 3.637e-05 Validation loss = 0.004703, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3491 Training loss: 2.647e-05 Validation loss = 0.00564, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3492 Training loss: 3.419e-05 Validation loss = 0.004334, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3493 Training loss: 3.101e-05 Validation loss = 0.004695, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3494 Training loss: 3.335e-05 Validation loss = 0.005284, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3495 Training loss: 2.636e-05 Validation loss = 0.005424, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3496 Training loss: 4.6e-05 Validation loss = 0.005394, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3497 Training loss: 2.235e-05 Validation loss = 0.004847, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3498 Training loss: 3.043e-05 Validation loss = 0.006017, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3499 Training loss: 3.395e-05 Validation loss = 0.004652, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3500 Training loss: 2.811e-05 Validation loss = 0.004069, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3501 Training loss: 3.761e-05 Validation loss = 0.004558, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3502 Training loss: 3.082e-05 Validation loss = 0.007491, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3503 Training loss: 3.311e-05 Validation loss = 0.006238, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3504 Training loss: 2.947e-05 Validation loss = 0.004573, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3505 Training loss: 3.204e-05 Validation loss = 0.004495, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3506 Training loss: 3.193e-05 Validation loss = 0.004537, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3507 Training loss: 3.632e-05 Validation loss = 0.006052, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3508 Training loss: 2.79e-05 Validation loss = 0.00435, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3509 Training loss: 3.43e-05 Validation loss = 0.004918, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3510 Training loss: 3.029e-05 Validation loss = 0.004981, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3511 Training loss: 2.904e-05 Validation loss = 0.005667, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3512 Training loss: 3.125e-05 Validation loss = 0.004641, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3513 Training loss: 4.015e-05 Validation loss = 0.004203, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3514 Training loss: 2.365e-05 Validation loss = 0.007315, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3515 Training loss: 3.063e-05 Validation loss =  0.005, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3516 Training loss: 3.035e-05 Validation loss = 0.00804, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0039453290675950095\n",
      "Epoch: 3517 Training loss: 3.65e-05 Validation loss = 0.003945, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3518 Training loss: 2.88e-05 Validation loss = 0.006244, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3519 Training loss: 3.087e-05 Validation loss = 0.005223, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3520 Training loss: 3.148e-05 Validation loss = 0.00706, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3521 Training loss: 3.664e-05 Validation loss = 0.003982, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3522 Training loss: 3.787e-05 Validation loss = 0.005724, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3523 Training loss: 2.531e-05 Validation loss = 0.004172, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3524 Training loss: 2.756e-05 Validation loss = 0.007321, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3525 Training loss: 3.242e-05 Validation loss = 0.004856, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3526 Training loss: 2.989e-05 Validation loss = 0.005731, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3527 Training loss: 3.422e-05 Validation loss = 0.004188, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3528 Training loss: 2.953e-05 Validation loss = 0.006404, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3529 Training loss: 3.503e-05 Validation loss = 0.006581, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3530 Training loss: 2.547e-05 Validation loss = 0.005988, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3531 Training loss: 3.618e-05 Validation loss = 0.005048, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3532 Training loss: 2.877e-05 Validation loss = 0.004878, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3533 Training loss: 2.986e-05 Validation loss = 0.005075, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3534 Training loss: 3.225e-05 Validation loss = 0.006423, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3535 Training loss: 2.961e-05 Validation loss = 0.006482, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3536 Training loss: 4.031e-05 Validation loss = 0.004192, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3537 Training loss: 2.467e-05 Validation loss = 0.005721, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3538 Training loss: 4.044e-05 Validation loss = 0.004632, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3539 Training loss: 2.232e-05 Validation loss = 0.006072, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3540 Training loss: 3.729e-05 Validation loss = 0.004482, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3541 Training loss: 3.082e-05 Validation loss = 0.007317, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3542 Training loss: 2.843e-05 Validation loss = 0.006131, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3543 Training loss: 3.217e-05 Validation loss = 0.004162, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3544 Training loss: 3.186e-05 Validation loss = 0.004052, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3545 Training loss: 3.465e-05 Validation loss = 0.004405, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3546 Training loss: 2.409e-05 Validation loss = 0.00498, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3547 Training loss: 3.123e-05 Validation loss = 0.004305, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3548 Training loss: 4.081e-05 Validation loss = 0.004886, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3549 Training loss: 2.424e-05 Validation loss = 0.005627, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3550 Training loss: 3.325e-05 Validation loss = 0.005382, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3551 Training loss: 2.408e-05 Validation loss = 0.004251, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3552 Training loss: 3.366e-05 Validation loss = 0.005285, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3553 Training loss: 2.817e-05 Validation loss = 0.00715, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3554 Training loss: 3.695e-05 Validation loss = 0.004343, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3555 Training loss: 2.861e-05 Validation loss = 0.005731, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3556 Training loss: 3.034e-05 Validation loss = 0.00555, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3557 Training loss: 3.015e-05 Validation loss = 0.007131, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3558 Training loss: 2.917e-05 Validation loss = 0.01093, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3559 Training loss: 3.225e-05 Validation loss = 0.004831, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3560 Training loss: 2.943e-05 Validation loss = 0.005505, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3561 Training loss: 3.877e-05 Validation loss = 0.004943, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3562 Training loss: 2.805e-05 Validation loss = 0.006037, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3563 Training loss: 2.972e-05 Validation loss = 0.006551, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3564 Training loss: 2.998e-05 Validation loss = 0.006518, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0038753183076953284\n",
      "Epoch: 3565 Training loss: 3.745e-05 Validation loss = 0.003875, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3566 Training loss: 2.371e-05 Validation loss = 0.004914, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3567 Training loss: 3.174e-05 Validation loss = 0.00589, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3568 Training loss: 2.848e-05 Validation loss = 0.006964, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3569 Training loss: 3.483e-05 Validation loss = 0.004378, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3570 Training loss: 2.693e-05 Validation loss = 0.004715, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3571 Training loss: 2.881e-05 Validation loss = 0.005336, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3572 Training loss: 3.547e-05 Validation loss = 0.005082, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3573 Training loss: 3.245e-05 Validation loss = 0.004493, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3574 Training loss: 3.046e-05 Validation loss = 0.006073, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3575 Training loss: 2.677e-05 Validation loss = 0.004688, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3576 Training loss: 3.462e-05 Validation loss = 0.004339, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3577 Training loss: 2.952e-05 Validation loss = 0.004351, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3578 Training loss: 3.27e-05 Validation loss = 0.004174, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3579 Training loss: 3.373e-05 Validation loss = 0.004364, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3580 Training loss: 2.383e-05 Validation loss = 0.004843, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3581 Training loss: 3.87e-05 Validation loss = 0.004077, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3582 Training loss: 2.244e-05 Validation loss = 0.00436, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3583 Training loss: 3.406e-05 Validation loss = 0.004733, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3584 Training loss: 3.184e-05 Validation loss = 0.005572, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3585 Training loss: 2.799e-05 Validation loss = 0.004921, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3586 Training loss: 3.723e-05 Validation loss = 0.00485, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3587 Training loss: 2.26e-05 Validation loss = 0.004926, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3588 Training loss: 3.136e-05 Validation loss = 0.004801, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3589 Training loss: 3.027e-05 Validation loss = 0.005618, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3590 Training loss: 3.119e-05 Validation loss = 0.004834, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3591 Training loss: 3.192e-05 Validation loss = 0.004643, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3592 Training loss: 2.773e-05 Validation loss = 0.004582, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3593 Training loss: 2.941e-05 Validation loss = 0.005506, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3594 Training loss: 3.947e-05 Validation loss = 0.004542, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3595 Training loss: 2.562e-05 Validation loss = 0.004888, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3596 Training loss: 2.695e-05 Validation loss = 0.006017, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3597 Training loss: 3.564e-05 Validation loss = 0.004453, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3598 Training loss: 2.606e-05 Validation loss = 0.005223, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3599 Training loss: 3.358e-05 Validation loss = 0.004328, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3600 Training loss: 2.779e-05 Validation loss = 0.005373, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3601 Training loss: 2.739e-05 Validation loss = 0.006072, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3602 Training loss: 3.218e-05 Validation loss = 0.00475, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3603 Training loss: 3.429e-05 Validation loss = 0.01251, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3604 Training loss: 2.97e-05 Validation loss = 0.004098, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3605 Training loss: 3.254e-05 Validation loss = 0.006983, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3606 Training loss: 2.465e-05 Validation loss = 0.004817, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3607 Training loss: 3.383e-05 Validation loss = 0.004549, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3608 Training loss: 2.865e-05 Validation loss = 0.00445, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3609 Training loss: 3.675e-05 Validation loss = 0.003936, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3610 Training loss: 2.462e-05 Validation loss = 0.005837, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3611 Training loss: 3.255e-05 Validation loss = 0.004227, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3612 Training loss: 2.639e-05 Validation loss = 0.00639, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3613 Training loss: 2.917e-05 Validation loss = 0.004261, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3614 Training loss: 3.547e-05 Validation loss = 0.00476, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3615 Training loss: 2.937e-05 Validation loss = 0.005268, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3616 Training loss: 2.557e-05 Validation loss = 0.006486, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3617 Training loss: 3.234e-05 Validation loss = 0.005851, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3618 Training loss: 2.973e-05 Validation loss = 0.003937, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3619 Training loss: 2.994e-05 Validation loss = 0.004553, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3620 Training loss: 3.642e-05 Validation loss = 0.005123, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3621 Training loss: 2.384e-05 Validation loss = 0.004272, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3622 Training loss: 3.16e-05 Validation loss = 0.004041, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3623 Training loss: 3.029e-05 Validation loss = 0.00497, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3624 Training loss: 2.747e-05 Validation loss = 0.003976, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3625 Training loss: 4.187e-05 Validation loss = 0.005126, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3626 Training loss: 2.226e-05 Validation loss = 0.005667, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3627 Training loss: 3.866e-05 Validation loss = 0.00467, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3628 Training loss: 2.172e-05 Validation loss = 0.008362, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3629 Training loss: 2.869e-05 Validation loss = 0.00626, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3630 Training loss: 2.883e-05 Validation loss = 0.004467, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3631 Training loss: 3.819e-05 Validation loss = 0.003936, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3632 Training loss: 2.419e-05 Validation loss = 0.01012, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3633 Training loss: 3.092e-05 Validation loss = 0.004293, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3634 Training loss: 2.916e-05 Validation loss = 0.004342, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3635 Training loss: 2.748e-05 Validation loss = 0.007452, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3636 Training loss: 2.95e-05 Validation loss = 0.004321, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3637 Training loss: 3.826e-05 Validation loss = 0.006757, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3638 Training loss: 2.547e-05 Validation loss = 0.004463, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3639 Training loss: 2.953e-05 Validation loss = 0.005117, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3640 Training loss: 3.152e-05 Validation loss = 0.004372, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3641 Training loss: 2.612e-05 Validation loss = 0.004367, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3642 Training loss: 3.06e-05 Validation loss = 0.007528, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3643 Training loss: 3.268e-05 Validation loss = 0.004309, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3644 Training loss: 3.24e-05 Validation loss = 0.005085, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3645 Training loss: 2.494e-05 Validation loss = 0.004255, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3646 Training loss: 3.115e-05 Validation loss = 0.004328, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3647 Training loss: 2.679e-05 Validation loss = 0.00639, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3648 Training loss: 2.967e-05 Validation loss = 0.004353, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3649 Training loss: 2.974e-05 Validation loss = 0.004898, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3650 Training loss: 3.102e-05 Validation loss = 0.005874, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3651 Training loss: 2.984e-05 Validation loss = 0.005502, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3652 Training loss: 3.077e-05 Validation loss = 0.004807, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3653 Training loss: 3.259e-05 Validation loss = 0.004097, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3654 Training loss: 2.688e-05 Validation loss = 0.005211, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3655 Training loss: 3.361e-05 Validation loss = 0.006749, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3656 Training loss: 2.449e-05 Validation loss = 0.004121, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3657 Training loss: 3.108e-05 Validation loss = 0.00464, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3658 Training loss: 3.01e-05 Validation loss = 0.005307, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3659 Training loss: 3.011e-05 Validation loss = 0.004217, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3660 Training loss: 2.39e-05 Validation loss = 0.009181, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3661 Training loss: 3.814e-05 Validation loss = 0.004109, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3662 Training loss: 2.589e-05 Validation loss = 0.004767, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3663 Training loss: 2.737e-05 Validation loss = 0.005374, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3664 Training loss: 4.332e-05 Validation loss = 0.004442, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3665 Training loss: 2.049e-05 Validation loss = 0.005542, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3666 Training loss: 2.869e-05 Validation loss = 0.005134, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3667 Training loss: 3.507e-05 Validation loss = 0.005427, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3668 Training loss: 2.406e-05 Validation loss = 0.007927, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3669 Training loss: 3.085e-05 Validation loss = 0.006112, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3670 Training loss: 2.874e-05 Validation loss = 0.004466, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3671 Training loss: 3.013e-05 Validation loss = 0.006584, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3672 Training loss: 2.964e-05 Validation loss = 0.006849, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3673 Training loss: 2.719e-05 Validation loss = 0.004582, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3674 Training loss: 2.84e-05 Validation loss = 0.004484, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3675 Training loss: 3.106e-05 Validation loss = 0.005268, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3676 Training loss: 2.58e-05 Validation loss = 0.01074, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3677 Training loss: 3.407e-05 Validation loss = 0.004513, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3678 Training loss: 2.643e-05 Validation loss = 0.00686, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3679 Training loss: 3.429e-05 Validation loss = 0.00417, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3680 Training loss: 2.619e-05 Validation loss = 0.00411, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3681 Training loss: 3.178e-05 Validation loss = 0.005533, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3682 Training loss: 2.956e-05 Validation loss = 0.004483, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3683 Training loss: 2.434e-05 Validation loss = 0.005761, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3684 Training loss: 3.145e-05 Validation loss = 0.004226, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3685 Training loss: 3.462e-05 Validation loss = 0.004317, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3686 Training loss: 2.697e-05 Validation loss = 0.004015, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3687 Training loss: 2.827e-05 Validation loss = 0.006991, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3688 Training loss: 2.902e-05 Validation loss = 0.004549, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3689 Training loss: 2.953e-05 Validation loss = 0.004629, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3690 Training loss: 2.437e-05 Validation loss = 0.004565, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3691 Training loss: 3.508e-05 Validation loss = 0.004703, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3692 Training loss: 2.735e-05 Validation loss = 0.003999, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3693 Training loss: 3.487e-05 Validation loss = 0.005441, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3694 Training loss: 2.236e-05 Validation loss = 0.005413, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3695 Training loss: 3.134e-05 Validation loss = 0.005151, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3696 Training loss: 3.311e-05 Validation loss = 0.005068, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3697 Training loss: 2.693e-05 Validation loss = 0.004446, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3698 Training loss: 3.204e-05 Validation loss = 0.00502, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3699 Training loss: 2.715e-05 Validation loss = 0.01566, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3700 Training loss: 3.147e-05 Validation loss = 0.004951, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3701 Training loss: 2.222e-05 Validation loss = 0.004299, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0037989081782140214\n",
      "Epoch: 3702 Training loss: 3.621e-05 Validation loss = 0.003799, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3703 Training loss: 2.769e-05 Validation loss = 0.004108, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3704 Training loss: 2.497e-05 Validation loss = 0.006821, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3705 Training loss: 3.403e-05 Validation loss = 0.004347, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3706 Training loss: 2.748e-05 Validation loss = 0.004679, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3707 Training loss: 3.075e-05 Validation loss = 0.006155, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3708 Training loss: 2.702e-05 Validation loss = 0.003954, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3709 Training loss: 2.935e-05 Validation loss = 0.006192, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3710 Training loss: 2.884e-05 Validation loss = 0.004556, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3711 Training loss: 3.491e-05 Validation loss = 0.005396, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3712 Training loss: 2.243e-05 Validation loss = 0.005991, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3713 Training loss: 3.194e-05 Validation loss = 0.005043, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3714 Training loss: 2.887e-05 Validation loss = 0.008703, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3715 Training loss: 2.996e-05 Validation loss = 0.006783, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3716 Training loss: 2.611e-05 Validation loss = 0.004335, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3717 Training loss: 3.137e-05 Validation loss = 0.004847, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3718 Training loss: 3.079e-05 Validation loss = 0.008681, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3719 Training loss: 2.566e-05 Validation loss = 0.004456, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3720 Training loss: 3.041e-05 Validation loss = 0.00531, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3721 Training loss: 2.892e-05 Validation loss = 0.003849, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3722 Training loss: 2.924e-05 Validation loss = 0.004054, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3723 Training loss: 2.856e-05 Validation loss = 0.005312, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3724 Training loss: 2.787e-05 Validation loss = 0.004641, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3725 Training loss: 3.104e-05 Validation loss = 0.003914, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3726 Training loss: 2.583e-05 Validation loss = 0.004752, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3727 Training loss: 3.121e-05 Validation loss = 0.006439, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3728 Training loss: 3.537e-05 Validation loss = 0.005579, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3729 Training loss: 2.073e-05 Validation loss = 0.004784, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3730 Training loss: 2.8e-05 Validation loss = 0.006532, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3731 Training loss: 2.994e-05 Validation loss = 0.007697, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3732 Training loss: 3.317e-05 Validation loss = 0.004238, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3733 Training loss: 2.463e-05 Validation loss = 0.004585, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3734 Training loss: 2.856e-05 Validation loss = 0.009682, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3735 Training loss: 2.78e-05 Validation loss = 0.006341, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3736 Training loss: 2.864e-05 Validation loss = 0.004887, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3737 Training loss: 3.02e-05 Validation loss = 0.005307, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3738 Training loss: 2.939e-05 Validation loss = 0.005236, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3739 Training loss: 3.018e-05 Validation loss = 0.004575, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3740 Training loss: 2.628e-05 Validation loss = 0.008376, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3741 Training loss: 4.358e-05 Validation loss = 0.005266, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0037786949262800414\n",
      "Epoch: 3742 Training loss: 2.174e-05 Validation loss = 0.003779, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3743 Training loss: 2.078e-05 Validation loss = 0.004674, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3744 Training loss: 3.244e-05 Validation loss = 0.006507, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3745 Training loss: 2.97e-05 Validation loss = 0.003936, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3746 Training loss: 2.566e-05 Validation loss = 0.00424, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3747 Training loss: 3.933e-05 Validation loss = 0.004639, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3748 Training loss: 2.194e-05 Validation loss = 0.004585, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3749 Training loss: 3.131e-05 Validation loss = 0.006486, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3750 Training loss: 2.342e-05 Validation loss =  0.009, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3751 Training loss: 3.079e-05 Validation loss = 0.005946, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3752 Training loss: 2.528e-05 Validation loss = 0.005492, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.003690678216057414\n",
      "Epoch: 3753 Training loss: 3.221e-05 Validation loss = 0.003691, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3754 Training loss: 2.723e-05 Validation loss = 0.007887, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3755 Training loss: 3.731e-05 Validation loss = 0.007517, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3756 Training loss: 2.443e-05 Validation loss = 0.005619, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3757 Training loss: 2.414e-05 Validation loss = 0.004277, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3758 Training loss: 2.888e-05 Validation loss = 0.01165, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3759 Training loss: 3.047e-05 Validation loss = 0.004722, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3760 Training loss: 2.664e-05 Validation loss = 0.005848, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3761 Training loss: 2.85e-05 Validation loss = 0.003877, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3762 Training loss: 2.904e-05 Validation loss = 0.005618, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3763 Training loss: 3.405e-05 Validation loss = 0.003801, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3764 Training loss: 2.288e-05 Validation loss = 0.00376, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3765 Training loss: 3.86e-05 Validation loss = 0.004103, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3766 Training loss: 2.128e-05 Validation loss = 0.00449, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3767 Training loss: 2.904e-05 Validation loss = 0.003731, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3768 Training loss: 3.056e-05 Validation loss = 0.004354, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3769 Training loss: 2.421e-05 Validation loss = 0.007589, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3770 Training loss: 2.972e-05 Validation loss = 0.00422, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3771 Training loss: 2.83e-05 Validation loss = 0.004022, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3772 Training loss: 3.021e-05 Validation loss = 0.004302, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3773 Training loss: 2.595e-05 Validation loss = 0.005138, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3774 Training loss: 2.798e-05 Validation loss = 0.00534, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3775 Training loss: 2.661e-05 Validation loss = 0.003945, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3776 Training loss: 3.391e-05 Validation loss = 0.004095, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3777 Training loss: 2.373e-05 Validation loss = 0.005075, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3778 Training loss: 3.044e-05 Validation loss = 0.005231, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3779 Training loss: 3.063e-05 Validation loss = 0.005307, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3780 Training loss: 2.895e-05 Validation loss = 0.004718, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3781 Training loss: 2.522e-05 Validation loss = 0.005633, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3782 Training loss: 2.858e-05 Validation loss = 0.005349, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3783 Training loss: 3.395e-05 Validation loss = 0.004365, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3784 Training loss: 2.155e-05 Validation loss = 0.003799, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0036530462003085454\n",
      "Epoch: 3785 Training loss: 3.095e-05 Validation loss = 0.003653, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3786 Training loss: 2.69e-05 Validation loss = 0.009233, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3787 Training loss: 3.158e-05 Validation loss = 0.004071, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3788 Training loss: 4.038e-05 Validation loss = 0.005006, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3789 Training loss: 2.118e-05 Validation loss = 0.003837, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3790 Training loss: 2.203e-05 Validation loss = 0.004644, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3791 Training loss: 3.353e-05 Validation loss = 0.005096, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3792 Training loss: 2.945e-05 Validation loss = 0.004897, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3793 Training loss: 2.337e-05 Validation loss = 0.004161, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3794 Training loss: 2.99e-05 Validation loss = 0.004615, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3795 Training loss: 2.877e-05 Validation loss = 0.006625, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3796 Training loss: 2.571e-05 Validation loss = 0.003756, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3797 Training loss: 3.875e-05 Validation loss = 0.003731, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3798 Training loss: 1.951e-05 Validation loss = 0.005007, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3799 Training loss: 2.724e-05 Validation loss = 0.004828, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3800 Training loss: 2.973e-05 Validation loss = 0.005715, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3801 Training loss: 3.344e-05 Validation loss = 0.005962, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3802 Training loss: 2.078e-05 Validation loss = 0.004491, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3803 Training loss: 2.776e-05 Validation loss = 0.004268, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3804 Training loss: 3.005e-05 Validation loss = 0.004359, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3805 Training loss: 3.179e-05 Validation loss = 0.005992, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3806 Training loss: 2.319e-05 Validation loss = 0.005113, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3807 Training loss: 2.92e-05 Validation loss = 0.005298, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3808 Training loss: 2.952e-05 Validation loss = 0.004858, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3809 Training loss: 2.393e-05 Validation loss = 0.003924, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3810 Training loss: 3.248e-05 Validation loss = 0.00421, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3811 Training loss: 2.688e-05 Validation loss = 0.004679, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3812 Training loss: 2.934e-05 Validation loss = 0.007537, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3813 Training loss: 2.767e-05 Validation loss = 0.00593, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3814 Training loss: 3.019e-05 Validation loss = 0.004679, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3815 Training loss: 2.357e-05 Validation loss = 0.004074, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3816 Training loss: 3.222e-05 Validation loss = 0.003701, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3817 Training loss: 2.638e-05 Validation loss = 0.005231, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3818 Training loss: 3.537e-05 Validation loss = 0.003871, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3819 Training loss: 2.185e-05 Validation loss = 0.005085, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3820 Training loss: 2.347e-05 Validation loss = 0.005949, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3821 Training loss: 3.624e-05 Validation loss = 0.003737, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3822 Training loss: 2.148e-05 Validation loss = 0.004861, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3823 Training loss: 2.836e-05 Validation loss = 0.003848, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3824 Training loss: 2.937e-05 Validation loss = 0.004276, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3825 Training loss: 2.715e-05 Validation loss = 0.005757, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3826 Training loss: 3.188e-05 Validation loss = 0.004427, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3827 Training loss: 2.688e-05 Validation loss = 0.004609, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3828 Training loss: 2.562e-05 Validation loss = 0.004711, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3829 Training loss: 2.942e-05 Validation loss = 0.004077, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3830 Training loss: 2.632e-05 Validation loss = 0.007325, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3831 Training loss: 3.112e-05 Validation loss = 0.005814, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3832 Training loss: 2.732e-05 Validation loss = 0.006428, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3833 Training loss: 3.986e-05 Validation loss = 0.008747, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3834 Training loss: 2.237e-05 Validation loss = 0.003662, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.0035977566486810057\n",
      "Epoch: 3835 Training loss: 2.436e-05 Validation loss = 0.003598, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3836 Training loss: 2.382e-05 Validation loss = 0.003755, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3837 Training loss: 2.852e-05 Validation loss = 0.004977, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3838 Training loss: 3.114e-05 Validation loss = 0.004344, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3839 Training loss: 2.564e-05 Validation loss = 0.003926, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3840 Training loss: 3.017e-05 Validation loss = 0.00438, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3841 Training loss: 2.448e-05 Validation loss = 0.003915, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3842 Training loss: 3.052e-05 Validation loss = 0.005252, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3843 Training loss: 2.724e-05 Validation loss = 0.007531, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3844 Training loss: 2.555e-05 Validation loss = 0.004274, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3845 Training loss: 2.907e-05 Validation loss = 0.005894, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3846 Training loss: 2.778e-05 Validation loss = 0.003892, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3847 Training loss: 3.349e-05 Validation loss = 0.004212, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3848 Training loss: 2.444e-05 Validation loss = 0.007159, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3849 Training loss: 2.182e-05 Validation loss = 0.01171, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3850 Training loss: 3.419e-05 Validation loss = 0.004268, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3851 Training loss: 2.841e-05 Validation loss = 0.006326, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3852 Training loss: 2.649e-05 Validation loss = 0.006241, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3853 Training loss: 3.491e-05 Validation loss = 0.005042, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3854 Training loss: 2.029e-05 Validation loss = 0.00401, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3855 Training loss: 3.051e-05 Validation loss = 0.004493, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3856 Training loss: 2.467e-05 Validation loss = 0.006161, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3857 Training loss: 3.186e-05 Validation loss = 0.004387, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3858 Training loss: 3.046e-05 Validation loss = 0.005272, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3859 Training loss: 2.462e-05 Validation loss = 0.004038, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3860 Training loss: 2.648e-05 Validation loss = 0.005639, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3861 Training loss: 2.768e-05 Validation loss = 0.005149, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3862 Training loss: 3.775e-05 Validation loss = 0.003975, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3863 Training loss: 1.801e-05 Validation loss = 0.004191, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3864 Training loss: 3.327e-05 Validation loss = 0.005367, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3865 Training loss: 2.17e-05 Validation loss = 0.00456, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3866 Training loss: 2.63e-05 Validation loss = 0.0057, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3867 Training loss: 3.622e-05 Validation loss = 0.003915, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3868 Training loss: 2.31e-05 Validation loss = 0.004496, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3869 Training loss: 2.639e-05 Validation loss = 0.004221, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3870 Training loss: 3.045e-05 Validation loss = 0.006398, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3871 Training loss: 2.698e-05 Validation loss = 0.005006, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3872 Training loss: 2.556e-05 Validation loss = 0.004264, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3873 Training loss: 3.104e-05 Validation loss = 0.00584, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3874 Training loss: 2.673e-05 Validation loss = 0.006211, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3875 Training loss: 2.564e-05 Validation loss = 0.004529, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3876 Training loss: 2.867e-05 Validation loss = 0.004186, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3877 Training loss: 2.986e-05 Validation loss = 0.004804, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3878 Training loss: 2.471e-05 Validation loss = 0.006131, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3879 Training loss: 2.704e-05 Validation loss = 0.004097, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3880 Training loss: 3.143e-05 Validation loss = 0.005665, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3881 Training loss: 2.543e-05 Validation loss = 0.004762, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3882 Training loss: 2.959e-05 Validation loss = 0.004171, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3883 Training loss: 2.567e-05 Validation loss = 0.008355, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3884 Training loss: 2.858e-05 Validation loss = 0.006091, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3885 Training loss: 2.539e-05 Validation loss = 0.004574, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3886 Training loss: 2.706e-05 Validation loss = 0.004359, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3887 Training loss: 2.576e-05 Validation loss = 0.005689, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3888 Training loss: 3.432e-05 Validation loss = 0.005774, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3889 Training loss: 2.508e-05 Validation loss = 0.004396, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3890 Training loss: 2.675e-05 Validation loss = 0.005596, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3891 Training loss: 3.319e-05 Validation loss = 0.005052, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3892 Training loss: 2.145e-05 Validation loss = 0.006092, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3893 Training loss: 2.553e-05 Validation loss = 0.00451, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3894 Training loss: 2.89e-05 Validation loss = 0.005046, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3895 Training loss: 2.725e-05 Validation loss = 0.003764, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3896 Training loss: 3.294e-05 Validation loss = 0.005066, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3897 Training loss: 2.732e-05 Validation loss = 0.00401, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3898 Training loss: 2.045e-05 Validation loss = 0.004948, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3899 Training loss: 2.975e-05 Validation loss = 0.004434, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3900 Training loss: 2.584e-05 Validation loss = 0.006128, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3901 Training loss: 2.773e-05 Validation loss = 0.004288, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3902 Training loss: 3.524e-05 Validation loss = 0.004701, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3903 Training loss: 2.369e-05 Validation loss = 0.004335, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3904 Training loss: 2.501e-05 Validation loss = 0.004623, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3905 Training loss: 2.777e-05 Validation loss = 0.005744, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3906 Training loss: 2.619e-05 Validation loss = 0.007679, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3907 Training loss: 3.162e-05 Validation loss = 0.003925, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3908 Training loss: 2.443e-05 Validation loss = 0.005987, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3909 Training loss: 2.764e-05 Validation loss = 0.005621, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3910 Training loss: 2.552e-05 Validation loss = 0.003723, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3911 Training loss: 3.403e-05 Validation loss = 0.004795, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3912 Training loss: 2.32e-05 Validation loss = 0.005027, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3913 Training loss: 2.748e-05 Validation loss = 0.005406, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3914 Training loss: 2.809e-05 Validation loss = 0.005123, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3915 Training loss: 2.651e-05 Validation loss = 0.006846, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3916 Training loss: 3.445e-05 Validation loss = 0.004133, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3917 Training loss: 2.044e-05 Validation loss = 0.01092, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3918 Training loss: 3.071e-05 Validation loss = 0.003943, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3919 Training loss: 2.393e-05 Validation loss = 0.007815, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3920 Training loss: 3.426e-05 Validation loss = 0.006155, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3921 Training loss: 2.687e-05 Validation loss = 0.003895, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3922 Training loss: 2.934e-05 Validation loss = 0.004028, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3923 Training loss: 2.027e-05 Validation loss = 0.00726, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3924 Training loss: 2.606e-05 Validation loss = 0.004565, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3925 Training loss: 3.009e-05 Validation loss = 0.004512, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3926 Training loss: 2.674e-05 Validation loss = 0.006691, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3927 Training loss: 2.7e-05 Validation loss = 0.004381, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3928 Training loss: 2.541e-05 Validation loss = 0.004313, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3929 Training loss: 3.079e-05 Validation loss = 0.004555, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3930 Training loss: 2.834e-05 Validation loss = 0.004134, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3931 Training loss: 2.476e-05 Validation loss = 0.004504, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3932 Training loss: 2.801e-05 Validation loss = 0.00732, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3933 Training loss: 2.298e-05 Validation loss = 0.003857, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3934 Training loss: 3.203e-05 Validation loss = 0.004031, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3935 Training loss: 2.692e-05 Validation loss = 0.005317, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3936 Training loss: 2.554e-05 Validation loss = 0.005119, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.003505410860761592\n",
      "Epoch: 3937 Training loss: 2.816e-05 Validation loss = 0.003505, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3938 Training loss: 2.194e-05 Validation loss = 0.004981, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3939 Training loss: 3.041e-05 Validation loss = 0.003681, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3940 Training loss: 3.057e-05 Validation loss = 0.005987, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3941 Training loss: 2.612e-05 Validation loss = 0.003604, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3942 Training loss: 2.843e-05 Validation loss = 0.003729, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3943 Training loss: 2.768e-05 Validation loss = 0.005995, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3944 Training loss: 2.636e-05 Validation loss = 0.00448, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3945 Training loss: 2.148e-05 Validation loss = 0.004397, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3946 Training loss: 3.104e-05 Validation loss = 0.004067, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3947 Training loss: 3.001e-05 Validation loss = 0.003783, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3948 Training loss: 2.668e-05 Validation loss = 0.003658, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3949 Training loss: 2.509e-05 Validation loss = 0.005326, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3950 Training loss: 2.569e-05 Validation loss = 0.008958, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3951 Training loss: 2.918e-05 Validation loss = 0.004505, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3952 Training loss: 2.386e-05 Validation loss = 0.006154, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3953 Training loss: 3.091e-05 Validation loss = 0.003971, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3954 Training loss: 2.71e-05 Validation loss = 0.004838, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3955 Training loss: 2.305e-05 Validation loss = 0.004748, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3956 Training loss: 2.944e-05 Validation loss = 0.005549, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3957 Training loss: 2.588e-05 Validation loss = 0.005499, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "########## new best ########### 0.003489124784692761\n",
      "Epoch: 3958 Training loss: 2.82e-05 Validation loss = 0.003489, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3959 Training loss: 2.301e-05 Validation loss = 0.003739, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3960 Training loss: 2.899e-05 Validation loss = 0.005939, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3961 Training loss: 3.231e-05 Validation loss = 0.005792, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3962 Training loss: 1.929e-05 Validation loss = 0.005353, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3963 Training loss: 2.992e-05 Validation loss = 0.003877, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3964 Training loss: 2.573e-05 Validation loss = 0.004972, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3965 Training loss: 3.452e-05 Validation loss = 0.004741, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3966 Training loss: 2.113e-05 Validation loss = 0.00427, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3967 Training loss: 2.622e-05 Validation loss = 0.004518, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3968 Training loss: 2.603e-05 Validation loss = 0.005683, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3969 Training loss: 3.265e-05 Validation loss = 0.004462, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3970 Training loss: 2.254e-05 Validation loss = 0.003785, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3971 Training loss: 2.601e-05 Validation loss = 0.004765, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3972 Training loss: 2.6e-05 Validation loss = 0.003622, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3973 Training loss: 2.743e-05 Validation loss = 0.003719, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3974 Training loss: 2.64e-05 Validation loss = 0.004215, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3975 Training loss: 3.051e-05 Validation loss = 0.003894, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3976 Training loss: 2.411e-05 Validation loss = 0.003839, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3977 Training loss: 2.548e-05 Validation loss = 0.00421, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3978 Training loss: 3.43e-05 Validation loss = 0.00372, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3979 Training loss: 2.464e-05 Validation loss = 0.005485, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3980 Training loss: 2.282e-05 Validation loss = 0.007593, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3981 Training loss: 3.148e-05 Validation loss = 0.006098, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3982 Training loss: 2.233e-05 Validation loss = 0.005008, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3983 Training loss: 3.072e-05 Validation loss = 0.0036, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3984 Training loss: 2.519e-05 Validation loss = 0.004029, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3985 Training loss: 3.3e-05 Validation loss = 0.003753, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3986 Training loss: 2.093e-05 Validation loss = 0.005506, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3987 Training loss: 2.711e-05 Validation loss = 0.006075, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3988 Training loss: 2.409e-05 Validation loss =  0.005, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3989 Training loss: 2.942e-05 Validation loss = 0.004373, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3990 Training loss: 3.022e-05 Validation loss = 0.003569, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3991 Training loss: 2.787e-05 Validation loss = 0.006579, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3992 Training loss: 2.23e-05 Validation loss = 0.00409, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3993 Training loss: 2.95e-05 Validation loss = 0.004129, time Loss: 31.9%, back: 30.7%, val: 34.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3994 Training loss: 2.352e-05 Validation loss = 0.003946, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3995 Training loss: 2.857e-05 Validation loss = 0.004553, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3996 Training loss: 2.748e-05 Validation loss = 0.00387, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3997 Training loss: 2.311e-05 Validation loss = 0.005939, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3998 Training loss: 2.74e-05 Validation loss = 0.006363, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 3999 Training loss: 2.96e-05 Validation loss = 0.004303, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "Epoch: 4000 Training loss: 2.42e-05 Validation loss = 0.005049, time Loss: 31.9%, back: 30.7%, val: 34.9%\n",
      "N_training_samples=99871, batch_size=99871, N_batch_updates_per_epoch=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d9b38e799e445fb194977bfaf501c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1 Training loss: 1.41e-05 Validation loss = 0.00373, time Loss: 7.9%, back: 16.8%, val: 75.0%\n",
      "Epoch:    2 Training loss: 1.478e-05 Validation loss = 0.003933, time Loss: 7.6%, back: 16.8%, val: 75.3%\n",
      "Epoch:    3 Training loss: 1.558e-05 Validation loss = 0.003976, time Loss: 7.3%, back: 16.5%, val: 75.9%\n",
      "Epoch:    4 Training loss: 1.623e-05 Validation loss = 0.004135, time Loss: 7.2%, back: 16.2%, val: 76.3%\n",
      "Epoch:    5 Training loss: 1.69e-05 Validation loss = 0.004158, time Loss: 7.2%, back: 16.9%, val: 75.6%\n",
      "Epoch:    6 Training loss: 1.74e-05 Validation loss = 0.004223, time Loss: 7.0%, back: 17.0%, val: 75.7%\n",
      "Epoch:    7 Training loss: 1.699e-05 Validation loss = 0.004119, time Loss: 7.1%, back: 17.2%, val: 75.4%\n",
      "Epoch:    8 Training loss: 1.705e-05 Validation loss = 0.004158, time Loss: 7.1%, back: 17.1%, val: 75.5%\n",
      "Epoch:    9 Training loss: 1.652e-05 Validation loss = 0.004013, time Loss: 7.1%, back: 17.1%, val: 75.5%\n",
      "Epoch:   10 Training loss: 1.587e-05 Validation loss = 0.003937, time Loss: 7.1%, back: 17.0%, val: 75.7%\n",
      "Epoch:   11 Training loss: 1.52e-05 Validation loss = 0.003798, time Loss: 7.0%, back: 16.8%, val: 75.9%\n",
      "Epoch:   12 Training loss: 1.446e-05 Validation loss = 0.003727, time Loss: 7.1%, back: 16.8%, val: 75.8%\n",
      "########## new best ########### 0.003619600428481052\n",
      "Epoch:   13 Training loss: 1.372e-05 Validation loss = 0.00362, time Loss: 7.1%, back: 16.8%, val: 75.8%\n",
      "########## new best ########### 0.003570507181954897\n",
      "Epoch:   14 Training loss: 1.313e-05 Validation loss = 0.003571, time Loss: 7.2%, back: 16.8%, val: 75.8%\n",
      "########## new best ########### 0.0035535596458706304\n",
      "Epoch:   15 Training loss: 1.273e-05 Validation loss = 0.003554, time Loss: 7.2%, back: 16.8%, val: 75.7%\n",
      "########## new best ########### 0.0035508314054871126\n",
      "Epoch:   16 Training loss: 1.249e-05 Validation loss = 0.003551, time Loss: 7.2%, back: 16.8%, val: 75.7%\n",
      "Epoch:   17 Training loss: 1.268e-05 Validation loss = 0.003597, time Loss: 7.2%, back: 16.9%, val: 75.7%\n",
      "Epoch:   18 Training loss: 1.27e-05 Validation loss = 0.003626, time Loss: 7.2%, back: 16.9%, val: 75.6%\n",
      "Epoch:   19 Training loss: 1.3e-05 Validation loss = 0.0037, time Loss: 7.2%, back: 16.9%, val: 75.7%\n",
      "Epoch:   20 Training loss: 1.334e-05 Validation loss = 0.0037, time Loss: 7.2%, back: 17.0%, val: 75.5%\n",
      "Epoch:   21 Training loss: 1.346e-05 Validation loss = 0.00375, time Loss: 7.2%, back: 17.0%, val: 75.6%\n",
      "Epoch:   22 Training loss: 1.364e-05 Validation loss = 0.003728, time Loss: 7.1%, back: 17.0%, val: 75.6%\n",
      "Epoch:   23 Training loss: 1.367e-05 Validation loss = 0.003756, time Loss: 7.1%, back: 17.1%, val: 75.5%\n",
      "Epoch:   24 Training loss: 1.36e-05 Validation loss = 0.003706, time Loss: 7.1%, back: 17.1%, val: 75.5%\n",
      "Epoch:   25 Training loss: 1.351e-05 Validation loss = 0.003727, time Loss: 7.1%, back: 17.0%, val: 75.6%\n",
      "Epoch:   26 Training loss: 1.339e-05 Validation loss = 0.003681, time Loss: 7.1%, back: 17.0%, val: 75.6%\n",
      "Epoch:   27 Training loss: 1.326e-05 Validation loss = 0.003686, time Loss: 7.1%, back: 17.0%, val: 75.7%\n",
      "Epoch:   28 Training loss: 1.315e-05 Validation loss = 0.003641, time Loss: 7.1%, back: 17.0%, val: 75.7%\n",
      "Epoch:   29 Training loss: 1.302e-05 Validation loss = 0.003659, time Loss: 7.1%, back: 16.9%, val: 75.7%\n",
      "Epoch:   30 Training loss: 1.297e-05 Validation loss = 0.003616, time Loss: 7.1%, back: 16.9%, val: 75.8%\n",
      "Epoch:   31 Training loss: 1.285e-05 Validation loss = 0.003628, time Loss: 7.1%, back: 16.9%, val: 75.8%\n",
      "Epoch:   32 Training loss: 1.276e-05 Validation loss = 0.003597, time Loss: 7.1%, back: 16.9%, val: 75.8%\n",
      "Epoch:   33 Training loss: 1.27e-05 Validation loss = 0.003599, time Loss: 7.0%, back: 16.8%, val: 75.9%\n",
      "Epoch:   34 Training loss: 1.259e-05 Validation loss = 0.003572, time Loss: 7.0%, back: 16.8%, val: 75.9%\n",
      "Epoch:   35 Training loss: 1.254e-05 Validation loss = 0.003579, time Loss: 7.0%, back: 16.8%, val: 75.9%\n",
      "Epoch:   36 Training loss: 1.246e-05 Validation loss = 0.003553, time Loss: 7.1%, back: 16.8%, val: 75.8%\n",
      "Epoch:   37 Training loss: 1.24e-05 Validation loss = 0.003562, time Loss: 7.1%, back: 16.8%, val: 75.9%\n",
      "########## new best ########### 0.003545629433121616\n",
      "Epoch:   38 Training loss: 1.237e-05 Validation loss = 0.003546, time Loss: 7.1%, back: 16.8%, val: 75.9%\n",
      "Epoch:   39 Training loss: 1.234e-05 Validation loss = 0.003554, time Loss: 7.1%, back: 16.8%, val: 75.9%\n",
      "########## new best ########### 0.003544237922933554\n",
      "Epoch:   40 Training loss: 1.232e-05 Validation loss = 0.003544, time Loss: 7.1%, back: 16.8%, val: 75.9%\n",
      "Epoch:   41 Training loss: 1.232e-05 Validation loss = 0.003554, time Loss: 7.1%, back: 16.8%, val: 75.9%\n",
      "Epoch:   42 Training loss: 1.232e-05 Validation loss = 0.003544, time Loss: 7.1%, back: 16.7%, val: 75.9%\n",
      "Epoch:   43 Training loss: 1.231e-05 Validation loss = 0.003554, time Loss: 7.0%, back: 16.7%, val: 76.0%\n",
      "Epoch:   44 Training loss: 1.232e-05 Validation loss = 0.003545, time Loss: 7.1%, back: 16.7%, val: 76.0%\n",
      "Epoch:   45 Training loss: 1.233e-05 Validation loss = 0.003557, time Loss: 7.0%, back: 16.7%, val: 76.0%\n",
      "Epoch:   46 Training loss: 1.233e-05 Validation loss = 0.003549, time Loss: 7.0%, back: 16.7%, val: 76.0%\n",
      "Epoch:   47 Training loss: 1.234e-05 Validation loss = 0.003563, time Loss: 7.0%, back: 16.7%, val: 76.1%\n",
      "Epoch:   48 Training loss: 1.236e-05 Validation loss = 0.003556, time Loss: 7.0%, back: 16.6%, val: 76.1%\n",
      "Epoch:   49 Training loss: 1.238e-05 Validation loss = 0.003575, time Loss: 7.0%, back: 16.6%, val: 76.1%\n",
      "Epoch:   50 Training loss: 1.242e-05 Validation loss = 0.003569, time Loss: 7.0%, back: 16.6%, val: 76.1%\n",
      "Epoch:   51 Training loss: 1.247e-05 Validation loss = 0.0036, time Loss: 7.0%, back: 16.6%, val: 76.2%\n",
      "Epoch:   52 Training loss: 1.255e-05 Validation loss = 0.003597, time Loss: 7.0%, back: 16.6%, val: 76.2%\n",
      "Epoch:   53 Training loss: 1.266e-05 Validation loss = 0.003646, time Loss: 7.0%, back: 16.6%, val: 76.1%\n",
      "Epoch:   54 Training loss: 1.282e-05 Validation loss = 0.003655, time Loss: 7.0%, back: 16.6%, val: 76.1%\n",
      "Epoch:   55 Training loss: 1.304e-05 Validation loss = 0.003735, time Loss: 7.0%, back: 16.6%, val: 76.1%\n",
      "Epoch:   56 Training loss: 1.334e-05 Validation loss = 0.00377, time Loss: 7.0%, back: 16.6%, val: 76.1%\n",
      "Epoch:   57 Training loss: 1.378e-05 Validation loss = 0.003905, time Loss: 7.0%, back: 16.6%, val: 76.1%\n",
      "Epoch:   58 Training loss: 1.44e-05 Validation loss = 0.003995, time Loss: 7.0%, back: 16.6%, val: 76.2%\n",
      "Epoch:   59 Training loss: 1.53e-05 Validation loss = 0.004232, time Loss: 7.0%, back: 16.6%, val: 76.2%\n",
      "Epoch:   60 Training loss: 1.659e-05 Validation loss = 0.00443, time Loss: 7.0%, back: 16.6%, val: 76.2%\n",
      "Epoch:   61 Training loss: 1.848e-05 Validation loss = 0.004842, time Loss: 6.9%, back: 16.6%, val: 76.2%\n",
      "Epoch:   62 Training loss: 2.116e-05 Validation loss = 0.005237, time Loss: 6.9%, back: 16.6%, val: 76.2%\n",
      "Epoch:   63 Training loss: 2.511e-05 Validation loss = 0.005903, time Loss: 6.9%, back: 16.6%, val: 76.2%\n",
      "Epoch:   64 Training loss: 3.062e-05 Validation loss = 0.006577, time Loss: 6.9%, back: 16.6%, val: 76.2%\n",
      "Epoch:   65 Training loss: 3.858e-05 Validation loss = 0.00754, time Loss: 6.9%, back: 16.6%, val: 76.2%\n",
      "Epoch:   66 Training loss: 4.905e-05 Validation loss = 0.008498, time Loss: 6.9%, back: 16.6%, val: 76.2%\n",
      "Epoch:   67 Training loss: 6.321e-05 Validation loss = 0.009637, time Loss: 6.9%, back: 16.6%, val: 76.2%\n",
      "Epoch:   68 Training loss: 7.926e-05 Validation loss = 0.0106, time Loss: 6.9%, back: 16.6%, val: 76.2%\n",
      "Epoch:   69 Training loss: 9.732e-05 Validation loss = 0.01138, time Loss: 6.9%, back: 16.6%, val: 76.3%\n",
      "Epoch:   70 Training loss: 0.0001104 Validation loss = 0.01161, time Loss: 6.9%, back: 16.6%, val: 76.3%\n",
      "Epoch:   71 Training loss: 0.000117 Validation loss = 0.01125, time Loss: 6.9%, back: 16.6%, val: 76.3%\n",
      "Epoch:   72 Training loss: 0.0001086 Validation loss = 0.01012, time Loss: 6.9%, back: 16.6%, val: 76.3%\n",
      "Epoch:   73 Training loss: 9.019e-05 Validation loss = 0.008751, time Loss: 6.9%, back: 16.6%, val: 76.3%\n",
      "Epoch:   74 Training loss: 6.677e-05 Validation loss = 0.007468, time Loss: 6.9%, back: 16.6%, val: 76.3%\n",
      "Epoch:   75 Training loss:   5e-05 Validation loss = 0.007345, time Loss: 6.9%, back: 16.6%, val: 76.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   76 Training loss: 4.675e-05 Validation loss = 0.008006, time Loss: 6.9%, back: 16.6%, val: 76.3%\n",
      "Epoch:   77 Training loss: 5.491e-05 Validation loss = 0.009055, time Loss: 6.9%, back: 16.6%, val: 76.3%\n",
      "Epoch:   78 Training loss: 6.927e-05 Validation loss = 0.009442, time Loss: 6.9%, back: 16.6%, val: 76.3%\n",
      "Epoch:   79 Training loss: 7.488e-05 Validation loss = 0.00919, time Loss: 6.9%, back: 16.6%, val: 76.3%\n",
      "Epoch:   80 Training loss: 7.143e-05 Validation loss = 0.007926, time Loss: 6.9%, back: 16.6%, val: 76.3%\n",
      "Epoch:   81 Training loss: 5.371e-05 Validation loss = 0.006482, time Loss: 6.9%, back: 16.5%, val: 76.3%\n",
      "Epoch:   82 Training loss: 3.709e-05 Validation loss = 0.005451, time Loss: 6.9%, back: 16.5%, val: 76.3%\n",
      "Epoch:   83 Training loss: 2.746e-05 Validation loss = 0.005577, time Loss: 6.9%, back: 16.5%, val: 76.3%\n",
      "Epoch:   84 Training loss: 2.841e-05 Validation loss = 0.006018, time Loss: 6.9%, back: 16.5%, val: 76.4%\n",
      "Epoch:   85 Training loss: 3.307e-05 Validation loss = 0.006186, time Loss: 6.9%, back: 16.5%, val: 76.4%\n",
      "Epoch:   86 Training loss: 3.38e-05 Validation loss = 0.005545, time Loss: 6.9%, back: 16.5%, val: 76.4%\n",
      "Epoch:   87 Training loss: 2.779e-05 Validation loss = 0.004569, time Loss: 6.9%, back: 16.5%, val: 76.4%\n",
      "Epoch:   88 Training loss: 1.901e-05 Validation loss = 0.003877, time Loss: 6.9%, back: 16.5%, val: 76.4%\n",
      "Epoch:   89 Training loss: 1.435e-05 Validation loss = 0.004177, time Loss: 6.8%, back: 16.5%, val: 76.4%\n",
      "Epoch:   90 Training loss: 1.669e-05 Validation loss = 0.005031, time Loss: 6.8%, back: 16.5%, val: 76.4%\n",
      "Epoch:   91 Training loss: 2.322e-05 Validation loss = 0.005509, time Loss: 6.8%, back: 16.5%, val: 76.4%\n",
      "Epoch:   92 Training loss: 2.813e-05 Validation loss = 0.005541, time Loss: 6.8%, back: 16.5%, val: 76.4%\n",
      "Epoch:   93 Training loss: 2.784e-05 Validation loss = 0.004955, time Loss: 6.8%, back: 16.5%, val: 76.4%\n",
      "Epoch:   94 Training loss: 2.306e-05 Validation loss = 0.004387, time Loss: 6.8%, back: 16.5%, val: 76.4%\n",
      "Epoch:   95 Training loss: 1.796e-05 Validation loss = 0.004079, time Loss: 6.8%, back: 16.5%, val: 76.5%\n",
      "Epoch:   96 Training loss: 1.578e-05 Validation loss = 0.004256, time Loss: 6.8%, back: 16.5%, val: 76.5%\n",
      "Epoch:   97 Training loss: 1.683e-05 Validation loss = 0.004501, time Loss: 6.8%, back: 16.5%, val: 76.5%\n",
      "Epoch:   98 Training loss: 1.855e-05 Validation loss = 0.0045, time Loss: 6.8%, back: 16.5%, val: 76.5%\n",
      "Epoch:   99 Training loss: 1.868e-05 Validation loss = 0.004228, time Loss: 6.8%, back: 16.4%, val: 76.5%\n",
      "Epoch:  100 Training loss: 1.669e-05 Validation loss = 0.003852, time Loss: 6.8%, back: 16.4%, val: 76.5%\n",
      "Epoch:  101 Training loss: 1.423e-05 Validation loss = 0.003663, time Loss: 6.8%, back: 16.4%, val: 76.5%\n",
      "Epoch:  102 Training loss: 1.308e-05 Validation loss = 0.003789, time Loss: 6.8%, back: 16.4%, val: 76.5%\n",
      "Epoch:  103 Training loss: 1.371e-05 Validation loss = 0.003979, time Loss: 6.8%, back: 16.5%, val: 76.5%\n",
      "Epoch:  104 Training loss: 1.508e-05 Validation loss = 0.004108, time Loss: 6.8%, back: 16.5%, val: 76.4%\n",
      "Epoch:  105 Training loss: 1.574e-05 Validation loss = 0.004009, time Loss: 6.8%, back: 16.5%, val: 76.5%\n",
      "Epoch:  106 Training loss: 1.521e-05 Validation loss = 0.00383, time Loss: 6.8%, back: 16.5%, val: 76.4%\n",
      "Epoch:  107 Training loss: 1.398e-05 Validation loss = 0.003698, time Loss: 6.8%, back: 16.5%, val: 76.4%\n",
      "Epoch:  108 Training loss: 1.315e-05 Validation loss = 0.00369, time Loss: 6.8%, back: 16.5%, val: 76.4%\n",
      "Epoch:  109 Training loss: 1.327e-05 Validation loss = 0.003847, time Loss: 6.8%, back: 16.5%, val: 76.4%\n",
      "Epoch:  110 Training loss: 1.406e-05 Validation loss = 0.003911, time Loss: 6.8%, back: 16.5%, val: 76.4%\n",
      "Epoch:  111 Training loss: 1.479e-05 Validation loss = 0.003974, time Loss: 6.8%, back: 16.6%, val: 76.3%\n",
      "Epoch:  112 Training loss: 1.487e-05 Validation loss = 0.003845, time Loss: 6.8%, back: 16.6%, val: 76.4%\n",
      "Epoch:  113 Training loss: 1.434e-05 Validation loss = 0.003779, time Loss: 6.8%, back: 16.6%, val: 76.4%\n",
      "Epoch:  114 Training loss: 1.362e-05 Validation loss = 0.003681, time Loss: 6.8%, back: 16.6%, val: 76.4%\n",
      "Epoch:  115 Training loss: 1.32e-05 Validation loss = 0.003706, time Loss: 6.8%, back: 16.5%, val: 76.4%\n",
      "Epoch:  116 Training loss: 1.321e-05 Validation loss = 0.003731, time Loss: 6.8%, back: 16.5%, val: 76.4%\n",
      "Epoch:  117 Training loss: 1.342e-05 Validation loss = 0.003752, time Loss: 6.8%, back: 16.5%, val: 76.4%\n",
      "Epoch:  118 Training loss: 1.351e-05 Validation loss = 0.003722, time Loss: 6.8%, back: 16.5%, val: 76.5%\n",
      "Epoch:  119 Training loss: 1.333e-05 Validation loss = 0.003662, time Loss: 6.8%, back: 16.5%, val: 76.5%\n",
      "Epoch:  120 Training loss: 1.296e-05 Validation loss = 0.003604, time Loss: 6.8%, back: 16.5%, val: 76.5%\n",
      "Epoch:  121 Training loss: 1.264e-05 Validation loss = 0.003592, time Loss: 6.8%, back: 16.4%, val: 76.5%\n",
      "Epoch:  122 Training loss: 1.253e-05 Validation loss = 0.003594, time Loss: 6.8%, back: 16.4%, val: 76.5%\n",
      "Epoch:  123 Training loss: 1.263e-05 Validation loss = 0.003636, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  124 Training loss: 1.28e-05 Validation loss = 0.003631, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  125 Training loss: 1.288e-05 Validation loss = 0.003637, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  126 Training loss: 1.281e-05 Validation loss = 0.003595, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  127 Training loss: 1.264e-05 Validation loss = 0.003582, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  128 Training loss: 1.248e-05 Validation loss = 0.003565, time Loss: 6.7%, back: 16.4%, val: 76.6%\n",
      "Epoch:  129 Training loss: 1.242e-05 Validation loss = 0.003576, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  130 Training loss: 1.245e-05 Validation loss = 0.003589, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  131 Training loss: 1.253e-05 Validation loss = 0.003599, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  132 Training loss: 1.259e-05 Validation loss = 0.003599, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  133 Training loss: 1.26e-05 Validation loss = 0.003597, time Loss: 6.7%, back: 16.3%, val: 76.8%\n",
      "Epoch:  134 Training loss: 1.257e-05 Validation loss = 0.003587, time Loss: 6.7%, back: 16.3%, val: 76.8%\n",
      "Epoch:  135 Training loss: 1.255e-05 Validation loss = 0.003604, time Loss: 6.7%, back: 16.3%, val: 76.8%\n",
      "Epoch:  136 Training loss: 1.259e-05 Validation loss = 0.003605, time Loss: 6.7%, back: 16.3%, val: 76.8%\n",
      "Epoch:  137 Training loss: 1.27e-05 Validation loss = 0.003654, time Loss: 6.7%, back: 16.3%, val: 76.8%\n",
      "Epoch:  138 Training loss: 1.288e-05 Validation loss = 0.003662, time Loss: 6.7%, back: 16.3%, val: 76.8%\n",
      "Epoch:  139 Training loss: 1.309e-05 Validation loss = 0.003731, time Loss: 6.7%, back: 16.3%, val: 76.8%\n",
      "Epoch:  140 Training loss: 1.335e-05 Validation loss = 0.00375, time Loss: 6.7%, back: 16.3%, val: 76.8%\n",
      "Epoch:  141 Training loss: 1.367e-05 Validation loss = 0.003852, time Loss: 6.7%, back: 16.3%, val: 76.8%\n",
      "Epoch:  142 Training loss: 1.411e-05 Validation loss = 0.003913, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  143 Training loss: 1.474e-05 Validation loss = 0.004089, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  144 Training loss: 1.567e-05 Validation loss = 0.004235, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  145 Training loss: 1.699e-05 Validation loss = 0.004539, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  146 Training loss: 1.892e-05 Validation loss = 0.004825, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  147 Training loss: 2.156e-05 Validation loss = 0.005331, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  148 Training loss: 2.547e-05 Validation loss = 0.00583, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  149 Training loss: 3.074e-05 Validation loss = 0.006632, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  150 Training loss: 3.859e-05 Validation loss = 0.007407, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  151 Training loss: 4.866e-05 Validation loss = 0.008562, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  152 Training loss: 6.336e-05 Validation loss = 0.00957, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  153 Training loss: 8.013e-05 Validation loss = 0.01095, time Loss: 6.7%, back: 16.3%, val: 76.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  154 Training loss: 0.0001028 Validation loss = 0.01185, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  155 Training loss: 0.000122 Validation loss = 0.01292, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  156 Training loss: 0.0001425 Validation loss = 0.01289, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  157 Training loss: 0.0001441 Validation loss = 0.01265, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  158 Training loss: 0.0001367 Validation loss = 0.01093, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  159 Training loss: 0.0001044 Validation loss = 0.008929, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  160 Training loss: 6.91e-05 Validation loss = 0.006308, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  161 Training loss: 3.61e-05 Validation loss = 0.004652, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  162 Training loss: 2.04e-05 Validation loss = 0.004961, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  163 Training loss: 2.263e-05 Validation loss = 0.006315, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  164 Training loss: 3.607e-05 Validation loss = 0.007551, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  165 Training loss: 4.981e-05 Validation loss = 0.007824, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  166 Training loss: 5.418e-05 Validation loss = 0.007423, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  167 Training loss: 4.796e-05 Validation loss = 0.006125, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  168 Training loss: 3.395e-05 Validation loss = 0.00489, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  169 Training loss: 2.179e-05 Validation loss = 0.004213, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  170 Training loss: 1.688e-05 Validation loss = 0.004618, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  171 Training loss: 2.026e-05 Validation loss = 0.005518, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  172 Training loss: 2.752e-05 Validation loss = 0.006003, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  173 Training loss: 3.292e-05 Validation loss = 0.006035, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  174 Training loss: 3.252e-05 Validation loss = 0.005377, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  175 Training loss: 2.66e-05 Validation loss = 0.004526, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  176 Training loss: 1.88e-05 Validation loss = 0.003712, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  177 Training loss: 1.339e-05 Validation loss = 0.0036, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  178 Training loss: 1.262e-05 Validation loss = 0.004092, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  179 Training loss: 1.567e-05 Validation loss = 0.004582, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  180 Training loss: 1.979e-05 Validation loss = 0.004932, time Loss: 6.7%, back: 16.4%, val: 76.6%\n",
      "Epoch:  181 Training loss: 2.217e-05 Validation loss = 0.0048, time Loss: 6.7%, back: 16.4%, val: 76.6%\n",
      "Epoch:  182 Training loss: 2.155e-05 Validation loss = 0.004481, time Loss: 6.7%, back: 16.4%, val: 76.6%\n",
      "Epoch:  183 Training loss: 1.861e-05 Validation loss = 0.003984, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  184 Training loss: 1.526e-05 Validation loss = 0.003703, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  185 Training loss: 1.321e-05 Validation loss = 0.003697, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  186 Training loss: 1.314e-05 Validation loss = 0.003878, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  187 Training loss: 1.444e-05 Validation loss = 0.00413, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  188 Training loss: 1.591e-05 Validation loss = 0.004163, time Loss: 6.7%, back: 16.4%, val: 76.6%\n",
      "Epoch:  189 Training loss: 1.647e-05 Validation loss = 0.00412, time Loss: 6.7%, back: 16.4%, val: 76.6%\n",
      "Epoch:  190 Training loss: 1.586e-05 Validation loss = 0.003875, time Loss: 6.7%, back: 16.4%, val: 76.6%\n",
      "Epoch:  191 Training loss: 1.447e-05 Validation loss = 0.003695, time Loss: 6.7%, back: 16.4%, val: 76.6%\n",
      "Epoch:  192 Training loss: 1.31e-05 Validation loss = 0.003566, time Loss: 6.7%, back: 16.4%, val: 76.6%\n",
      "Epoch:  193 Training loss: 1.24e-05 Validation loss = 0.003591, time Loss: 6.7%, back: 16.4%, val: 76.6%\n",
      "Epoch:  194 Training loss: 1.255e-05 Validation loss = 0.003719, time Loss: 6.7%, back: 16.4%, val: 76.6%\n",
      "Epoch:  195 Training loss: 1.325e-05 Validation loss = 0.003811, time Loss: 6.7%, back: 16.4%, val: 76.6%\n",
      "Epoch:  196 Training loss: 1.401e-05 Validation loss = 0.003898, time Loss: 6.7%, back: 16.4%, val: 76.6%\n",
      "Epoch:  197 Training loss: 1.438e-05 Validation loss = 0.00384, time Loss: 6.7%, back: 16.4%, val: 76.6%\n",
      "Epoch:  198 Training loss: 1.421e-05 Validation loss = 0.003783, time Loss: 6.7%, back: 16.4%, val: 76.6%\n",
      "Epoch:  199 Training loss: 1.365e-05 Validation loss = 0.003655, time Loss: 6.7%, back: 16.4%, val: 76.6%\n",
      "Epoch:  200 Training loss: 1.299e-05 Validation loss = 0.003594, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  201 Training loss: 1.253e-05 Validation loss = 0.003567, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  202 Training loss: 1.239e-05 Validation loss = 0.003585, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  203 Training loss: 1.254e-05 Validation loss = 0.003642, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  204 Training loss: 1.281e-05 Validation loss = 0.003655, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  205 Training loss: 1.302e-05 Validation loss = 0.003684, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  206 Training loss: 1.305e-05 Validation loss = 0.003638, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  207 Training loss: 1.29e-05 Validation loss = 0.003618, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  208 Training loss: 1.265e-05 Validation loss = 0.003562, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  209 Training loss: 1.24e-05 Validation loss = 0.003546, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "########## new best ########### 0.003538089183176087\n",
      "Epoch:  210 Training loss: 1.225e-05 Validation loss = 0.003538, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  211 Training loss: 1.221e-05 Validation loss = 0.003546, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  212 Training loss: 1.229e-05 Validation loss = 0.003575, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  213 Training loss: 1.241e-05 Validation loss = 0.003579, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  214 Training loss: 1.252e-05 Validation loss = 0.003602, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  215 Training loss: 1.257e-05 Validation loss = 0.003583, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  216 Training loss: 1.254e-05 Validation loss = 0.003586, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  217 Training loss: 1.247e-05 Validation loss = 0.003559, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  218 Training loss: 1.238e-05 Validation loss = 0.003556, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  219 Training loss: 1.231e-05 Validation loss = 0.003545, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  220 Training loss: 1.228e-05 Validation loss = 0.003548, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  221 Training loss: 1.228e-05 Validation loss = 0.003556, time Loss: 6.8%, back: 16.4%, val: 76.7%\n",
      "Epoch:  222 Training loss: 1.232e-05 Validation loss = 0.00356, time Loss: 6.8%, back: 16.4%, val: 76.7%\n",
      "Epoch:  223 Training loss: 1.237e-05 Validation loss = 0.003576, time Loss: 6.8%, back: 16.4%, val: 76.7%\n",
      "Epoch:  224 Training loss: 1.243e-05 Validation loss = 0.003573, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  225 Training loss: 1.246e-05 Validation loss = 0.003585, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  226 Training loss: 1.248e-05 Validation loss = 0.003578, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  227 Training loss: 1.249e-05 Validation loss = 0.003586, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  228 Training loss: 1.25e-05 Validation loss = 0.003584, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  229 Training loss: 1.252e-05 Validation loss = 0.003593, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  230 Training loss: 1.256e-05 Validation loss = 0.003604, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  231 Training loss: 1.264e-05 Validation loss = 0.00362, time Loss: 6.7%, back: 16.3%, val: 76.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  232 Training loss: 1.276e-05 Validation loss = 0.003648, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  233 Training loss: 1.292e-05 Validation loss = 0.003676, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  234 Training loss: 1.314e-05 Validation loss = 0.003728, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  235 Training loss: 1.343e-05 Validation loss = 0.003778, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  236 Training loss: 1.383e-05 Validation loss = 0.003865, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  237 Training loss: 1.437e-05 Validation loss = 0.00396, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  238 Training loss: 1.511e-05 Validation loss = 0.004109, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  239 Training loss: 1.611e-05 Validation loss = 0.004288, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  240 Training loss: 1.754e-05 Validation loss = 0.004541, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  241 Training loss: 1.946e-05 Validation loss = 0.004862, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  242 Training loss: 2.226e-05 Validation loss = 0.005275, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  243 Training loss: 2.593e-05 Validation loss = 0.005802, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  244 Training loss: 3.125e-05 Validation loss = 0.006418, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  245 Training loss: 3.79e-05 Validation loss = 0.007181, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  246 Training loss: 4.732e-05 Validation loss = 0.007958, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  247 Training loss: 5.776e-05 Validation loss = 0.008858, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  248 Training loss: 7.145e-05 Validation loss = 0.009549, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  249 Training loss: 8.275e-05 Validation loss = 0.0102, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  250 Training loss: 9.441e-05 Validation loss = 0.01026, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  251 Training loss: 9.547e-05 Validation loss =   0.01, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  252 Training loss: 9.075e-05 Validation loss = 0.008904, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  253 Training loss: 7.206e-05 Validation loss = 0.007388, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  254 Training loss: 4.999e-05 Validation loss = 0.005493, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  255 Training loss: 2.8e-05 Validation loss = 0.003927, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  256 Training loss: 1.486e-05 Validation loss = 0.003626, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  257 Training loss: 1.279e-05 Validation loss = 0.004551, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  258 Training loss: 1.96e-05 Validation loss = 0.005681, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  259 Training loss: 3.003e-05 Validation loss = 0.0064, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  260 Training loss: 3.773e-05 Validation loss = 0.006583, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  261 Training loss: 3.993e-05 Validation loss = 0.006128, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  262 Training loss: 3.464e-05 Validation loss = 0.005284, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  263 Training loss: 2.604e-05 Validation loss = 0.004295, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  264 Training loss: 1.748e-05 Validation loss = 0.003638, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  265 Training loss: 1.29e-05 Validation loss = 0.003684, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  266 Training loss: 1.317e-05 Validation loss = 0.004189, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  267 Training loss: 1.679e-05 Validation loss = 0.004718, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  268 Training loss: 2.104e-05 Validation loss = 0.004983, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  269 Training loss: 2.328e-05 Validation loss = 0.00492, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  270 Training loss: 2.276e-05 Validation loss = 0.004569, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  271 Training loss: 1.969e-05 Validation loss = 0.004092, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  272 Training loss: 1.604e-05 Validation loss = 0.003715, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  273 Training loss: 1.332e-05 Validation loss = 0.003578, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  274 Training loss: 1.252e-05 Validation loss = 0.003725, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  275 Training loss: 1.344e-05 Validation loss = 0.003961, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  276 Training loss: 1.513e-05 Validation loss = 0.004151, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  277 Training loss: 1.65e-05 Validation loss = 0.004195, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  278 Training loss: 1.681e-05 Validation loss = 0.004092, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  279 Training loss: 1.607e-05 Validation loss = 0.003904, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  280 Training loss: 1.466e-05 Validation loss = 0.003699, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  281 Training loss: 1.331e-05 Validation loss = 0.003591, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  282 Training loss: 1.253e-05 Validation loss = 0.003571, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  283 Training loss: 1.249e-05 Validation loss = 0.003657, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  284 Training loss: 1.298e-05 Validation loss = 0.003744, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  285 Training loss: 1.362e-05 Validation loss = 0.003814, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  286 Training loss: 1.406e-05 Validation loss = 0.003812, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  287 Training loss: 1.407e-05 Validation loss = 0.003761, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  288 Training loss: 1.371e-05 Validation loss = 0.003677, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  289 Training loss: 1.314e-05 Validation loss = 0.003595, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  290 Training loss: 1.262e-05 Validation loss = 0.003553, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  291 Training loss: 1.232e-05 Validation loss = 0.003541, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  292 Training loss: 1.229e-05 Validation loss = 0.003577, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  293 Training loss: 1.246e-05 Validation loss = 0.003609, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  294 Training loss: 1.272e-05 Validation loss = 0.003648, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  295 Training loss: 1.292e-05 Validation loss = 0.00365, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  296 Training loss: 1.299e-05 Validation loss = 0.003643, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  297 Training loss: 1.291e-05 Validation loss = 0.003608, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  298 Training loss: 1.271e-05 Validation loss = 0.003576, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  299 Training loss: 1.248e-05 Validation loss = 0.003545, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "########## new best ########### 0.0035286514374476413\n",
      "Epoch:  300 Training loss: 1.229e-05 Validation loss = 0.003529, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  301 Training loss: 1.219e-05 Validation loss = 0.003529, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  302 Training loss: 1.218e-05 Validation loss = 0.003537, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  303 Training loss: 1.225e-05 Validation loss = 0.003556, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  304 Training loss: 1.235e-05 Validation loss = 0.003566, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  305 Training loss: 1.243e-05 Validation loss = 0.003578, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  306 Training loss: 1.248e-05 Validation loss = 0.003573, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  307 Training loss: 1.248e-05 Validation loss = 0.00357, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  308 Training loss: 1.244e-05 Validation loss = 0.003556, time Loss: 6.8%, back: 16.4%, val: 76.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  309 Training loss: 1.237e-05 Validation loss = 0.003546, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  310 Training loss: 1.229e-05 Validation loss = 0.003536, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  311 Training loss: 1.223e-05 Validation loss = 0.003531, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  312 Training loss: 1.22e-05 Validation loss = 0.003531, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  313 Training loss: 1.219e-05 Validation loss = 0.003532, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  314 Training loss: 1.221e-05 Validation loss = 0.003542, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  315 Training loss: 1.225e-05 Validation loss = 0.003544, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  316 Training loss: 1.23e-05 Validation loss = 0.003561, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  317 Training loss: 1.236e-05 Validation loss = 0.003563, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  318 Training loss: 1.243e-05 Validation loss = 0.00359, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  319 Training loss: 1.252e-05 Validation loss = 0.003597, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  320 Training loss: 1.265e-05 Validation loss = 0.003649, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  321 Training loss: 1.286e-05 Validation loss = 0.00368, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  322 Training loss: 1.32e-05 Validation loss = 0.003794, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  323 Training loss: 1.375e-05 Validation loss = 0.003897, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  324 Training loss: 1.464e-05 Validation loss = 0.004149, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  325 Training loss: 1.61e-05 Validation loss = 0.004424, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  326 Training loss: 1.846e-05 Validation loss = 0.004958, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  327 Training loss: 2.232e-05 Validation loss = 0.005582, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  328 Training loss: 2.848e-05 Validation loss = 0.006609, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  329 Training loss: 3.851e-05 Validation loss = 0.007798, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  330 Training loss: 5.401e-05 Validation loss = 0.009511, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  331 Training loss: 7.824e-05 Validation loss = 0.01132, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  332 Training loss: 0.0001118 Validation loss = 0.01355, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  333 Training loss: 0.0001574 Validation loss = 0.01528, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  334 Training loss: 0.0002018 Validation loss = 0.01662, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  335 Training loss: 0.0002358 Validation loss = 0.01601, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  336 Training loss: 0.0002211 Validation loss = 0.01397, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  337 Training loss: 0.000166 Validation loss = 0.009774, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  338 Training loss: 8.338e-05 Validation loss = 0.00606, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  339 Training loss: 3.272e-05 Validation loss = 0.006166, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  340 Training loss: 3.478e-05 Validation loss = 0.008924, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  341 Training loss: 7.183e-05 Validation loss = 0.01061, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  342 Training loss: 9.889e-05 Validation loss = 0.00999, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  343 Training loss: 8.782e-05 Validation loss = 0.007375, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  344 Training loss: 4.799e-05 Validation loss = 0.004384, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  345 Training loss: 1.798e-05 Validation loss = 0.004895, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  346 Training loss: 2.261e-05 Validation loss = 0.007372, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  347 Training loss: 4.86e-05 Validation loss = 0.008442, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  348 Training loss: 6.481e-05 Validation loss = 0.007704, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  349 Training loss: 5.305e-05 Validation loss = 0.005476, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  350 Training loss: 2.802e-05 Validation loss = 0.003885, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  351 Training loss: 1.447e-05 Validation loss = 0.004886, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  352 Training loss: 2.167e-05 Validation loss = 0.006231, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  353 Training loss: 3.543e-05 Validation loss = 0.00653, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  354 Training loss: 3.784e-05 Validation loss = 0.005367, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  355 Training loss: 2.683e-05 Validation loss = 0.00403, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  356 Training loss: 1.53e-05 Validation loss = 0.00396, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  357 Training loss: 1.478e-05 Validation loss = 0.004967, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  358 Training loss: 2.282e-05 Validation loss = 0.005654, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  359 Training loss: 2.845e-05 Validation loss = 0.005211, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  360 Training loss: 2.501e-05 Validation loss = 0.004293, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  361 Training loss: 1.708e-05 Validation loss = 0.00366, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  362 Training loss: 1.299e-05 Validation loss = 0.004038, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  363 Training loss: 1.559e-05 Validation loss = 0.004675, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  364 Training loss: 2.013e-05 Validation loss = 0.004729, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  365 Training loss: 2.084e-05 Validation loss = 0.0043, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  366 Training loss: 1.722e-05 Validation loss = 0.003742, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  367 Training loss: 1.345e-05 Validation loss = 0.003682, time Loss: 6.8%, back: 16.3%, val: 76.7%\n",
      "Epoch:  368 Training loss: 1.316e-05 Validation loss = 0.004069, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  369 Training loss: 1.557e-05 Validation loss = 0.00425, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  370 Training loss: 1.724e-05 Validation loss = 0.004148, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  371 Training loss: 1.617e-05 Validation loss = 0.003755, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  372 Training loss: 1.367e-05 Validation loss = 0.003553, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  373 Training loss: 1.23e-05 Validation loss = 0.003688, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  374 Training loss: 1.306e-05 Validation loss = 0.003887, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  375 Training loss: 1.46e-05 Validation loss = 0.004001, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  376 Training loss: 1.507e-05 Validation loss = 0.003816, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  377 Training loss: 1.412e-05 Validation loss = 0.003658, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  378 Training loss: 1.288e-05 Validation loss = 0.003595, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  379 Training loss: 1.257e-05 Validation loss = 0.0037, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  380 Training loss: 1.323e-05 Validation loss = 0.003814, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  381 Training loss: 1.391e-05 Validation loss = 0.003787, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  382 Training loss: 1.382e-05 Validation loss = 0.003686, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  383 Training loss: 1.31e-05 Validation loss = 0.003578, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  384 Training loss: 1.246e-05 Validation loss = 0.003565, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  385 Training loss: 1.242e-05 Validation loss = 0.00365, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  386 Training loss: 1.283e-05 Validation loss = 0.003675, time Loss: 6.7%, back: 16.3%, val: 76.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  387 Training loss: 1.315e-05 Validation loss = 0.003683, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  388 Training loss: 1.304e-05 Validation loss = 0.0036, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  389 Training loss: 1.265e-05 Validation loss = 0.003566, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  390 Training loss: 1.237e-05 Validation loss = 0.003569, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  391 Training loss: 1.241e-05 Validation loss = 0.003599, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  392 Training loss: 1.264e-05 Validation loss = 0.003635, time Loss: 6.7%, back: 16.3%, val: 76.7%\n",
      "Epoch:  393 Training loss: 1.28e-05 Validation loss = 0.003612, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  394 Training loss: 1.272e-05 Validation loss = 0.003586, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  395 Training loss: 1.251e-05 Validation loss = 0.003561, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  396 Training loss: 1.236e-05 Validation loss = 0.00356, time Loss: 6.8%, back: 16.4%, val: 76.7%\n",
      "Epoch:  397 Training loss: 1.238e-05 Validation loss = 0.003595, time Loss: 6.8%, back: 16.4%, val: 76.7%\n",
      "Epoch:  398 Training loss: 1.252e-05 Validation loss = 0.003595, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  399 Training loss: 1.263e-05 Validation loss = 0.003615, time Loss: 6.7%, back: 16.4%, val: 76.7%\n",
      "Epoch:  400 Training loss: 1.263e-05 Validation loss = 0.003586, time Loss: 6.8%, back: 16.4%, val: 76.7%\n",
      "Epoch:  401 Training loss: 1.256e-05 Validation loss = 0.003596, time Loss: 6.8%, back: 16.4%, val: 76.7%\n",
      "Epoch:  402 Training loss: 1.253e-05 Validation loss = 0.003597, time Loss: 6.8%, back: 16.4%, val: 76.7%\n",
      "Epoch:  403 Training loss: 1.261e-05 Validation loss = 0.003633, time Loss: 6.8%, back: 16.4%, val: 76.7%\n",
      "Epoch:  404 Training loss: 1.279e-05 Validation loss = 0.003659, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  405 Training loss: 1.3e-05 Validation loss = 0.003701, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  406 Training loss: 1.321e-05 Validation loss = 0.003727, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  407 Training loss: 1.345e-05 Validation loss = 0.003798, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  408 Training loss: 1.38e-05 Validation loss = 0.003856, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  409 Training loss: 1.434e-05 Validation loss = 0.004004, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  410 Training loss: 1.514e-05 Validation loss = 0.004126, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  411 Training loss: 1.626e-05 Validation loss = 0.004376, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  412 Training loss: 1.779e-05 Validation loss = 0.004597, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  413 Training loss: 1.987e-05 Validation loss = 0.004993, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  414 Training loss: 2.277e-05 Validation loss = 0.005379, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  415 Training loss: 2.669e-05 Validation loss = 0.005973, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  416 Training loss: 3.207e-05 Validation loss = 0.006543, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  417 Training loss: 3.885e-05 Validation loss = 0.007302, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  418 Training loss: 4.736e-05 Validation loss = 0.007932, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  419 Training loss: 5.65e-05 Validation loss = 0.00863, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  420 Training loss: 6.577e-05 Validation loss = 0.008954, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  421 Training loss: 7.174e-05 Validation loss = 0.009103, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  422 Training loss: 7.323e-05 Validation loss = 0.008598, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  423 Training loss: 6.651e-05 Validation loss = 0.007781, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  424 Training loss: 5.402e-05 Validation loss = 0.006413, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  425 Training loss: 3.779e-05 Validation loss = 0.005147, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  426 Training loss: 2.436e-05 Validation loss = 0.004272, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  427 Training loss: 1.721e-05 Validation loss = 0.004349, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  428 Training loss: 1.76e-05 Validation loss = 0.005059, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  429 Training loss: 2.296e-05 Validation loss = 0.005705, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  430 Training loss: 2.944e-05 Validation loss = 0.006108, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  431 Training loss: 3.293e-05 Validation loss = 0.005923, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  432 Training loss: 3.18e-05 Validation loss = 0.005434, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  433 Training loss: 2.643e-05 Validation loss = 0.004575, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  434 Training loss: 1.967e-05 Validation loss = 0.003911, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  435 Training loss: 1.447e-05 Validation loss = 0.003601, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  436 Training loss: 1.267e-05 Validation loss = 0.003816, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  437 Training loss: 1.408e-05 Validation loss = 0.004278, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  438 Training loss: 1.705e-05 Validation loss = 0.004556, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  439 Training loss: 1.955e-05 Validation loss = 0.004701, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  440 Training loss: 2.022e-05 Validation loss = 0.004471, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  441 Training loss: 1.888e-05 Validation loss = 0.004196, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  442 Training loss: 1.64e-05 Validation loss = 0.003811, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  443 Training loss: 1.408e-05 Validation loss = 0.003652, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  444 Training loss: 1.289e-05 Validation loss = 0.003673, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  445 Training loss: 1.309e-05 Validation loss = 0.00383, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  446 Training loss: 1.422e-05 Validation loss = 0.004047, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  447 Training loss: 1.549e-05 Validation loss = 0.004104, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  448 Training loss: 1.617e-05 Validation loss = 0.004125, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  449 Training loss: 1.6e-05 Validation loss = 0.003957, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  450 Training loss: 1.51e-05 Validation loss = 0.003826, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  451 Training loss: 1.398e-05 Validation loss = 0.003671, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  452 Training loss: 1.31e-05 Validation loss = 0.003628, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  453 Training loss: 1.277e-05 Validation loss = 0.003673, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  454 Training loss: 1.299e-05 Validation loss = 0.003732, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  455 Training loss: 1.351e-05 Validation loss = 0.003842, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  456 Training loss: 1.401e-05 Validation loss = 0.003842, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  457 Training loss: 1.426e-05 Validation loss = 0.003866, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  458 Training loss: 1.416e-05 Validation loss = 0.003776, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  459 Training loss: 1.382e-05 Validation loss = 0.003742, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  460 Training loss: 1.34e-05 Validation loss = 0.003671, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  461 Training loss: 1.31e-05 Validation loss = 0.003671, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  462 Training loss: 1.302e-05 Validation loss = 0.003693, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  463 Training loss: 1.319e-05 Validation loss = 0.003738, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  464 Training loss: 1.351e-05 Validation loss = 0.003808, time Loss: 6.8%, back: 16.4%, val: 76.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  465 Training loss: 1.391e-05 Validation loss = 0.003849, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  466 Training loss: 1.427e-05 Validation loss = 0.00391, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  467 Training loss: 1.46e-05 Validation loss = 0.003941, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  468 Training loss: 1.488e-05 Validation loss = 0.004002, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  469 Training loss: 1.526e-05 Validation loss = 0.004072, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  470 Training loss: 1.575e-05 Validation loss = 0.004181, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  471 Training loss: 1.655e-05 Validation loss = 0.004342, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  472 Training loss: 1.763e-05 Validation loss = 0.004539, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  473 Training loss: 1.925e-05 Validation loss = 0.004809, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  474 Training loss: 2.123e-05 Validation loss = 0.005117, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  475 Training loss: 2.404e-05 Validation loss = 0.005495, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  476 Training loss: 2.722e-05 Validation loss = 0.005926, time Loss: 6.8%, back: 16.4%, val: 76.7%\n",
      "Epoch:  477 Training loss: 3.17e-05 Validation loss = 0.00639, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  478 Training loss: 3.632e-05 Validation loss = 0.006925, time Loss: 6.8%, back: 16.4%, val: 76.7%\n",
      "Epoch:  479 Training loss: 4.272e-05 Validation loss = 0.007392, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  480 Training loss: 4.82e-05 Validation loss = 0.007916, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  481 Training loss: 5.539e-05 Validation loss = 0.008195, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  482 Training loss: 5.908e-05 Validation loss = 0.008457, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  483 Training loss: 6.311e-05 Validation loss = 0.008282, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  484 Training loss: 6.053e-05 Validation loss = 0.007994, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  485 Training loss: 5.667e-05 Validation loss = 0.007234, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  486 Training loss: 4.672e-05 Validation loss = 0.006375, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  487 Training loss: 3.662e-05 Validation loss = 0.005308, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  488 Training loss: 2.595e-05 Validation loss = 0.004424, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  489 Training loss: 1.831e-05 Validation loss = 0.003833, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  490 Training loss: 1.424e-05 Validation loss = 0.003811, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  491 Training loss: 1.382e-05 Validation loss = 0.004102, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  492 Training loss: 1.602e-05 Validation loss = 0.004571, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  493 Training loss: 1.933e-05 Validation loss = 0.004946, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  494 Training loss: 2.261e-05 Validation loss = 0.005159, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  495 Training loss: 2.44e-05 Validation loss = 0.005208, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  496 Training loss: 2.491e-05 Validation loss = 0.00504, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  497 Training loss: 2.349e-05 Validation loss = 0.0048, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  498 Training loss: 2.132e-05 Validation loss = 0.004436, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  499 Training loss: 1.859e-05 Validation loss = 0.004153, time Loss: 6.8%, back: 16.4%, val: 76.6%\n",
      "Epoch:  500 Training loss: 1.624e-05 Validation loss = 0.003875, time Loss: 6.8%, back: 16.4%, val: 76.6%\n"
     ]
    }
   ],
   "source": [
    "if True: #train model\n",
    "    sys = deepSI.fit_systems.System_encoder(nx=6,na=50,nb=50) #load the encoder system\n",
    "    sys.n_hidden_layers = 1\n",
    "    sys.n_nodes_per_layer = 15\n",
    "    sys.fit(train, epochs=4000, batch_size=1024, Loss_kwargs=dict(nf=80), sim_val=train[-20000:]) #can be stopped early with keyboard\n",
    "    #nf = number of steps in each encoder mutiple schooting section (T in the paper)\n",
    "    sys.save_system('./WH-data/nx6WH-encoder')\n",
    "    sim_val = train[-40_000:] #larger validation set\n",
    "    sys.bestfit = sys.apply_experiment(sim_val).NRMS(sim_val) #reset checkpoint\n",
    "    #use all the data for final local minima search\n",
    "    sys.fit(train, epochs=500 ,batch_size=10**10, Loss_kwargs=dict(nf=80), sim_val=sim_val) \n",
    "    sys.save_system('./WH-data/nx6WH-encoder-stepped')\n",
    "else: #load model from file\n",
    "    sys = deepSI.load_system('./WH-data/nx6WH-encoder-stepped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit/load BLA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: #train model\n",
    "    sys_BLA = deepSI.fit_systems.statespace_linear_system(nx=6) \n",
    "    sys_BLA.fit(train) #uses N4SID method, SS_f=20 (code used from https://github.com/CPCLAB-UNIPI/SIPPY/blob/master/sippy/OLSims_methods.py)\n",
    "    sys_BLA.save_system('./WH-data/nx6WH-BLA')\n",
    "else: #load model from file\n",
    "    sys_BLA = deepSI.load_system('./WH-data/nx6WH-BLA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate test RMS/NRMS simulation\n",
    "\n",
    " * NRMS = RMS/$\\sigma_y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoder_sim = sys.apply_experiment(test) #simulation\n",
    "train_encoder_sim = sys.apply_experiment(train) #simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set result:\n",
      "   NRMS test  encoder 0.09870% \n",
      "    RMS test  encoder 0.24068 mV\n",
      "\n",
      "Train set result:\n",
      "   NRMS train encoder 0.09789% \n",
      "    RMS train encoder 0.23944 mV\n",
      "\n",
      "sigma_y = 0.24715164906136317\n"
     ]
    }
   ],
   "source": [
    "print('Test set result:')\n",
    "print(f'   NRMS test  encoder {test_encoder_sim.NRMS(test):.5%} ')\n",
    "print(f'    RMS test  encoder {test_encoder_sim.RMS(test)*1000:.5} mV')\n",
    "print('\\nTrain set result:')\n",
    "print(f'   NRMS train encoder {train_encoder_sim.NRMS(train):.5%} ')\n",
    "print(f'    RMS train encoder {train_encoder_sim.RMS(train)*1000:.5} mV')\n",
    "print('\\nsigma_y =',np.mean(test.y**2)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis\n",
    "### Time domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 486x270 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(9*0.75,5*0.75))\n",
    "time = np.arange(len(test_encoder_sim))/1000\n",
    "plt.plot(time,test.y)\n",
    "plt.plot(time,test.y-test_encoder_sim.y)\n",
    "plt.legend(['System','Error'])\n",
    "plt.xlabel('time index ($10^3$)')\n",
    "plt.ylabel('y (V)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figure_folder+'error-time.pdf')\n",
    "plt.savefig(figure_folder+'error-time.png',dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency/Fourier domain\n",
    "\n",
    "#### Load other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLA test simulation result:\n",
      "  NRMS 18.20754%\n",
      "   RMS 44.4004\n",
      "\n",
      "NLLFR simulation test result:\n",
      "  NRMS 0.12059%\n",
      "   RMS 0.29408\n"
     ]
    }
   ],
   "source": [
    "test_BLA_sim = sys_BLA.apply_experiment(test) #simulation\n",
    "print('BLA test simulation result:')\n",
    "print(f'  NRMS {test_BLA_sim.NRMS(test):.5%}') #don't know why it is lower than BLA from Lauwers et al 2009. \n",
    "print(f'   RMS {test_BLA_sim.RMS(test)*1000:.6}') #don't know why it is lower than BLA from Lauwers et al 2009. \n",
    "\n",
    "y_test_NLLFR_res = scipy.io.loadmat('./WH-data/yval-matlab-NL-LFR.mat')['yValSSNNdiff'][:,0] #residual of the NLLFR model\n",
    "print('\\nNLLFR simulation test result:')\n",
    "print(f'  NRMS {np.mean(y_test_NLLFR_res**2)**0.5/np.std(test.y):.5%}') \n",
    "print(f'   RMS {np.mean(y_test_NLLFR_res**2)**0.5*1000:.5}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 486x270 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy import fftpack\n",
    "\n",
    "\n",
    "#two utility function\n",
    "db = lambda a: 20*np.log10(np.abs(fftpack.fft(a)/len(a))) #convert ot db\n",
    "Hz = lambda x: np.arange(len(x))/1000 #rescale to kHz\n",
    "n_skip = 150 #skip transient\n",
    "\n",
    "\n",
    "#\n",
    "plt.figure(figsize=(9*0.75,5*0.75))\n",
    "#original:\n",
    "ydb = db(test.y[n_skip:]) \n",
    "plt.plot(Hz(ydb),ydb,'r,')\n",
    "#BLA:\n",
    "ydb = db(test.y[n_skip:]-test_BLA_sim.y[n_skip:]) \n",
    "plt.plot(Hz(ydb),ydb,'b,')\n",
    "#NL-LFR:\n",
    "ydb = db(y_test_NLLFR_res)\n",
    "plt.plot(Hz(ydb),ydb,'y,')\n",
    "#ss-encoder:\n",
    "ydb = db(test.y[n_skip:]-test_encoder_sim.y[n_skip:])\n",
    "plt.plot(Hz(ydb),ydb,'g,')\n",
    "\n",
    "#manual creation of legend (markers dont with with , as marker)\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines \n",
    "red_line = mlines.Line2D([], [], color='r', marker='.',\n",
    "                          markersize=15, label='Original',linestyle='None')\n",
    "blue_line = mlines.Line2D([], [], color='b', marker='.',\n",
    "                          markersize=15, label='BLA',linestyle='None')\n",
    "yellow_line = mlines.Line2D([], [], color='y', marker='.',\n",
    "                          markersize=15, label='NL-LFR',linestyle='None')\n",
    "green_line = mlines.Line2D([], [], color='g', marker='.',\n",
    "                          markersize=15, label='encoder (this work)',linestyle='None')\n",
    "plt.legend(handles=[red_line,blue_line,yellow_line,green_line],loc='upper right')\n",
    "\n",
    "plt.xlim(0,(len(test)//2-n_skip//2)/1000)\n",
    "plt.ylabel('dB')\n",
    "plt.xlabel('kHz')\n",
    "plt.ylim(-160,-30)\n",
    "plt.tight_layout()\n",
    "plt.savefig(figure_folder+'fft.png',dpi=115)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training curve figure\n",
    "\n",
    "### Get other modes of operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: #train model\n",
    "    sys_no_encode = deepSI.fit_systems.System_encoder(nx=6,na=1,nb=1) #encoder is used ones to initialize one state, same as parametric start\n",
    "    sys_no_encode.n_hidden_layers = 1\n",
    "    sys_no_encode.n_nodes_per_layer = 15\n",
    "    sys_no_encode.fit(train,epochs=15184,batch_size=1,Loss_kwargs=dict(nf=len(train)-sys.na),sim_val=train[-20000:])\n",
    "    sys_no_encode.save_system('./WH-data/nx6WH-non-encoder')\n",
    "else: #load model from file\n",
    "    sys_no_encode = deepSI.load_system('./WH-data/nx6WH-non-encoder')\n",
    "    \n",
    "if False: #train model\n",
    "    sys_no_batch = deepSI.fit_systems.System_encoder(nx=6,na=50,nb=50) #encoder is used ones to initialize one state, same as parametric start\n",
    "    sys_no_batch.n_hidden_layers = 1\n",
    "    sys_no_batch.n_nodes_per_layer = 15\n",
    "    sys_no_batch.fit(train,epochs=10000,batch_size=99901,Loss_kwargs=dict(nf=80),sim_val=train[-4000:]) #small validation otherwise computation speed is lowered by a lot\n",
    "    sys_no_batch.save_system('./WH-data/nx6WH-no-batch')\n",
    "else: #load model from file\n",
    "    sys_no_batch = deepSI.load_system('./WH-data/nx6WH-no-batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 486x259.2 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def min_val_array(lis):\n",
    "    '''Minimum value in to the left'''\n",
    "    minval = [lis[0]]\n",
    "    for a in lis[1:]:\n",
    "        if minval[-1]>a:\n",
    "            minval.append(a)\n",
    "        else:\n",
    "            minval.append(minval[-1])\n",
    "    return np.array(minval)\n",
    "\n",
    "plt.figure(figsize=(9*0.75,6*0.6))\n",
    "label = 'time-compare-2'\n",
    "plt.grid()\n",
    "\n",
    "hours = np.array(sys_no_encode.time)/60/60\n",
    "plt.plot(hours,min_val_array(sys_no_encode.Loss_val),label='Simulation loss')\n",
    "\n",
    "hours = np.array(sys_no_batch.time)/60/60\n",
    "plt.plot(hours,min_val_array(sys_no_batch.Loss_val),label='Encoder no batch')\n",
    "\n",
    "hours = np.array(sys.time[:len(sys.time)//2])/60/60\n",
    "plt.plot(hours,min_val_array(sys.Loss_val[1:len(sys.time)//2+1]),label='Encoder') #[...+1] is a minor deepSI version mess up correction remove if you retrain the model\n",
    "\n",
    "\n",
    "plt.xlabel('time (hours)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.semilogy()\n",
    "plt.ylabel('NRMS val. set')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figure_folder+f'training-{label}.pdf',dpi=200)\n",
    "plt.savefig(figure_folder+f'training-{label}.png',dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-step error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 486x270 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_step_error = sys.n_step_error(test,nf=70)\n",
    "plt.figure(figsize=(9*0.75,5*0.75))\n",
    "plt.plot(n_step_error)\n",
    "plt.xlabel('n step')\n",
    "plt.ylabel('NRMS n-step error')\n",
    "plt.ylim(0,0.00225)\n",
    "plt.xlim(None,62)\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "plt.savefig(figure_folder+'n-step.pdf')\n",
    "plt.savefig(figure_folder+'n-step.png',dpi=200)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
